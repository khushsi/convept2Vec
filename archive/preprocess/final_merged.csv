iir 1,Boolean retrieval,Standard Boolean model,"INFORMATION RETRIEVAL, GREP, INDEX, INCIDENCE MATRIX, TERM, BOOLEAN RETRIEVAL MODEL, DOCUMENT, COLLECTION, CORPUS","The meaning of the term information retrieval can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study, information retrieval might be defined thus: Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers). As defined in this way, information retrieval used to be an activity that only a few people engaged in: reference librarians, paralegals, and similar professional searchers. Now the world has changed, and hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email. Information retrieval is fast becoming the dominant form of information access, overtaking traditional database-style searching (the sort that is going on when a clerk says to you: ``I'm sorry, I can only look up your order if you can give me your Order ID''). IR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term ``unstructured data'' refers to data which does not have clear, semantically overt, easy-for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly ``unstructured''. This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web pages). IR is also used to facilitate ``semistructured'' search such as finding a document where the title contains Java and the body contains threading. The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically. Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In web search , the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in webcharlink. At the other extreme is personal information retrieval . In the last few years, consumer operating systems have integrated information retrieval (such as Apple's Mac OS X Spotlight or Windows Vista's Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of enterprise, institutional, and domain-specific search , where retrieval might be provided for collections such as a corporation's internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios. In this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1 ) and the central inverted index data structure (Section 1.2 ). We will then examine the Boolean retrieval model and how Boolean queries are processed."""
iir 1 1,An example information retrieval problem,Standard Boolean model,"AD HOC RETRIEVAL, INFORMATION NEED, QUERY, RELEVANCE, EFFECTIVENESS, PRECISION, RECALL, INVERTED INDEX, DICTIONARY, VOCABULARY, LEXICON, POSTING, POSTINGS LIST, POSTINGS","A fat book which many people own is Shakespeare's Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar and NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as grepping through text, after the Unix command grep, which performs this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of . With modern computers, for simple querying of modest collections (the size of Shakespeare's Collected Works is a bit under one million words of text in total), you really need nothing more.But for many purposes, you do need more: To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like tobe able to search collections that total in the order of billions to trillions of words. To allow more flexible matching operations. For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as ``within 5 words'' or ``within the same sentence''. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words. The way to avoid linearly scanning the texts for each query is to index the documents in advance. Let us stick with Shakespeare's Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document - here a play of Shakespeare's - whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document incidence matrix , as in Figure 1.1 . Terms are the indexed units (further discussed in Section 2.2 ); they are usually words, and for the moment you can think of them as words, but the information retrieval literature normally speaks of terms because some of them, such as perhaps I-9 or Hong Kong are not usually thought of as words. Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it. To answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND. The answers for this query are thus Antony and Cleopatra and Hamlet. The Boolean retrieval model is a model for information retrieval in which we can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators and, or, and not. The model views each document as just a set of words. Let us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation. Suppose we have N = 1{ million} documents. By documents we mean whatever units we have decided to build a retrieval system over. They might be individual memos or chapters of a book (see Section 2.1.2 for further discussion). We will refer to the group of documents over which we perform retrieval as the (document) collection . It is sometimes also referred to as a corpus (a body of texts). Suppose each document is about 1000 words long (2-3 book pages). If we assume an average of 6 bytes per word including spaces and punctuation, then this is a document collection about 6 GB in size. Typically, there might be about M = 500{,}000distinct terms in these documents. There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle. We will discuss and model these size assumptions in Section 5.1. Our goal is to develop a system to address the ad hoc retrieval task. This is the most standard IR task. In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query. An information need is the topic about which the user desires to know more, and is differentiated from a query , which is what the user conveys to the computer in an attempt to communicate the information need. A document is relevant if it is one that the user perceives as containing information of value with respect to their personal information need. Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like ``pipeline leaks'' and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture. To assess the effectiveness of an IR system (i.e., the quality of its search results), a user will usually want to know two key statistics about the system's returned results for a query. Precision : What fraction of the returned results are relevant to the information need? Recall : What fraction of the relevant documents in the collection were returned by the system? Detailed discussion of relevance and evaluation measures including precision and recall is found in Chapter 8. We now cannot build a term-document matrix in a naive way. A 500{K} 1{M} matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory. But the crucial observation is that the matrix is extremely sparse, that is, it has few non-zero entries. Because each document is 1000 words long, the matrix has no more than one billion 1's, so a minimum of 99.8% of the cells are zero. A much better representation is to record only the things that do occur, that is, the 1 positions.This idea is central to the first major concept in information retrieval, the inverted index . The name is actually redundant: an index always maps back from terms to the parts of a document where they occur. Nevertheless, inverted index, or sometimes inverted file , has become the standard term in information retrieval.[*]The basic idea of an inverted index is shown in Figure 1.3 . We keep a dictionary of terms (sometimes also referred to as a vocabulary or lexicon ; in this book, we use dictionary for the data structure and vocabulary for the set of terms). Then for each term, we have a list that records which documents the term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called a posting .[*]The list is then called a postings list (or ), and all the postings lists taken together are referred to as the postings . The dictionary in Figure 1.3 has been sorted alphabetically and each postings list is sorted by document ID. We will see why this is useful in Section 1.3 , below, but later we will also consider alternatives to doing this (Section 7.1.5 )."""
iir 1 2,A first take at building an inverted index,Inverted index,"DOCID, SORTING, DOCUMENT FREQUENCY","To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are: Collect the documents to be indexed, Tokenize the text, turning each document into a list of tokens, Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms, Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings. We will define and discuss the earlier stages of processing, that is, steps 1-3, in Section 2.2 . Until then you can think of tokens and normalized tokens as also loosely equivalent to words. Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by sort-based indexing. Within a document collection, we assume that each document has a unique serial number, known as the document identifier ( docID ). During index construction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4 . The core indexing step is sorting this list so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4 . Multiple occurrences of the same term from the same document are then merged.[*]Instances of the same term are then grouped, and the result is split into a dictionary and postings , as shown in the right column of Figure 1.4 . Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the document frequency , which is here also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search. In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3 ), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3 ), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory."""
iir 1 3,Processing Boolean queries,Standard Boolean model,"SIMPLE CONJUNCTIVE QUERIES, POSTINGS LIST, INTERSECTION, POSTINGS MERGE, QUERY OPTIMIZATION","How do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the simple conjunctive query over the inverted index partially shown in Figure 1.3. We Locate Brutus in the Dictionary, Retrieve its postings, Locate Calpurnia in the Dictionary, Retrieve its postings, and Intersect the two postings lists. The intersection is the crucial one: we need to efficiently intersect postings lists so as to be able to quickly find documents that contain both terms. (This operation is sometimes referred to as merging postings lists: this slightly counterintuitive name reflects using the term merge algorithm for a general family of algorithms that combine multiple sorted lists by interleaved advancing of pointers through each; here we are merging the lists with a logical AND operation.). There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6 ): we maintain pointers into both lists and walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are x and y, the intersection takes O(x+y) operations. Formally, the complexity of querying is Theta(N),where N is the number of documents in the collection.[*]Our indexing methods gain us just a constant, not a difference in Theta time complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries. Query optimization is the process of selecting how to organize the work of answering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of t terms. For each of the t terms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document frequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 , we execute the above query. This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list. Consider now the optimization of more general queries. As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term. For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7 . The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings intersection can still be done by the algorithm in Figure 1.6 , but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5 . Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common."""
iir 1 4,The extended Boolean model versus ranked retrieval,Extended Boolean model,"RANKED RETRIEVAL MODEL, FREE TEXT QUERIES, PROXIMITY OPERATOR, TERM FREQUENCY","The Boolean retrieval model contrasts with ranked retrieval models such as the vector space model (Section 6.3 ), in which users largely use free text queries , that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boolean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A proximity operator is a way of specifying that two terms in a query must occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph. Westlaw (http://www.westlaw.com/) is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called ``Terms and Connectors'' by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called ``Natural Language'' by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw. Note the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator), & is AND and /s, /p, and /k ask for matches in the same sentence, same paragraph or within k words respectively. Double quotes give a phrase search (consecutive words); see Section 2.4 . The exclamation mark (!) gives a trailing wildcard query wildcard; thus liab! matches all words starting with liab. Additionally work-site matches any of worksite, work-site or work site; see Section 2.2.1 . Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user. Many users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw's own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground. End worked example. In this chapter, we have looked at the structure and construction of a basic inverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In dictionaryranking-ir-system we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do: We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words. It is often useful to search for compounds or phrases that denote a concept such as ``operating system''. As the Westlaw examples show, we might also wish to do proximity queries such as Gates near Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need term frequency information (the number of times a term occurs in a document) in postings lists. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or ``rank'') the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query. With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying , most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance."""
iir 2 1,The term vocabulary and postings lists,Inverted index,"DOCUMENT UNIT, INDEXING GRANULARITY","Recall the major steps in inverted index construction: Collect the documents to be indexed. Tokenize the text. Do linguistic preprocessing of tokens. Index the documents that each term occurs in. In this chapter we first briefly mention how the basic unit of a document can be defined and how the character sequence that it comprises is determined (Section 2.1 ). We then examine in detail some of the substantive linguistic issues of tokenization and linguistic preprocessing, which determine the vocabulary of terms which a system uses (Section 2.2 ). Tokenization is the process of chopping character streams into tokens , while linguistic preprocessing then deals with building equivalence classes of tokens which are the set of terms that are indexed. Indexing itself is covered in Chapters 1 4 . Then we return to the implementation of postings lists. In Section 2.3 , we examine an extended postings list data structure that supports faster querying, while Section 2.4 covers building postings data structures suitable for handling phrase and proximity queries, of the sort that commonly appear in both extended Boolean models and on the web. Document delineation and character sequence decoding. Obtaining the character sequence in a document. Digital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more complex. The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards. We need to determine the correct encoding. This can be regarded as a machine learning classification problem, as discussed in Chapter 13 ,[*]but is often handled by heuristic methods, user selection, or by using provided document metadata. Once the encoding is determined, we decode the byte sequence to a character sequence. We might save the choice of encoding because it gives some evidence about what language the document is written in. The characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files. Again, we must determine the document format, and then an appropriate decoder has to be used. Even for plain text documents, additional decoding may need to be done. In XML documents xmlbasic, character entities, such as &amp;, need to be decoded to give the correct character, namely & for &amp;. Finally, the textual part of the document may need to be extracted out of other material that will not be processed. This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files. We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters. Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is. Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk. This problem is usually solved by licensing a software library that handles decoding document formats and character encodings. The idea that text is a linear sequence of characters is also called into question by some writing systems, such as Arabic, where text takes on some two dimensional and mixed order characteristics, as shown in and 2.2 . But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1. An example of a vocalized Modern Standard Arabic word.The writing is from right to left and letters undergo complex mutations as they are combined. The representation of short vowels (here, /i/ and /u/) and the final /n/ (nunation) departs from strict linearity by being represented as diacritics above and below letters. Nevertheless, the represented text is still clearly a linear ordering of characters representing sounds. Full vocalization, as here, normally appears only in the Koran and children's books. Day-to-day text is unvocalized (short vowels are not represented but the letter for a would still appear) or partially vocalized, with short vowels inserted in places where the writer perceives ambiguities. These choices add further complexities to indexing.The conceptual linear order of characters is not necessarily the order that you see on the page. In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts. With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings. The next phase is to determine what the document unit for indexing is. Thus far we have assumed that documents are fixed units for the purposes of indexing. For example, we take each file in a folder as a document. But there are many cases in which you might want to do something different. A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document. Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents. If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document. Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a LATEX document) and split them into separate HTML pages for each slide or subsection, stored as separate files. In these cases, you might want to combine multiple files into a single document. More generally, for very long documents, the issue of indexing granularity arises. For a collection of books, it would usually be a bad idea to index an entire book as a document. A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query. Instead, we may well wish to index each chapter or paragraph as a mini-document. Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document. But why stop there? We could treat individual sentences as mini-documents. It becomes clear that there is a precision recall tradeoff here. If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find. The problems with large document units can be alleviated by use of explicit or implicit proximity search ( and 7.2.2 ), and the tradeoffs in resulting system performance that we are hinting at are discussed in Chapter 8 . The issue of index granularity, and in particular a need to simultaneously index documents at multiple levels of granularity, appears prominently in XML retrieval, and is taken up again in Chapter 10 . An IR system should be designed to offer choices of granularity. For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns. For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed."""
iir 2 2,Determining the vocabulary of terms,"Controlled vocabulary,Text segmentation,Word segmentation,Stop words,Word normalization,Stemming, Lemmatisation","TOKEN, TYPE, TERM, LANGUAGE IDENTIFICATION, HYPHENS, COMPOUNDS, COMPOUND-SPLITTER, WORD SEGMENTATION, TOKENNORMALIZATION, EQUIVALENCE CLASSES, CASE-FOLDING, TRUECASING, STEMMING, LEMMATIZATION, LEMMA, PORTER STEMMER, LEMMATIZER","Tokenization. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization. These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence. A term is a (perhaps normalized) type that is included in the IR system's dictionary. The set of index terms could be entirely distinct from the tokens, for instance, they could be semantic identifiers in a taxonomy, but in practice in modern IR systems they are strongly related to the tokens in the document. However, rather than being exactly the tokens that appear in the document, they are usually derived from them by various normalization processes which are discussed in Section 2.2.3 .[*]For example, if the document to be indexed is to sleep perchance to dream, then there are 5 tokens, but only 4 types (since there are 2 instances of to). However, if to is omitted from the index (as a stop word, see Section 2.2.2 ), then there will be only 3 terms: sleep, perchance, and dream. The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions? For O'Neill, which of the following is the desired tokenization?A simple strategy is to just split on all non-alphanumeric characters, but while {o} {neill} looks okay, {aren} {t} looks intuitively bad. For all of them, the choices determine which Boolean queries will match. A query of neill AND capital will match in three cases but not the other two. In how many cases would a query of o'neill AND capital match? If no preprocessing of a query is done, then it would match in only one of the five cases. For either Boolean or free text queries, you always want to do the exact same tokenization of document and query words, generally by processing queries with the same tokenizer. This guarantees that a sequence of characters in a text will always match the same sequence typed in a query. These issues of tokenization are language-specific. It thus requires the language of the document to be known. Language identification based on classifiers that use short character subsequences as features is highly effective; most languages have distinctive signature patterns (see page 2.5 for references). For most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms, such as the programming languages C++ and C#, aircraft names like B-52, or a T.V. show name such as M*A*S*H - which is sufficiently integrated into popular culture that you find usages such as M*A*S*H-style hospitals. Computer technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token, including email addresses (jblack@mail.yahoo.com), web URLs (http://stuff.big.com/new/specials.html), numeric IP addresses (142.32.48.231), package tracking numbers (1Z9999W99845399981), and more. One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary. However, this comes at a large cost in restricting what people can search for. For instance, people might want to search in a bug database for the line number where an error occurs. Items such as the date of an email, which have a clear semantic type, are often indexed separately as document metadata parametricsection. In English, hyphenation is used for various purposes ranging from splitting up vowels in words (co-education) to joining nouns as names (Hewlett-Packard) to a copyediting device to show word grouping (the hold-him-back-and-drag-him-away maneuver). It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation), the last should be separated into words, and that the middle case is unclear. Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms. Conceptually, splitting on white space can also split what should be regarded as a single token. This occurs most commonly with names (San Francisco, Los Angeles) but also with borrowed foreign phrases (au fait) and compounds that are sometimes written as a single word and sometimes space separated (such as white space vs. whitespace). Other cases with internal spaces that we might wish to regard as a single token include phone numbers ((800) 234-2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing New York University. The problems of hyphens and non-separating whitespace can even interact. Advertisements for air fares frequently contain items like San Francisco-Los Angeles, where simply doing whitespace splitting would give unfortunate results. In such cases, issues of tokenization interact with handling phrase queries (which we discuss in Section 2.4 ), particularly if we would like queries for all of lowercase, lower-case and lower case to return the same results. The last two can be handled by splitting on hyphens and using a phrase index. Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way. One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (westlaw), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR ``over eager'' OR overeager. However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization. Each new language presents some new issues. For instance, French has a variant use of the apostrophe for a reduced definite article the before a word beginning with a vowel (e.g., l'ensemble) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g., donne-moi give me). Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both l'ensemble and un ensemble to be indexed under ensemble. Other languages make the problem harder in new ways. German writes compound nouns without spaces (e.g., Computerlinguistik `computational linguistics'; Lebensversicherungsgesellschaftsangestellter `life insurance company employee'). Retrieval systems for German greatly benefit from the use of a compound-splitter module, which is usually implemented by seeing if a word can be subdivided into multiple words that appear in a vocabulary. This phenomenon reaches its limit case with major East Asian Languages (e.g., Chinese, Japanese, Korean, and Thai), where text is written without any spaces between words. An example is shown in Figure 2.3 . One approach here is to perform word segmentation as prior linguistic processing. Methods of word segmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine learning sequence models, such as hidden Markov models or conditional random fields, trained over hand-segmented words (see the references in Section 2.5 ). Since there are multiple possible segmentations of character sequences (see Figure 2.4 ), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization. The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character k-grams), regardless of whether particular sequences cross word boundaries or not. Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway. Even in English, some cases of where to put word boundaries are just orthographic conventions - think of notwithstanding vs. not to mention or into vs. on to - but people are educated to write the words with consistent use of spaces. Dropping common terms: stop words. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . The general strategy for determining a stop list is to sort the terms by collection frequency (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a stop list , the members of which are then discarded during indexing. An example of a stop list is shown in Figure 2.5 . Using a stop list significantly reduces the number of postings that a system has to store; we will present some statistics on this in Chapter 5 (see Table 5.1 , page 5.1 ). And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don't seem very useful. However, this is not true for phrase searches. The phrase query ``President of the United States'', which contains two stop words, is more precise than President AND ``United States''. The meaning of flights to London is likely to be lost if the word to is stopped out. A search for Vannevar Bush's article As we may think will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think. Some special query types are disproportionately affected. Some song titles and well known pieces of verse consist entirely of words that are commonly on stop lists (To be or not to be, Let It Be, I don't want to be, ...). The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists. Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways. We will show in Section 5.3 how good compression techniques greatly reduce the cost of storing the postings for common words. idf then discusses how standard term weighting leads to very common words having little impact on document rankings. Finally, Section 7.1.5 shows how an IR system with impact-sorted indexes can terminate scanning a postings list early when weights get small, and hence common words do not cause a large additional processing cost for the average query, even though postings lists for stop words are very long. So for most modern IR systems, the additional cost of including stop words is not that big - neither in terms of index size nor in terms of query processing time. Normalization (equivalence classing of terms). Having broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document. However, there are many cases when two character sequences are not quite the same but you would like a match to occur. For instance, if you search for USA, you might hope to also match documents containing U.S.A. Token normalization is the process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens.[*] The most standard way to normalize is to implicitly create equivalence classes , which are normally named after one member of the set. For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either. The advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes. It is only easy to write rules of this sort that remove characters. Since the equivalence classes are implicit, it is not obvious when you might want to add characters. For instance, it would be hard to know to turn antidiscriminatory into anti-discriminatory. An alternative to creating equivalence classes is to maintain relations between unnormalized tokens. This method can be extended to hand-constructed lists of synonyms such as car and automobile, a topic we discuss further in Chapter 9 . These term relationships can be achieved in two ways. The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term. A query term is then effectively a disjunction of several postings lists. The alternative is to perform the expansion during index construction. When the document contains automobile, we index it under car as well (and, usually, also vice-versa). Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge. The first method adds a query expansion dictionary and requires more processing at query time, while the second method requires more space for storing postings. Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing. These approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical. This means there can be an asymmetry in expansion. An example of how such an asymmetry can be exploited is shown in Figure 2.6 : if the user enters windows, we wish to allow matches with the capitalized Windows operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase windows. The best amount of equivalence classing or query expansion to do is a fairly open question. Doing some definitely seems a good idea. But doing a lot can easily have unexpected consequences of broadening queries in unintended ways. For instance, equivalence-classing U.S.A. and USA to the latter by deleting periods from tokens might at first seem very reasonable, given the prevalent pattern of optional use of periods in acronyms. However, if I put in as my query term C.A.T., I might be rather upset if it matches every appearance of the word cat in documents. Below we present some of the forms of normalization that are commonly employed and how they are implemented. In many cases they seem helpful, but they can also do harm. In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance. Accents and diacritics. Diacritics on characters in English have a fairly marginal status, and we might well want clichÔäêë Ôäêë and cliche to match, or naive and naÔäêë Ôäêë ve. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in Spanish, peÔäêë Ôäêë a is `a cliff', while pena is `sorrow'. Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics. Capitalization/case-folding. A common strategy is to do case-folding by reducing all letters to lower case. Often this is a good idea: it will allow instances of Automobile at the beginning of a sentence to match with a query of automobile. It will also help on a web search engine when most of your users type in ferrari when they are interested in a Ferrari car. On the other hand, such case folding can equate words that might better be kept apart. Many proper nouns are derived from common nouns and so are distinguished only by case, including companies (General Motors, The Associated Press), government organizations (the Fed vs. fed) and person names (Bush, Black). We already mentioned an example of unintended query expansion with acronyms, which involved not only acronym normalization (C.A.T. CAT) but also case-folding (CAT cat). For English, an alternative to making every token lowercase is to just make some tokens lowercase. The simplest heuristic is to convert to lowercase words at the beginning of a sentence and all words occurring in a title that is all uppercase or in which most or all words are capitalized. These words are usually ordinary words that have been capitalized. Mid-sentence capitalized words are left as capitalized (which is usually correct). This will mostly avoid case-folding in cases where distinctions should be kept apart. The same task can be done more accurately by a machine learning sequence model which uses more features to make the decision of when to case-fold. This is known as truecasing. However, trying to get capitalization right in this way probably doesn't help if your users usually use lowercase regardless of the correct case of words. Thus, lowercasing everything often remains the most practical solution. Other issues in English. Other possible normalizations are quite idiosyncratic and particular to English. For instance, you might wish to equate ne'er and never or the British spelling colour and the American spelling color. Dates, times and similar items come in multiple formats, presenting additional challenges. You might wish to collapse together 3/12/91 and Mar. 12, 1991. However, correct processing here is complicated by the fact that in the U.S., 3/12/91 is Mar. 12, 1991, whereas in Europe it is 3 Dec 1991. Other languages. English has maintained a dominant position on the WWW; approximately 60% of web pages are in English (Gerrand, 2007). But that still leaves 40% of the web, and the non-English portion might be expected to grow over time, since less than one third of Internet users and less than 10% of the world's population primarily speak English. And there are signs of change: Sifry (2007) reports that only about one third of blog posts are in English. Other languages again present distinctive issues in equivalence classing. The French word for the has distinctive forms based not only on the gender (masculine or feminine) and number of the following noun, but also depending on whether the following word begins with a vowel: le, la, l', les. We may well wish to equivalence class these various forms of the. German has a convention whereby vowels with an umlaut can be rendered instead as a two vowel digraph. We would want to treat SchÔäêëtze and Schuetze as equivalent. includegraphics[angle=270]{Japanese-example.eps} Japanese makes use of multiple intermingled writing systems and, like Chinese, does not segment words. The text is mainly Chinese characters with the hiragana syllabary for inflectional endings and function words. The part in latin letters is actually a Japanese expression, but has been taken up as the name of an environmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His name is written using the katakana syllabary in the middle of the first line. The first four characters of the final line express a monetary amount that we would want to match with Ôäêë Ôäêë 500,000 (500,000 Japanese yen). Japanese is a well-known difficult writing system, as illustrated in Figure 2.7 . Modern Japanese is standardly an intermingling of multiple alphabets, principally Chinese characters, two syllabaries (hiragana and katakana) and western characters (Latin letters, Arabic numerals, and various symbols). While there are strong conventions and standardization through the education system over the choice of writing system, in many cases the same word can be written with multiple writing systems. For example, a word may be written in katakana for emphasis (somewhat like italics). Or a word may sometimes be written in hiragana and sometimes in Chinese characters. Successful retrieval thus requires complex equivalence classing across the writing systems. In particular, an end user might commonly present a query entirely in hiragana, because it is easier to type, just as Western end users commonly use all lowercase. Document collections being indexed can include documents from many different languages. Or a single document can easily contain text from multiple languages. For instance, a French email might quote clauses from a contract document written in English. Most commonly, the language is detected and language-particular tokenization and normalization rules are applied at a predetermined granularity, such as whole documents or individual paragraphs, but this still will not correctly deal with cases where language changes occur for brief quotations. When document collections contain multiple languages, a single index may have to contain terms of several languages. One option is to run a language identification classifier on documents and then to tag terms in the vocabulary for their language. Or this tagging can simply be omitted, since it is relatively rare for the exact same character sequence to be a word in different languages. When dealing with foreign or complex words, particularly foreign names, the spelling may be unclear or there may be variant transliteration standards giving different spellings (for example, Chebyshev and Tchebycheff or Beijing and Peking). One way of dealing with this is to use heuristics to equivalence class or expand terms with phonetic equivalents. The traditional and best known such algorithm is the Soundex algorithm, which we cover in Section 3.4. Stemming and lemmatization. For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter's algorithm (Porter, 1980). The entire algorithm is too long and intricate to present here, but we will indicate its general nature. Porter's algorithm consists of 5 phases of word reductions, applied sequentially. Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix. In the first phase, this convention is used with the following rule group. Many of the later rules use a concept of the measure of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word. The official site for the Porter Stemmer is. Other stemmers exist, including the older, one-pass Lovins stemmer (Lovins, 1968), and newer entrants like the Paice/Husk stemmer (Paice, 1990). Figure 2.8 presents an informal comparison of the different behaviors of these stemmers. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words. Particular domains may also require special stemming rules. However, the exact stemmed form does not matter, only the equivalence classes it forms. Rather than using a stemmer, you can use a lemmatizer , a tool from Natural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval. It is hard to say more, because either form of normalization tends not to improve English information retrieval performance in aggregate - at least not by very much. While it helps a lot for some queries, it equally hurts performance a lot for others. Stemming increases recall while harming precision. As an example of what can go wrong, note that the Porter stemmer stems all of the following words. However, since operate in its various forms is a common verb, we would expect to lose considerable precision on queries such as the following with Porter stemming. For a case like this, moving to using a lemmatizer would not completely fix the problem because particular inflectional forms are used in particular collocations: a sentence with the words operate and system is not a good match for the query operating and system. Getting better value from term normalization depends more on pragmatic issues of word use than on formal issues of linguistic morphology. The situation is different for languages with much more morphology (such as Spanish, German, and Finnish). Results in the European CLEF evaluations have repeatedly shown quite large gains from the use of stemmers (and compound splitting for languages like German); see the references in Section 2.5."""
iir 2 3,Faster postings list intersection via skip pointers,Skip list,SKIP LIST,"In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3: we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are m and n, the intersection takes O(m+n) operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn't changing too fast. One way to do this is to use a skip list by augmenting postings lists with skip pointers (at indexing time), as shown in Figure 2.9 . Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers. Consider first efficient merging, with Figure 2.9 as an example. Suppose we've stepped through the lists in the figure until we have matched {8} on each list and moved it to the results list. We advance both pointers, giving us {16} on the upper list and {41} on the lower list. The smallest item is then the element {16} on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to {28}. We thus avoid stepping to {19} and {23} on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown in Figure 2.10 . Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call {{hasSkip}}(p) will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries. Where do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length P, use {P} evenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms. Building effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective. Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you're running a search engine with everything in memory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 and the impact of index size on system speed in Chapter 5."""
iir 2 4,Positional postings and phrase queries,Phrase search,"PHRASE QUERIES, BIWORD INDEX, PHRASE INDEX, POSITIONAL INDEX, NEXT WORD INDEX","Many complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university. is not a match. Most recent search engines support a double quotes syntax (``stanford university'') for phrase queries , which has proven to be very easily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 in the context of ranked retrieval. Biword indexes. One approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase. For example, the text Friends, Romans, Countrymen would generate the biwords. In this model, we treat each of these biwords as a vocabulary term. Being able to process two-word phrase queries is immediate. Longer phrases can be processed by breaking them down. The query stanford university palo alto can be broken into the Boolean query on biwords. This query could be expected to work fairly well in practice, but there can and will be occasional false positives. Without examining the documents, we cannot verify that the documents matching the above Boolean query do actually contain the original 4 word phrase. Among possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for. But related nouns can often be divided from each other by various function words, in phrases such as the abolition of slavery or renegotiation of the constitution. These needs can be incorporated into the biword indexing model in the following way. First, we tokenize the text and perform part-of-speech-tagging. We can then group terms into nouns, including proper nouns, (N) and function words, including articles and prepositions, among other classes. Now deem any string of terms of the form NX*N to be an extended biword. Each such extended biword is made a term in the vocabulary. To process a query using such an extended biword index, we need to also parse it into N's and X's, and then segment the query into extended biwords, which can be looked up in the index.This algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries. Using the above algorithm, the query is parsed into whereas it might seem a better query to omit the middle biword. Better results can be obtained by using more precise part-of-speech patterns that define which extended biwords should be indexed. The concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a phrase index . Indeed, searches for a single term are not naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms. While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed. But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size. Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary. However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme. Positional indexes. For the reasons given, a biword index is not the standard solution. Rather, a positional index is most commonly employed. Here, for each term in the vocabulary, we store postings of the form docID: 1, position2, ..., as shown in Figure 2.11 , where each position is a token index in the document. Each posting will also usually record the term frequency, for reasons discussed in Chapter 6. To process a phrase query, you still need to access the inverted index entries for each distinct term. As before, you would start with the least frequent term and then work to further restrict the list of possible candidates. In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated. This requires working out offsets between the words. Worked example. Satisfying phrase queries.phrasequery Suppose the postings lists for to and be are as in Figure 2.11 , and the query is ``to be or not to be''. The postings lists to access are: to, be, or, not. We will examine intersecting the postings lists for to and be. We first look for documents that contain both terms. Then, we look for places in the lists where there is an occurrence of be with a token index one higher than a position of to, and then we look for another occurrence of each word with token index 4 higher than the first occurrence. In the above lists, the pattern of occurrences that is a possible match is. The same general method is applied for within k word proximity searches, of the sort we saw in westlaw: employment /3 place.Here, /k means ``within k words of (on either side)''. Clearly, positional indexes can be used for such queries; biword indexes cannot. We show in Figure 2.12 an algorithm for satisfying within k word proximity searches; it is further discussed in Exercise 2.4.3. Positional index size. Adopting a positional index expands required postings storage significantly, even if we compress position values/offsets as we will discuss in Section 5.3 . Indeed, moving to a positional index also changes the asymptotic complexity of a postings intersection operation, because the number of items to check is now bounded not by the number of documents but by the total number of tokens in the document collection T. That is, the complexity of a Boolean query is Theta(T) rather than Theta(N). However, most applications have little choice but to accept this, since most users now expect to have the functionality of phrase and proximity searches. Let's examine the space implications of having a positional index. A posting now needs an entry for each occurrence of a term. The index size thus depends on the average document size. The average web page has less than 1000 terms, but documents like SEC stock filings, books, and even some epic poems easily reach 100,000 terms. Consider a term with frequency 1 in 1000 terms on average. The result is that large documents cause an increase of two orders of magnitude in the space required to store the postings list. While the exact numbers depend on the type of documents and the language being indexed, some rough rules of thumb are to expect a positional index to be 2 to 4 times as large as a non-positional index, and to expect a compressed positional index to be about one third to one half the size of the raw text (after removal of markup, etc.) of the original uncompressed documents. Specific numbers for an example collection are given in Table 5.1 (page 5.1 ) and Table 5.6. Combination schemes. The strategies of biword indexes and positional indexes can be fruitfully combined. If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists. A combination strategy uses a phrase index, or just a biword index , for certain queries and uses a positional index for other phrase queries. Good queries to include in the phrase index are ones known to be common based on recent querying behavior. But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare. Adding Britney Spears as a phrase index entry may only give a speedup factor to that query of about 3, since most documents that mention either word are valid results, whereas adding The Who as a phrase index entry may speed up that query by a factor of 1000. Hence, having the latter is more desirable, even if it is a relatively less common query. Williams et al. (2004) evaluate an even more sophisticated scheme which employs indexes of both these sorts and additionally a partial next word index as a halfway house between the first two strategies. For each term, a next word index records terms that follow it in a document. They conclude that such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone."""
iir 3 1,Dictionaries and tolerant retrieval,"Proximity search (text),Search data structure,Query understanding","WILDCARD QUERY, BINARY TREE, B-TREE","In Chapters 1 2 we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a wildcard query : a query such as *a*e*i*o*u*, which seeks documents containing any term that includes all the five vowels in sequence. The * symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated. We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3 . Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection. Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1 2 , in which each vocabulary term has a postings list with the documents in the collection. Search structures for dictionaries. Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the corresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot - and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain. At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2 . Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years' time. A binary search tree.In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest. Search trees overcome many of these issues - for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the binary tree , in which each internal node has two children. The search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is O( M)) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained. To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the B-tree - a search tree in which every internal node has a number of children in the interval [a,b], where a and b are appropriate positive integers; Figure 3.2 shows an example with a=2 and b=4. Each branch under an internal node again represents a test for a range of character sequences, as in the binary tree example of Figure 3.1 . A B-tree may be viewed as ``collapsing'' multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers a and b are determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees. A B-tree.In this example every internal node has between 2 and 4 children. It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets."""
iir 3 2,Wildcard queries,"Wildcard character,Query understanding","WILDCARD QUERY, PERMUTERM INDEX, k-GRAM INDEX","Wildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which leads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart). A query such as mon* is known as a trailing wildcard query , because the * symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set W of terms in the dictionary with the prefix mon. Finally, we use W lookups on the standard inverted index to retrieve all documents containing any term in W. But what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider leading wildcard queries, or queries of the form *mon. Consider a reverse B-tree on the dictionary - one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms R in the vocabulary with a given prefix. In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set W of dictionary terms beginning with the prefix se, then the reverse B-tree to enumerate the set R of terms ending with the suffix mon. Next, we take the intersection W R of these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single * symbol using two B-trees, the normal B-tree and a reverse B-tree. General wildcard queries. We now study two techniques for handling general wildcard queries. Both techniques share a common strategy: express the given wildcard query q w as a Boolean query Q on a specially constructed index, such that the answer to Q is a superset of the set of vocabulary terms matching q w. Then, we check each term in the answer to Q against q w, discarding those vocabulary terms that do not match q w. At this point we have the vocabulary terms matching q w and can resort to the standard inverted index. Permuterm indexes. Our first special index for general wildcard queries is the permuterm index into our character set, to mark the end of a term. Thus, the term hello is shown here as the augmented term hello. Next, we construct a permuterm index, in which the various rotations of each term (augmented with ) all link to the original vocabulary term. Figure 3.3 gives an example of such a permuterm index entry for the term hello. We refer to the set of rotated terms in the permuterm index as the permuterm vocabulary. How does this index help us with wildcard queries? Consider the wildcard query m*n. The key is to rotate such a wildcard query so that the * symbol appears at the end of the string - thus the rotated wildcard query becomes nm*. Next, we look up this string in the permuterm index, where seeking nm* (via a search tree) leads to rotations of (among others) the terms man and moron. Now that the permuterm index enables us to identify the original vocabulary terms matching a wildcard query, we look up these terms in the standard inverted index to retrieve matching documents. We can thus handle any wildcard query with a single * symbol. But what about a query such as fi*mo*er? In this case we first enumerate the terms in the dictionary that are in the permuterm index of erfi*. Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo. In this example, the term fishmonger would survive this filtering but filibuster would not. We then run the surviving terms through the standard inverted index for document retrieval. One disadvantage of the permuterm index is that its dictionary becomes quite large, including as it does all rotations of each term. Notice the close interplay between the B-tree and the permuterm index above. Indeed, it suggests that the structure should perhaps be viewed as a permuterm B-tree. However, we follow traditional terminology here in describing the permuterm index as distinct from the B-tree that allows us to select the rotations with a given prefix. k-gram indexes for wildcard queries. Whereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase. We now present a second technique, known as the k-gram index, for processing wildcard queries. We will also use k-gram indexes in Section 3.3.4 . A k-gram is a sequence of k characters. Thus cas, ast and stl are all 3-grams occurring in the term castle. We use a special character to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: ca, cas, ast, stl, tle, le. In a k-gram index , the dictionary contains all k-grams that occur in any term in the vocabulary. Each postings list points from a k-gram to all vocabulary terms containing that k-gram. For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval. An example is given in Figure 3.4 . How does such an index help us with wildcard queries? Consider the wildcard query re*ve. We are seeking documents containing any term that begins with re and ends with ve. Accordingly, we run the Boolean query re AND ve. This is looked up in the 3-gram index and yields a list of matching terms such as relive, remove and retrieve. Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query. There is however a difficulty with the use of k-gram indexes, that demands one further step of processing. Consider using the 3-gram index described above for the query red*. Following the process described above, we first issue the Boolean query re AND red to the 3-gram index. This leads to a match on terms such as retired, which contain the conjunction of the two 3-grams re and red, yet do not match the original wildcard query red*. To cope with this, we introduce a post-filtering step, in which the terms enumerated by the Boolean query on the 3-gram index are checked individually against the original query red*. This is a simple string-matching operation and weeds out terms such as retired that do not match the original query. Terms that survive are then searched in the standard inverted index as usual. We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index. Search engines do allow the combination of wildcard queries using Boolean operators, for example, re*d AND fe*ri. What is the appropriate semantics for such a query? Since each wildcard query turns into a disjunction of single-term queries, the appropriate interpretation of this example is that we have a conjunction of disjunctions: we seek all documents that contain any term matching re*d and any term matching fe*ri. Even without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index. A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an ``Advanced Query'' interface) that most users never use. Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine."""
iir 3 3,Spelling correction,"Spell checker,Query understanding","EDIT DISTANCE, LEVENSHTEIN DISTANCE, JACCARD COEFFICIENT","We next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney's spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on k-gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience. Implementing spelling correction. There are two basic principles underlying most spelling correction algorithms. Of various alternative correct spellings for a mis-spelled query, choose the ``nearest'' one. This demands that we have a notion of nearness or proximity between a pair of queries. We will develop these proximity measures in Section 3.3.3 . When two correctly spelled queries are tied (or nearly tied), select the one that is more common. For instance, grunt and grant both seem equally plausible as corrections for grnt. Then, the algorithm should choose the more common of grunt and grant as the correction. The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction. A different notion of more common is employed in many search engines, especially on the web. The idea is to use the correction that is most common among queries typed in by other users. The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt. Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation. Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways: On the query carot always retrieve documents containing carot as well as any ``spell-corrected'' version of carot, including carrot and tarot. As in (1) above, but only when the query term carot is not in the dictionary. As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents). When the original query returns fewer than a preset number of documents, the search interface presents a spelling suggestion to the end user: this suggestion consists of the spell-corrected query term(s). Thus, the search engine might respond to the user: ``Did you mean carrot?Ôäêë Ôäêë Ôäêë Ôäêë . Forms of spelling correction. We focus on two specific forms of spelling correction that we refer to as isolated-term correction and context-sensitive correction. In isolated-term correction, we attempt to correct a single query term at a time - even when we have a multiple-term query. The carot example demonstrates this type of correction. Such isolated-term correction would fail to detect, for instance, that the query flew form Heathrow contains a mis-spelling of the term from - because each term in the query is correctly spelled in isolation. We begin by examining two techniques for addressing isolated-term correction: edit distance, and k-gram overlap. We then proceed to context-sensitive correction. Edit distance. Given two character strings s 1 and s 2, the edit distance between them is the minimum number of edit operations required to transform s 1 into s 2. Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string and (iii) replace a character of a string by another character; for these operations, edit distance is sometimes known as Levenshtein distance . For example, the edit distance between cat and dog is 3. In fact, the notion of edit distance can be generalized to allowing different weights for different kinds of edit operations, for instance a higher weight may be placed on replacing the character s by the character p, than on replacing it by the character a (the latter being closer to s on the keyboard). Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity). However, the remainder of our treatment here will focus on the case in which all edit operations have the same weight. It is well-known how to compute the (weighted) edit distance between two strings in time O( s 1 s 2), where s i denotes the length of a string s i. The idea is to use the dynamic programming algorithm in Figure 3.5 , where the characters in s 1 and s 2 are given in array form. The algorithm fills the (integer) entries in a matrix m whose two dimensions equal the lengths of the two strings whose edit distances is being computed; the (i,j) entry of the matrix will hold (after the algorithm is executed) the edit distance between the strings consisting of the first i characters of s 1 and the first j characters of s 2. The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5 , where the three quantities whose minimum is taken correspond to substituting a character in s 1, inserting a character in s 1 and inserting a character in s 2. Figure 3.6 shows an example Levenshtein distance computation of Figure 3.5 . The typical cell [i,j] has four entries formatted as a 2 2 cell. The lower right entry in each cell is the of the other three, corresponding to the main dynamic programming step in Figure 3.5 . The other three entries are the three entries m[i-1,j-1] + 0 or 1 depending on whether s 1[i]=s 2[j], m[i-1, j] + 1 and m[i, j-1] + 1. The cells with numbers in italics depict the path by which we determine the Levenshtein distance. The spelling correction problem however demands more than computing edit distance: given a set { S} of strings (corresponding to terms in the vocabulary) and a query string q, we seek the string(s) in V of least edit distance from q. We may view this as a decoding problem, in which the codewords (the strings in V) are prescribed in advance. The obvious way of doing this is to compute the edit distance from q to each string in V, before selecting the string(s) of minimum edit distance. This exhaustive search is inordinately expensive. Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s). The simplest such heuristic is to restrict the search to dictionary terms beginning with the same letter as the query string; the hope would be that spelling errors do not occur in the first character of the query. A more sophisticated variant of this heuristic is to use a version of the permuterm index, in which we omit the end-of-word symbol . Consider the set of all rotations of the query string q. For each rotation r from this set, we traverse the B-tree into the permuterm index, thereby retrieving all dictionary terms that have a rotation beginning with r. For instance, if q is mase and we consider the rotation r={{sema}}, we would retrieve dictionary terms such as semantic and semaphore that do not have a small edit distance to q. Unfortunately, we would miss more pertinent dictionary terms such as mare and mane. To address this, we refine this rotation scheme: for each rotation, we omit a suffix of characters before performing the B-tree traversal. This ensures that each term in the set R of terms retrieved from the dictionary includes a ``long'' substring in common with q. The value of could depend on the length of q. Alternatively, we may set it to a fixed constant such as 2. k-gram indexes for spelling correction. To further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the k-gram index of Section 3.2.2 to assist with retrieving vocabulary terms with low edit distance to the query q. Once we retrieve such terms, we can then find the ones of least edit distance from q. In fact, we will use the k-gram index to retrieve vocabulary terms that have many k-grams in common with the query. We will argue that for reasonable definitions of ``many k-grams in common,'' the retrieval process is essentially that of a single scan through the postings for the k-grams in the query string q. The 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the postings for the three bigrams in the query bord. Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams. A single scan of the postings (much as in Chapter 1 ) would let us enumerate all such terms; in the example of Figure 3.7 we would enumerate aboard, boardroom and border. This straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of k-grams from the query q: terms like boardroom, an implausible ``correction'' of bord, get enumerated. Consequently, we require more nuanced measures of the overlap in k-grams between a vocabulary term and q. The linear scan intersection can be adapted when the measure of overlap is the Jaccard coefficient for measuring the overlap between two sets A and B, defined to be A B/ A B. The two sets we consider are the set of k-grams in the query q, and the set of k-grams in a vocabulary term. As the scan proceeds, we proceed from one vocabulary term t to the next, computing on the fly the Jaccard coefficient between q and t. If the coefficient exceeds a preset threshold, we add t to the output; if not, we move on to the next term in the postings. To compute the Jaccard coefficient, we need the set of k-grams in q and t. Since we are scanning the postings for all k-grams in q, we immediately have these k-grams on hand. What about the k-grams of t? In principle, we could enumerate these on the fly from t; in practice this is not only slow but potentially infeasible since, in all likelihood, the postings entries themselves do not contain the complete string t but rather some encoding of t. The crucial observation is that to compute the Jaccard coefficient, we only need the length of the string t. To see this, recall the example of Figure 3.7 and consider the point when the postings scan for query q= bord reaches term t= boardroom. We know that two bigrams match. If the postings stored the (pre-computed) number of bigrams in boardroom (namely, 8), we have all the information we require to compute the Jaccard coefficient to be 2/(8+3-2); the numerator is obtained from the number of postings hits (2, from bo and rd) while the denominator is the sum of the number of bigrams in bord and boardroom, less the number of postings hits. We could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans. How do we use these for spelling correction? One method that has some empirical support is to first use the k-gram index to enumerate a set of candidate vocabulary terms that are potential corrections of q. We then compute the edit distance from q to each term in this set, selecting terms from the set with small edit distance to q. Context sensitive spelling correction. Isolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled. When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow. The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4 ) even though each query term is correctly spelled, then try substitutions of each correction in the phrase. For the example flew form Heathrow, we enumerate such phrases as fled form Heathrow and flew fore Heathrow. For each such substitute phrase, the search engine runs the query and determines the number of matching results. This enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives. Several heuristics are used to trim this space. In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users. For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form. In this example, the biword fled fore is likely to be rare compared to the biword flew from. Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow. As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors."""
iir 3 4,Phonetic correction,"Speech disorder,Query understanding",SOUNDEX,"Our final technique for tolerant retrieval has to do with phonetic correction: misspellings that arise because the user types a query that sounds like the target term. Such algorithms are especially applicable to searches on the names of people. The main idea here is to generate, for each term, a ``phonetic hash'' so that similar-sounding terms hash to the same value. The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries. It is mainly used to correct phonetic misspellings in proper nouns. Algorithms for such phonetic hashing are commonly collectively known as soundex algorithms. However, there is an original soundex algorithm, with various variants, built on the following scheme: Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index. Do the same with query terms. When the query calls for a soundex match, search this soundex index. The variations in different soundex algorithms have to do with the conversion of terms to 4-character forms. A commonly used conversion results in a 4-character code, with the first character being a letter of the alphabet and the other three being digits between 0 and 9. Retain the first letter of the term. Change all occurrences of the following letters to '0' (zero): 'A', E', 'I', 'O', 'U', 'H', 'W', 'Y'. Change letters to digits as follows: B, F, P, V to 1. C, G, J, K, Q, S, X, Z to 2. D,T to 3. L to 4. M, N to 5. R to 6. Repeatedly remove one out of each pair of consecutive identical digits. Remove all zeros from the resulting string. Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits. For an example of a soundex map, Hermann maps to H655. Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index. This algorithm rests on a few observations: (1) vowels are viewed as interchangeable, in transcribing names; (2) consonants with similar sounds (e.g., D and T) are put in equivalence classes. This leads to related names often having the same soundex codes. While these rules work for many cases, especially European languages, such rules tend to be writing system dependent. For example, Chinese names can be written in Wade-Giles or Pinyin transcription. While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently."""
iir 4 1,Index construction,Search engine indexing,"INDEXING, INDEXER, CACHING, SEEK TIME, BUFFER","In this chapter, we look at how to construct an inverted index. We call this process index construction or indexing ; the process or machine that performs it the indexer . The design of indexing algorithms is governed by hardware constraints. We therefore begin this chapter with a review of the basics of computer hardware that are relevant for indexing. We then introduce blocked sort-based indexing (Section 4.2 ), an efficient single-machine algorithm designed for static collections that can be viewed as a more scalable version of the basic sort-based indexing algorithm we introduced in Chapter 1 . Section 4.3 describes single-pass in-memory indexing, an algorithm that has even better scaling properties because it does not hold the vocabulary in memory. For very large collections like the web, indexing has to be distributed over computer clusters with hundreds or thousands of machines. We discuss this in Section 4.4 . Collections with frequent changes require dynamic indexing introduced in Section 4.5 so that changes in the collection are immediately reflected in the index. Finally, we cover some complicating issues that can arise in indexing - such as security and indexes for ranked retrieval - in Section 4.6 . Index construction interacts with several topics covered in other chapters. The indexer needs raw text, but documents are encoded in many ways (see Chapter 2 ). Indexers compress and decompress intermediate files and the final index (see Chapter 5 ). In web search, documents are not on a local file system, but have to be spidered or crawled (see Chapter 20 ). In enterprise search , most documents are encapsulated in varied content management systems, email applications, and databases. We give some examples in Section 4.7 . Although most of these applications can be accessed via http, native Application Programming Interfaces (APIs) are usually more efficient. The reader should be aware that building the subsystem that feeds raw text to the indexing process can in itself be a challenging problem. When building an information retrieval (IR) system, many decisions are based on the characteristics of the computer hardware on which the system runs. We therefore begin this chapter with a brief review of computer hardware. Performance characteristics typical of systems in 2007 are shown in Table 4.1 . A list of hardware basics that we need in this book to motivate IR system design follows. Access to data in memory is much faster than access to data on disk. It takes a few clock cycles (perhaps 5 10^{-9} seconds) to access a byte in memory, but much longer to transfer it from disk (about 2 10^{-8} seconds). Consequently, we want to keep as much data as possible in memory, especially those data that we need to access frequently. We call the technique of keeping frequently used disk data in main memory caching . When doing a disk read or write, it takes a while for the disk head to move to the part of the disk where the data are located. This time is called the seek time and it averages 5 ms for typical disks. No data are being transferred during the seek. To maximize data transfer rates, chunks of data that will be read together should therefore be stored contiguously on disk. For example, using the numbers in Table 4.1 it may take as little as 0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is stored as one chunk, but up to 0.2+100(5 10^{-3})=0.7 seconds if it is stored in 100 noncontiguous chunks because we need to move the disk head up to 100 times. Operating systems generally read and write entire blocks. Thus, reading a single byte from disk can take as much time as reading the entire block. Block sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part of main memory where a block being read or written is stored a buffer . Data transfers from disk to memory are handled by the system bus, not by the processor. This means that the processor is available to process data during disk I/O. We can exploit this fact to speed up data transfers by storing compressed data on disk. Assuming an efficient decompression algorithm, the total time of reading and then decompressing compressed data is usually less than reading uncompressed data. Servers used in IR systems typically have several gigabytes (GB) of main memory, sometimes tens of GB. Available disk space is several orders of magnitude larger."""
iir 4 2,Blocked sort-based indexing,Search engine indexing,"TERMID, REUTERS-RCV1, EXTERNAL SORTING ALGORITHM, BLOCKED SORT-BASED INDEXING ALGORITHM, INVERSION, POSTING","The basic steps in constructing a nonpositional index are depicted in Figure 1.4 . We first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. In this chapter, we describe methods for large collections that require the use of secondary storage. To make index construction more efficient, we represent terms as termIDs (instead of strings as we did in Figure 1.4 ), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection; or, in a two-pass approach, we compile the vocabulary in the first pass and construct the inverted index in the second pass. The index construction algorithms described in this chapter all do a single pass through the data. Section 4.7 gives references to multipass algorithms that are preferable in certain applications, for example, when disk space is scarce. We work with the Reuters-RCV1 collection as our model collection in this chapter, a collection with roughly 1 GB of text. It consists of about 800,000 documents that were sent over the Reuters newswire during a 1-year period between August 20, 1996, and August 19, 1997. A typical document is shown in Figure 4.1 , but note that we ignore multimedia information like images in this book and are only concerned with text. Reuters-RCV1 covers a wide range of international topics, including politics, business, sports, and (as in this example) science. Some key statistics of the collection are shown in Table 4.2 . Reuters-RCV1 has 100 million tokens. Collecting all termID-docID pairs of the collection using 4 bytes each for termID and docID therefore requires 0.8 GB of storage. Typical collections today are often one or two orders of magnitude larger than Reuters-RCV1. You can easily see how such collections overwhelm even large computers if we try to sort their termID-docID pairs in memory. If the size of the intermediate files during index construction is within a small factor of available memory, then the compression techniques introduced in Chapter 5 can help; however, the postings file of many large collections cannot fit into memory even after compression. With main memory insufficient, we need to use an external sorting algorithm , that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks as we explained in Section 4.1 . One solution is the blocked sort-based indexing algorithm or BSBI in Figure 4.2 . BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. The algorithm parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full (PARSENEXTBLOCK in Figure 4.2 ). We choose the block size to fit comfortably into memory to permit a fast in-memory sort. The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk. Applying this to Reuters-RCV1 and assuming we can fit 10 million termID-docID pairs into memory, we end up with ten blocks, each an inverted index of one part of the collection. Merging in blocked sort-based indexing.Two blocks (``postings lists to be merged'') are loaded from disk into memory, merged in memory (``merged postings lists'') and written back to disk. We show terms instead of termIDs for better readability. In the final step, the algorithm simultaneously merges the ten blocks into one large merged index. An example with two blocks is shown in Figure 4.3 , where we use d i to denote the i^{th} document of the collection. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. In each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary. How expensive is BSBI? Its time complexity is Theta(T T) because the step with the highest time complexity is sorting and T is an upper bound for the number of items we must sort (i.e., the number of termID-docID pairs). But the actual indexing time is usually dominated by the time it takes to parse the documents (PARSENEXTBLOCK) and to do the final merge (MERGEBLOCKS). Exercise 4.6 asks you to compute the total index construction time for RCV1 that includes these steps as well as inverting the blocks and writing them to disk. Notice that Reuters-RCV1 is not particularly large in an age when one or more GB of memory are standard on personal computers. With appropriate compression (Chapter 5 ), we could have created an inverted index for RCV1 in memory on a not overly beefy server. The techniques we have described are needed, however, for collections that are several orders of magnitude larger."""
iir 4 3,Single-pass in-memory indexing,Search engine indexing,SINGLE-PASS IN-MEMORY INDEXING,"Blocked sort-based indexing has excellent scaling properties, but it needs a data structure for mapping terms to termIDs. For very large collections, this data structure does not fit into memory. A more scalable alternative is single-pass in-memory indexing or SPIMI . SPIMI uses terms instead of termIDs, writes each block's dictionary to disk, and then starts a new dictionary for the next block. SPIMI can index collections of any size as long as there is enough disk space available. The SPIMI algorithm is shown in Figure 4.4 . The part of the algorithm that parses documents and turns them into a stream of term-docID pairs, which we call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on the token stream until the entire collection has been processed. Tokens are processed one by one (line 4) during each successive call of SPIMI-INVERT. When a term occurs for the first time, it is added to the dictionary (best implemented as a hash), and a new postings list is created (line 6). The call in line 7 returns this postings list for subsequent occurrences of the term. A difference between BSBI and SPIMI is that SPIMI adds a posting directly to its postings list (line 10). Instead of first collecting all termID-docID pairs and then sorting them (as we did in BSBI), each postings list is dynamic (i.e., its size is adjusted as it grows) and it is immediately available to collect postings. This has two advantages: It is faster because there is no sorting required, and it saves memory because we keep track of the term a postings list belongs to, so the termIDs of postings need not be stored. As a result, the blocks that individual calls of SPIMI-INVERT can process are much larger and the index construction process as a whole is more efficient. Because we do not know how large the postings list of a term will be when we first encounter it, we allocate space for a short postings list initially and double the space each time it is full (lines 8-9). This means that some memory is wasted, which counteracts the memory savings from the omission of termIDs in intermediate data structures. However, the overall memory requirements for the dynamically constructed index of a block in SPIMI are still lower than in BSBI. When memory has been exhausted, we write the index of the block (which consists of the dictionary and the postings lists) to disk (line 12). We have to sort the terms (line 11) before doing this because we want to write postings lists in lexicographic order to facilitate the final merging step. If each block's postings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each block. Each call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last step of SPIMI (corresponding to line 7 in Figure 4.2 ; not shown in Figure 4.4 ) is then to merge the blocks into the final inverted index. In addition to constructing a new dictionary structure for each block and eliminating the expensive sorting step, SPIMI has a third important component: compression. Both the postings and the dictionary terms can be stored compactly on disk if we employ compression. Compression increases the efficiency of the algorithm further because we can process even larger blocks, and because the individual blocks require less space on disk. We refer readers to the literature for this aspect of the algorithm (Section 4.7 ). The time complexity of SPIMI is Theta(T) because no sorting of tokens is required and all operations are at most linear in the size of the collection."""
iir 4 4,Distributed indexing,Search engine indexing,"MAPREDUCE, MASTER NODE, SPLITS, KEY-VALUE PAIRS, MAP PHASE, PARSER, SEGMENT FILE, REDUCE PHASE, INVERTER","Collections are often so large that we cannot perform index construction efficiently on a single machine. This is particularly true of the World Wide Web for which we need large computer clusters [*]to construct any reasonably sized web index. Web search engines, therefore, use distributed indexing algorithms for index construction. The result of the construction process is a distributed index that is partitioned across several machines - either according to term or according to document. In this section, we describe distributed indexing for a term-partitioned index . Most large search engines prefer a document-partitioned index (which can be easily generated from a term-partitioned index). We discuss this topic further in Section 20.3 . The distributed index construction method we describe in this section is an application of MapReduce , a general architecture for distributed computing. MapReduce is designed for large computer clusters. The point of a cluster is to solve large computing problems on cheap commodity machines or nodes that are built from standard parts (processor, memory, disk) as opposed to on a supercomputer with specialized hardware. Although hundreds or thousands of machines are available in such clusters, individual machines can fail at any time. One requirement for robust distributed indexing is, therefore, that we divide the work up into chunks that we can easily assign and - in case of failure - reassign. A master node directs the process of assigning and reassigning tasks to individual worker nodes. The map and reduce phases of MapReduce split up the computing job into chunks that standard machines can process in a short time. The various steps of MapReduce are shown in Figure 4.5 and an example on a collection consisting of two documents is shown in Figure 4.6 . First, the input data, in our case a collection of web pages, are split into n splits where the size of the split is chosen to ensure that the work can be distributed evenly (chunks should not be too large) and efficiently (the total number of chunks we need to manage should not be too large); 16 or 64 MB are good sizes in distributed indexing. Splits are not preassigned to machines, but are instead assigned by the master node on an ongoing basis: As a machine finishes processing one split, it is assigned the next one. If a machine dies or becomes a laggard due to hardware problems, the split it is working on is simply reassigned to another machine. In general, MapReduce breaks a large computing problem into smaller parts by recasting it in terms of manipulation of key-value pairs . For indexing, a key-value pair has the form (termID,docID). In distributed indexing, the mapping from terms to termIDs is also distributed and therefore more complex than in single-machine indexing. A simple solution is to maintain a (perhaps precomputed) mapping for frequent terms that is copied to all nodes and to use terms directly (instead of termIDs) for infrequent terms. We do not address this problem here and assume that all nodes share a consistent term termID mapping. The map phase of MapReduce consists of mapping splits of the input data to key-value pairs. This is the same parsing task we also encountered in BSBI and SPIMI, and we therefore call the machines that execute the map phase parsers . Each parser writes its output to local intermediate files, the segment files (shown as {a-f} {g-p} {q-z} in Figure 4.5 ). For the reduce phase , we want all values for a given key to be stored close together, so that they can be read and processed quickly. This is achieved by partitioning the keys into j term partitions and having the parsers write key-value pairs for each term partition into a separate segment file. In Figure 4.5 , the term partitions are according to first letter: a-f, g-p, q-z, and j=3. (We chose these key ranges for ease of exposition. In general, key ranges need not correspond to contiguous terms or termIDs.) The term partitions are defined by the person who operates the indexing system (Exercise 4.6 ). The parsers then write corresponding segment files, one for each term partition. Each term partition thus corresponds to r segments files, where r is the number of parsers. For instance, Figure 4.5 shows three a-f segment files of the a-f partition, corresponding to the three parsers shown in the figure. Collecting all values (here: docIDs) for a given key (here: termID) into one list is the task of the inverters in the reduce phase. The master assigns each term partition to a different inverter - and, as in the case of parsers, reassigns term partitions in case of failing or slow inverters. Each term partition (corresponding to r segment files, one on each parser) is processed by one inverter. We assume here that segment files are of a size that a single machine can handle (Exercise 4.6 ). Finally, the list of values is sorted for each key and written to the final sorted postings list (``postings'' in the figure). (Note that postings in Figure 4.6 include term frequencies, whereas each posting in the other sections of this chapter is simply a docID without term frequency information.) The data flow is shown for a-f in Figure 4.5 . This completes the construction of the inverted index. Parsers and inverters are not separate sets of machines. The master identifies idle machines and assigns tasks to them. The same machine can be a parser in the map phase and an inverter in the reduce phase. And there are often other jobs that run in parallel with index construction, so in between being a parser and an inverter a machine might do some crawling or another unrelated task. To minimize write times before inverters reduce the data, each parser writes its segment files to its local disk. In the reduce phase, the master communicates to an inverter the locations of the relevant segment files (e.g., of the r segment files of the a-f partition). Each segment file only requires one sequential read because all data relevant to a particular inverter were written to a single segment file by the parser. This setup minimizes the amount of network traffic needed during indexing. Map and reduce functions in MapReduce. In general, the map function produces a list of key-value pairs. All values for a key are collected into one list in the reduce phase. This list is then processed further. The instantiations of the two functions and an example are shown for index construction. Because the map phase processes documents in a distributed fashion, termID-docID pairs need not be ordered correctly initially as in this example. The example shows terms instead of termIDs for better readability. We abbreviate Caesar as C and conquered as c'ed. Figure 4.6 shows the general schema of the MapReduce functions. Input and output are often lists of key-value pairs themselves, so that several MapReduce jobs can run in sequence. In fact, this was the design of the Google indexing system in 2004. What we describe in this section corresponds to only one of five to ten MapReduce operations in that indexing system. Another MapReduce operation transforms the term-partitioned index we just created into a document-partitioned one. MapReduce offers a robust and conceptually simple framework for implementing index construction in a distributed environment. By providing a semiautomatic method for splitting index construction into smaller tasks, it can scale to almost arbitrarily large collections, given computer clusters of sufficient size."""
iir 4 5,Dynamic indexing,Search engine indexing,"AUXILIARY INDEX, LOGARITHMIC MERGING","Thus far, we have assumed that the document collection is static. This is fine for collections that change infrequently or never (e.g., the Bible or Shakespeare). But most collections are modified frequently with documents being added, deleted, and updated. This means that new terms need to be added to the dictionary, and postings lists need to be updated for existing terms. The simplest way to achieve this is to periodically reconstruct the index from scratch. This is a good solution if the number of changes over time is small and a delay in making new documents searchable is acceptable - and if enough resources are available to construct a new index while the old one is still available for querying. If there is a requirement that new documents be included quickly, one solution is to maintain two indexes: a large main index and a small auxiliary index that stores new documents. The auxiliary index is kept in memory. Searches are run across both indexes and results merged. Deletions are stored in an invalidation bit vector. We can then filter out deleted documents before returning the search result. Documents are updated by deleting and reinserting them. Each time the auxiliary index becomes too large, we merge it into the main index. The cost of this merging operation depends on how we store the index in the file system. If we store each postings list as a separate file, then the merge simply consists of extending each postings list of the main index by the corresponding postings list of the auxiliary index. In this scheme, the reason for keeping the auxiliary index is to reduce the number of disk seeks required over time. Updating each document separately requires up to M {ave}disk seeks, where M {ave} is the average size of the vocabulary of documents in the collection. With an auxiliary index, we only put additional load on the disk when we merge auxiliary and main indexes. Unfortunately, the one-file-per-postings-list scheme is infeasible because most file systems cannot efficiently handle very large numbers of files. The simplest alternative is to store the index as one large file, that is, as a concatenation of all postings lists. In reality, we often choose a compromise between the two extremes (Section 4.7 ). To simplify the discussion, we choose the simple option of storing the index as one large file here. In this scheme, we process each posting times because we touch it during each of merges where n is the size of the auxiliary index and T the total number of postings. Thus, the overall time complexity is Theta(T^2/n). (We neglect the representation of terms here and consider only the docIDs. For the purpose of time complexity, a postings list is simply a list of docIDs.). We can do better than Theta(T^2/n) by introducing 2 (T/n) indexes I 0, I 1, I 2, ...of size 2^0 n, 2^1 n, 2^2 n .... Postings percolate up this sequence of indexes and are processed only once on each level. This scheme is called logarithmic merging (Figure 4.7 ). As before, up to n postings are accumulated in an in-memory auxiliary index, which we call Z 0. When the limit n is reached, the 2^0 n postings in Z 0 are transferred to a new index I 0 that is created on disk. The next time Z 0 is full, it is merged with I 0 to create an index Z 1 of size 2^1! n. Then Z 1 is either stored as I 1 (if there isn't already an I 1) or merged with I 1 into Z 2 (if I 1 exists); and so on. We service search requests by querying in-memory Z 0 and all currently valid indexes I i on disk and merging the results. Readers familiar with the binomial heap data structure[*] will recognize its similarity with the structure of the inverted indexes in logarithmic merging. Overall index construction time is Theta(T (T/n)) because each posting is processed only once on each of the (T/n) levels. We trade this efficiency gain for a slow down of query processing; we now need to merge results from (T/n) indexes as opposed to just two (the main and auxiliary indexes). As in the auxiliary index scheme, we still need to merge very large indexes occasionally (which slows down the search system during the merge), but this happens less frequently and the indexes involved in a merge on average are smaller. Having multiple indexes complicates the maintenance of collection-wide statistics. For example, it affects the spelling correction algorithm in Section 3.3 that selects the corrected alternative with the most hits. With multiple indexes and an invalidation bit vector, the correct number of hits for a term is no longer a simple lookup. In fact, all aspects of an IR system - index maintenance, query processing, distribution, and so on - are more complex in logarithmic merging. Because of this complexity of dynamic indexing, some large search engines adopt a reconstruction-from-scratch strategy. They do not construct indexes dynamically. Instead, a new index is built from scratch periodically. Query processing is then switched from the new index and the old index is deleted."""
iir 4 6,Other types of indexes,"Heaps law,Zipfs law","RANKED, RETRIEVAL SYSTEMS, SECURITY, ACCESS CONTROL LISTS","This chapter only describes construction of nonpositional indexes. Except for the much larger data volume we need to accommodate, the main difference for positional indexes is that (termID, docID, (position1, position2, ...)) triples, instead of (termID, docID) pairs have to be processed and that tokens and postings contain positional information in addition to docIDs. With this change, the algorithms discussed here can all be applied to positional indexes. In the indexes we have considered so far, postings lists are ordered with respect to docID. As we see in Chapter 5, this is advantageous for compression - instead of docIDs we can compress smaller gaps between IDs, thus reducing space requirements for the index. However, this structure for the index is not optimal when we build ranked (Chapters 6 7 ) - as opposed to Boolean - retrieval systems . In ranked retrieval, postings are often ordered according to weight or impact , with the highest-weighted postings occurring first. With this organization, scanning of long postings lists during query processing can usually be terminated early when weights have become so small that any further documents can be predicted to be of low similarity to the query (see Chapter 6 ). In a docID-sorted index, new documents are always inserted at the end of postings lists. In an impact-sorted index impactordered, the insertion can occur anywhere, thus complicating the update of the inverted index. Security is an important consideration for retrieval systems in corporations. A low-level employee should not be able to find the salary roster of the corporation, but authorized managers need to be able to search for it. Users' results lists must not contain documents they are barred from opening; the very existence of a document can be sensitive information. User authorization is often mediated through access control lists or ACLs. ACLs can be dealt with in an information retrieval system by representing each document as the set of users that can access them (Figure 4.8 ) and then inverting the resulting user-document matrix. The inverted ACL index has, for each user, a ``postings list'' of documents they can access - the user's access list. Search results are then intersected with this list. However, such an index is difficult to maintain when access permissions change - we discussed these difficulties in the context of incremental indexing for regular postings lists in Section 4.5. It also requires the processing of very long postings lists for users with access to large document subsets. User membership is therefore often verified by retrieving access information directly from the file system at query time - even though this slows down retrieval. We discussed indexes for storing and retrieving terms (as opposed to documents) in Chapter 3 ."""
iir 5 1,Index compression,"Search engine indexing,Compression","POSTING, RULE OF 30, LOSSLESS, LOSSY COMPRESSION, HEAPSÔäêë Ôäêë LAW, ZIPFÔäêë Ôäêë S LAW, POWER LAW","Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems. One benefit of compression is immediately clear. We need less disk space. As we will see, compression ratios of 1:4 are easy to achieve, potentially cutting the cost of storing the index by 75%. There are two more subtle benefits of compression. The first is increased use of caching. Search systems use some parts of the dictionary and the index much more than others. For example, if we cache the postings list of a frequently used query term t, then the computations necessary for responding to the one-term query t can be entirely done in memory. With compression, we can fit a lot more information into main memory. Instead of having to expend a disk seek when processing a query with t, we instead access its postings list in memory and decompress it. As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small. As a result, we are able to decrease the response time of the IR system substantially. Because memory is a more expensive resource than disk space, increased speed owing to caching - rather than decreased space requirements - is often the prime motivator for compression. The second more subtle advantage of compression is faster transfer of data from disk to memory. Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form. For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression. So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists. If the main goal of compression is to conserve disk space, then the speed of compression algorithms is of no concern. But for improved cache utilization and faster disk-to-memory transfer, decompression speeds must be high. The compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression. In this chapter, we define a posting as a docID in a postings list. For example, the postings list (6; 20, 45, 100), where 6 is the termID of the list's term, contains three postings. As discussed in Section 2.4.2 , postings in most search systems also contain frequency and position information; but we will only consider simple docID postings here. See Section 5.4 for references on compressing frequencies and positions. This chapter first gives a statistical characterization of the distribution of the entities we want to compress - terms and postings in large collections (Section 5.1 ). We then look at compression of the dictionary, using the dictionary-as-a-string method and blocked storage (Section 5.2 ). Section 5.3 describes two techniques for compressing the postings file, variable byte encoding and encoding. Statistical properties of terms in information retrieval. As in the last chapter, we use Reuters-RCV1 as our model collection (see Table 4.2 , page 4.2 ). We give some term and postings statistics for the collection in Table 5.1 . ``%'' indicates the reduction in size from the previous line. ``T%'' is the cumulative reduction from unfiltered. The table shows the number of terms for different levels of preprocessing (column 2). The number of terms is the main factor in determining the size of the dictionary. The number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection. The expected size of a positional index is related to the number of positions it must encode (column 4). In general, the statistics in Table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly. Stemming and case folding reduce the number of (distinct) terms by 17% each and the number of nonpositional postings by 4% and 3%, respectively. The treatment of the most frequent words is also important. The rule of 30 states that the 30 most common words account for 30% of the tokens in written text (31% in the table). Eliminating the 150 most common words from indexing (as stop words; cf. Section 2.2.2 , page 2.2.2 ) cuts 25% to 30% of the nonpositional postings. But, although a stop list of 150 words reduces the number of postings by a quarter or more, this size reduction does not carry over to the size of the compressed index. As we will see later in this chapter, the postings lists of frequent words require only a few bits per posting after compression. The deltas in the table are in a range typical of large collections. Note, however, that the percentage reductions can be very different for some text collections. For example, for a collection of web pages with a high proportion of French text, a lemmatizer for French reduces vocabulary size much more than the Porter stemmer does for an English-only collection because French is a morphologically richer language than English. The compression techniques we describe in the remainder of this chapter are lossless , that is, all information is preserved. Better compression ratios can be achieved with lossy compression , which discards some information. Case folding, stemming, and stop word elimination are forms of lossy compression. Similarly, the vector space model (Chapter 6 ) and dimensionality reduction techniques like latent semantic indexing (Chapter 18 ) create compact representations from which we cannot fully restore the original collection. Lossy compression makes sense when the ``lost'' information is unlikely ever to be used by the search system. For example, web search is characterized by a large number of documents, short queries, and users who only look at the first few pages of results. As a consequence, we can discard postings of documents that would only be used for hits far down the list. Thus, there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness. Before introducing techniques for compressing the dictionary, we want to estimate the number of distinct terms M in a collection. It is sometimes said that languages have a vocabulary of a certain size. The second edition of the Oxford English Dictionary (OED) defines more than 600,000 words. But the vocabulary of most large collections is much larger than the OED. The OED does not include most names of people, locations, products, or scientific entities like genes. These names need to be included in the inverted index, so our users can search for them. Heaps' law: Estimating the number of terms. Heaps' law.Vocabulary size M as a function of collection size T (number of tokens) for Reuters-RCV1. For these data, the dashed line {10} M = 0.49* {10} T + 1.64 is the best least-squares fit. A better way of getting a handle on M is Heaps' law , which estimates vocabulary size as a function of collection size: where T is the number of tokens in the collection. Typical values for the parameters. The motivation for Heaps' law is that the simplest possible relationship between collection size and vocabulary size is linear in log-log space and the assumption of linearity is usually born out in practice as shown in Figure 5.1 for Reuters-RCV1. In this case, the fit is excellent for T>10^5=100{,}000, for the parameter values b= 0.49 and k=44. For example, for the first 1,000,020 tokens Heaps' law predicts 38,323 terms: The actual number is 38,365 terms, very close to the prediction. The parameter k is quite variable because vocabulary growth depends a lot on the nature of the collection and how it is processed. Case-folding and stemming reduce the growth rate of the vocabulary, whereas including numbers and spelling errors increase it. Regardless of the values of the parameters for a particular collection, Heaps' law suggests that (i) the dictionary size continues to increase with more documents in the collection, rather than a maximum vocabulary size being reached, and (ii) the size of the dictionary is quite large for large collections. These two hypotheses have been empirically shown to be true of large text collections (Section 5.4 ). So dictionary compression is important for an effective information retrieval system. Zipf's law: Modeling the distribution of terms. We also want to understand how terms are distributed across documents. This helps us to characterize the properties of the algorithms for compressing postings lists in Section 5.3 . A commonly used model of the distribution of terms in a collection is Zipf's law . It states that, if t 1 is the most common term in the collection, t 2 is the next most common, and so on, then the collection frequency i of the ith most common term is proportional to 1/i. So if the most frequent term occurs 1 times, then the second most frequent term has half as many occurrences, the third most frequent term a third as many occurrences, and so on. The intuition is that frequency decreases very rapidly with rank. Equation 3 is one of the simplest ways of formalizing such a rapid decrease and it has been found to be a reasonably good model. Equivalently, we can write Zipf's law as i = c i^k or as i = c + k i where k=-1 and c is a constant to be defined in Section 5.3.2 . It is therefore a power law with exponent k=-1. See Chapter 19 , page 19.2.1 , for another power law, a law characterizing the distribution of links on web pages. Zipf's law for Reuters-RCV1. Frequency is plotted as a function of frequency rank for the terms in the collection. The line is the distribution predicted by Zipf's law (weighted least-squares fit; intercept is 6.95). The log-log graph in Figure 5.2 plots the collection frequency of a term as a function of its rank for Reuters-RCV1. A line with slope -1, corresponding to the Zipf function i = c - i, is also shown. The fit of the data to the law is not particularly good, but good enough to serve as a model for term distributions in our calculations in Section 5.3 ."""
iir 5 2,Dictionary compression,"Search engine indexing,Compression",,"This section presents a series of dictionary data structures that achieve increasingly higher compression ratios. The dictionary is small compared with the postings file as suggested by Table 5.1 . So why compress it if it is responsible for only a small percentage of the overall space requirements of the IR system? One of the primary factors in determining the response time of an IR system is the number of disk seeks necessary to process a query. If parts of the dictionary are on disk, then many more disk seeks are necessary in query evaluation. Thus, the main goal of compressing the dictionary is to fit it in main memory, or at least a large portion of it, to support high query throughput. Although dictionaries of very large collections fit into the memory of a standard desktop machine, this is not true of many other application scenarios. For example, an enterprise search server for a large corporation may have to index a multiterabyte collection with a comparatively large vocabulary because of the presence of documents in many different languages. We also want to be able to design search systems for limited hardware such as mobile phones and onboard computers. Other reasons for wanting to conserve memory are fast startup time and having to share resources with other applications. The search system on your PC must get along with the memory-hogging word processing suite you are using at the same time. Dictionary as a string. The simplest data structure for the dictionary is to sort the vocabulary lexicographically and store it in an array of fixed-width entries as shown in Figure 5.3 . Assuming a Unicode representation, we allocate 2 20We allocate 20 bytes for the term itself (because few terms have more than twenty characters in English), 4 bytes for its document frequency, and 4 bytes for the pointer to its postings list. Four-byte pointers resolve a 4 gigabytes (GB) address space. For large collections like the web, we need to allocate more bytes per pointer. We look up terms in the array by binary search. Dictionary-as-a-string storage.Pointers mark the end of the preceding term and the beginning of the next. For example, the first three terms in this example are systile, syzygetic, and syzygial. Using fixed-width entries for terms is clearly wasteful. The average length of a term in English is about eight characters icompresstb1, so on average we are wasting twelve characters (or 24 bytes) in the fixed-width scheme. Also, we have no way of storing terms with more than twenty characters like hydrochlorofluorocarbons and supercalifragilisticexpialidocious. We can overcome these shortcomings by storing the dictionary terms as one long string of characters, as shown in Figure 5.4 . The pointer to the next term is also used to demarcate the end of the current term. As before, we locate terms in the data structure by way of binary search in the (now smaller) table. This scheme saves us 60% compared to fixed-width storage - 24 bytes on average of the 40 bytes 12 bytes on average of the 20 bytes we allocated for terms before. However, we now also need to store term pointers. The term pointers resolve 400{,}000 8 = 3.2 10^6 positions, so they need to be 2 3.2 10^6 22 bits or 3 bytes long. In this new scheme, we need 400{,}000 (4+4+3+{2}{} 8) = {10.8}{7.6} for the Reuters-RCV1 dictionary: 4 bytes each for frequency and postings pointer, 3 bytes for the term pointer, and {2}{} 8 bytes on average for the term. So we have reduced the space requirements by one third from 19.211.2 to 10.87.6 MB. Blocked storage with four terms per block.The first block consists of systile, syzygetic, syzygial, and syzygy with lengths of seven, nine, eight, and six characters, respectively. Each term is preceded by a byte encoding its length that indicates how many bytes to skip to reach subsequent terms. Blocked storage. We can further compress the dictionary by grouping terms in the string into of size k and keeping a term pointer only for the first term of each block (Figure 5.5 ). We store the length of the term in the string as an additional byte at the beginning of the term. We thus eliminate k-1 term pointers, but need an additional k bytes for storing the length of each term. For k=4, we save (k-1) 3 = 9 bytes for term pointers, but need an additional k=4 bytes for term lengths. So the total space requirements for the dictionary of Reuters-RCV1 are reduced by 5 bytes per four-term block, or a total of 400{,}000 1/4 5 = 0.5 , bringing us down to 10.37.1 MB. By increasing the block size k, we get better compression. However, there is a tradeoff between compression and the speed of term lookup. For the eight-term dictionary in Figure 5.6 , steps in binary search are shown as double lines and steps in list search as simple lines. We search for terms in the uncompressed dictionary by binary search (a). In the compressed dictionary, we first locate the term's block by binary search and then its position within the list by linear search through the block (b). Searching the uncompressed dictionary in (a) takes on average (0+1+2+3+2+1+2+2)/8 1.6 steps, assuming each term is equally likely to come up in a query. For example, finding the two terms, aid and box, takes three and two steps, respectively. With blocks of size k=4 in (b), we need (0+1+2+3+4+1+2+3)/8 =2 steps on average, !25% more. For example, finding den takes one binary search step and two steps through the block. By increasing k, we can get the size of the compressed dictionary arbitrarily close to the minimum of 400{,}000 (4+4+1+{2 }{} 8) = {10}{6.8} , but term lookup becomes prohibitively slow for large values of k. One source of redundancy in the dictionary we have not exploited yet is the fact that consecutive entries in an alphabetically sorted list share common prefixes. This observation leads to front coding (Figure 5.7 ). A common prefix is identified for a subsequence of the term list and then referred to with a special character. In the case of Reuters, front coding saves another 2.41.2 MB, as we found in an experiment. Other schemes with even greater compression rely on minimal perfect hashing, that is, a hash function that maps M terms onto [1,,M] without collisions. However, we cannot adapt perfect hashes incrementally because each new term causes a collision and therefore requires the creation of a new perfect hash function. Therefore, they cannot be used in a dynamic environment. Even with the best compression scheme, it may not be feasible to store the entire dictionary in main memory for very large text collections and for hardware with limited memory. If we have to partition the dictionary onto pages that are stored on disk, then we can index the first term of each page using a B-tree. For processing most queries, the search system has to go to disk anyway to fetch the postings. One additional seek for retrieving the term's dictionary page from disk is a significant, but tolerable increase in the time it takes to process a query. Table 5.2 summarizes the compression achieved by the four dictionary data structures."""
iir 5 3,Postings file compression,"Tf „ idf, Vector space model","VARIABLE BYTE ENCODING, CONTINUATION BIT, NIBBLE, UNARY CODE, Ôäêë ENCODING, ENTROPY, UNIVERSAL CODE, PREFIX FREE, PARAMETER FREE","Recall from Table 4.2 (page 4.2 ) that Reuters-RCV1 has 800,000 documents, 200 tokens per document, six characters per token, and 100,000,000 postings where we define a posting in this chapter as a docID in a postings list, that is, excluding frequency and position information. These numbers correspond to line 3 (``case folding'') in Table 5.1 . Document identifiers are 2 800{,}000 20 bits long. Thus, the size of the collection is about 800{,}000 200 6 = 960 and the size of the uncompressed postings file is 100{,}000{,}000 20/ 8 = 250 . To devise a more efficient representation of the postings file, one that uses fewer than 20 bits per document, we observe that the postings for frequent terms are close together. Imagine going through the documents of a collection one by one and looking for a frequent term like computer. We will find a document containing computer, then we skip a few documents that do not contain it, then there is again a document with the term and so on (see Table 5.3 ). The key idea is that the gaps between postings are short, requiring a lot less space than 20 bits to store. In fact, gaps for the most frequent terms such as the and for are mostly equal to 1. But the gaps for a rare term that occurs only once or twice in a collection (e.g., arachnocentric in Table 5.3 ) have the same order of magnitude as the docIDs and need 20 bits. For an economical representation of this distribution of gaps, we need a variable encoding method that uses fewer bits for short gaps. To encode small numbers in less space than large numbers, we look at two types of methods: bytewise compression and bitwise compression. As the names suggest, these methods attempt to encode gaps with the minimum number of bytes and bits, respectively. Variable byte codes. Variable byte encoding uses an integral number of bytes to encode a gap. The last 7 bits of a byte are ``payload'' and encode part of the gap. The first bit of the byte is a continuation bit . It is set to 1 for the last byte of the encoded gap and to 0 otherwise. To decode a variable byte code, we read a sequence of bytes with continuation bit 0 terminated by a byte with continuation bit 1. We then extract and concatenate the 7-bit parts. Figure 5.8 gives pseudocode for VB encoding and decoding and Table 5.4 an example of a VB-encoded postings list. With VB compression, the size of the compressed index for Reuters-RCV1 is 116 MB as we verified in an experiment. This is a more than 50% reduction of the size of the uncompressed index (see Table 5.6 ). The idea of VB encoding can also be applied to larger or smaller units than bytes: 32-bit words, 16-bit words, and 4-bit words or nibbles . Larger words further decrease the amount of bit manipulation necessary at the cost of less effective (or no) compression. Word sizes smaller than bytes get even better compression ratios at the cost of more bit manipulation. In general, bytes offer a good compromise between compression ratio and speed of decompression. For most IR systems variable byte codes offer an excellent tradeoff between time and space. They are also simple to implement - most of the alternatives referred to in Section 5.4 are more complex. But if disk space is a scarce resource, we can achieve better compression ratios by using bit-level encodings, in particular two closely related encodings: codes, which we will turn to next, and codes (Exercise 5.3.2 ). Gamma codes. VB codes use an adaptive number of bytes depending on the size of the gap. Bit-level codes adapt the length of the code on the finer grained bit level. The simplest bit-level code is unary code . The unary code of n is a string of n 1s followed by a 0 (see the first two columns of Table 5.5 ). Obviously, this is not a very efficient code, but it will come in handy in a moment. How efficient can a code be in principle? Assuming the 2^n gaps G with 1 G 2^{n} are all equally likely, the optimal encoding uses n bits for each G. So some gaps (G=2^{n} in this case) cannot be encoded with fewer than 2 G bits. Our goal is to get as close to this lower bound as possible. A method that is within a factor of optimal is encoding . codes implement variable-length encoding by splitting the representation of a gap G into a pair of length and offset. Offset is G in binary, but with the leading 1 removed.[*] For example, for 13 (binary 1101) offset is 101. Length encodes the length of offset in unary code. For 13, the length of offset is 3 bits, which is 1110 in unary. The code of 13 is therefore 1110101, the concatenation of length 1110 and offset 101. The right hand column of Table 5.5 gives additional examples of codes. A code is decoded by first reading the unary code up to the 0 that terminates it, for example, the four bits 1110 when decoding 1110101. Now we know how long the offset is: 3 bits. The offset 101 can then be read correctly and the 1 that was chopped off in encoding is prepended: 101 1101 = 13. The length of offset is 2 G bits and the length of length is 2 G +1 bits, so the length of the entire code is 2 2 G +1 bits. codes are always of odd length and they are within a factor of 2 of what we claimed to be the optimal encoding length 2 G. We derived this optimum from the assumption that the 2^n gaps between 1 and 2^{n} are equiprobable. But this need not be the case. In general, we do not know the probability distribution over gaps a priori. The characteristic of a discrete probability distribution[*] P that determines its coding properties (including whether a code is optimal) is its entropy H(P), which is defined as follows. where X is the set of all possible numbers we need to be able to encode (and therefore {x X} P(x) = 1.0). Entropy is a measure of uncertainty as shown in Figure 5.9 for a probability distribution P over two possible outcomes, namely, X = . Entropy is maximized (H(P)=1) for P(x 1) = P(x 2) = 0.5 when uncertainty about which x i will appear next is largest; and minimized (H(P)=0) for P(x 1) =1, P(x 2)=0 and for P(x 1) =0, P(x 2)=1 when there is absolute certainty. It can be shown that the lower bound for the expected length E(L) of a code L is H(P) if certain conditions hold (see the references). It can further be shown that for 1 < H(P) < , encoding is within a factor of 3 of this optimal encoding, approaching 2 for large H(P). What is remarkable about this result is that it holds for any probability distribution P. So without knowing anything about the properties of the distribution of gaps, we can apply codes and be certain that they are within a factor of !2 of the optimal code for distributions of large entropy. A code like code with the property of being within a factor of optimal for an arbitrary distribution P is called universal . In addition to universality, codes have two other properties that are useful for index compression. First, they are prefix free , namely, no code is the prefix of another. This means that there is always a unique decoding of a sequence of codes - and we do not need delimiters between them, which would decrease the efficiency of the code. The second property is that codes are parameter free . For many other efficient codes, we have to fit the parameters of a model (e.g., the binomial distribution) to the distribution of gaps in the index. This complicates the implementation of compression and decompression. For instance, the parameters need to be stored and retrieved. And in dynamic indexing, the distribution of gaps can change, so that the original parameters are no longer appropriate. These problems are avoided with a parameter-free code. How much compression of the inverted index do codes achieve? To answer this question we use Zipf's law, the term distribution model introduced in Section 5.1.2 . According to Zipf's law, the collection frequency i is proportional to the inverse of the rank i, that is, there is a constant c' such that. We can choose a different constant c such that the fractions c/i are relative frequencies and sum to 1. where M is the number of distinct terms and H M is the Mth harmonic number . [*] Reuters-RCV1 has M= 400{,}000 distinct terms. Thus the ith term has a relative frequency of roughly 1/(13i), and the expected average number of occurrences of term i in a document of length. where we interpret the relative frequency as a term occurrence probability. Recall that 200 is the average number of tokens per document in Reuters-RCV1 (Table 4.2 ). Now we have derived term statistics that characterize the distribution of terms in the collection and, by extension, the distribution of gaps in the postings lists. From these statistics, we can calculate the space requirements for an inverted index compressed with encoding. We first stratify the vocabulary into blocks of size L c=15. On average, term i occurs 15/i times per document. So the average number of occurrences {f} per document is 1 {f} for terms in the first block, corresponding to a total number of N gaps per term. The average is {1}{2} {f} < 1 for terms in the second block, corresponding to N/2 gaps per term, and {1}{3} {f} < {1}{2} for terms in the third block, corresponding to N/3 gaps per term, and so on. (We take the lower bound because it simplifies subsequent calculations. As we will see, the final estimate is too pessimistic, even with this assumption.) We will make the somewhat unrealistic assumption that all gaps for a given term have the same size as shown in Figure 5.10. Assuming such a uniform distribution of gaps, we then have gaps of size 1 in block 1, gaps of size 2 in block 2, and so on. Encoding the N/j gaps of size j with codes, the number of bits needed for the postings list of a term in the jth block (corresponding to one row in the figure). So the postings file of the compressed inverted index for our 960 MB collection has a size of 224 MB, one fourth the size of the original collection. When we run compression on Reuters-RCV1, the actual size of the compressed index is even lower: 101 MB, a bit more than one tenth of the size of the collection. The reason for the discrepancy between predicted and actual value is that (i) Zipf's law is not a very good approximation of the actual distribution of term frequencies for Reuters-RCV1 and (ii) gaps are not uniform. The Zipf model predicts an index size of 251 MB for the unrounded numbers from Table 4.2 . If term frequencies are generated from the Zipf model and a compressed index is created for these artificial terms, then the compressed size is 254 MB. So to the extent that the assumptions about the distribution of term frequencies are accurate, the predictions of the model are correct. Table 5.6 summarizes the compression techniques covered in this chapter. The term incidence matrix (Figure 1.1 , page 1.1 ) for Reuters-RCV1 has size 400{,}000 800{,}000 = 40 8 10^9 bits or 40 GB. The numbers were the collection (3600 MB and 960 MB) are for the encoding of RCV1 of CD, which uses one byte per character, not Unicode. codes achieve great compression ratios - about 15% better than variable byte codes for Reuters-RCV1. But they are expensive to decode. This is because many bit-level operations - shifts and masks - are necessary to decode a sequence of codes as the boundaries between codes will usually be somewhere in the middle of a machine word. As a result, query processing is more expensive for codes than for variable byte codes. Whether we choose variable byte or encoding depends on the characteristics of an application, for example, on the relative weights we give to conserving disk space versus maximizing query response time. The compression ratio for the index in Table 5.6 is about 25%: 400 MB (uncompressed, each posting stored as a 32-bit word) versus 101 MB ( ) and 116 MB (VB). This shows that both and VB codes meet the objectives we stated in the beginning of the chapter. Index compression substantially improves time and space efficiency of indexes by reducing the amount of disk space needed, increasing the amount of information that can be kept in the cache, and speeding up data transfers from disk to memory."""
iir 6,"Scoring, term weighting and the vector space model","Tf „ idf, Vector space model",,"Thus far we have dealt with indexes that support Boolean queries: a document either matches or does not match a query. In the case of large document collections, the resulting number of matching documents can far exceed the number a human user could possibly sift through. Accordingly, it is essential for a search engine to rank-order the documents matching a query. To do this, the search engine computes, for each matching document, a score with respect to the query at hand. In this chapter we initiate the study of assigning a score to a (query, document) pair. This chapter consists of three main ideas. We introduce parametric and zone indexes in Section 6.1 , which serve two purposes. First, they allow us to index and retrieve documents by metadata such as the language in which a document is written. Second, they give us a simple means for scoring (and thereby ranking) documents in response to a query. Next, in Section 6.2 we develop the idea of weighting the importance of a term in a document, based on the statistics of occurrence of the term. In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document. This view is known as vector space scoring. Section 6.4 develops several variants of term-weighting for the vector space model. Chapter 7 develops computational aspects of vector space scoring, and related topics. As we develop these ideas, the notion of a query will assume multiple nuances. In Section 6.1 we consider queries in which specific query terms occur in specified regions of a matching document. Beginning Section 6.2 we will in fact relax the requirement of matching specific regions of a document; instead, we will look at so-called free text queries that simply consist of query terms with no specification on their relative order, importance or where in a document they should be found. The bulk of our study of scoring will be in this latter notion of a query being such a set of terms."""
iir 6 1,Parametric and zone indexes,"Tf „ idf, Vector space model","RANKED BOOLEAN retrieval,MACHINE-LEARNED relevance","We have thus far viewed a document as a sequence of terms. In fact, most documents have additional structure. Digital documents generally encode, in machine-recognizable form, certain metadata associated with each document. By metadata, we mean specific forms of data about a document, such as its author(s), title and date of publication. This metadata would generally include fields such as the date of creation and the format of the document, as well the author and possibly the title of the document. The possible values of a field should be thought of as finite - for instance, the set of all dates of authorship. Consider queries of the form ``find documents authored by William Shakespeare in 1601, containing the phrase alas poor Yorick''. Query processing then consists as usual of postings intersections, except that we may merge postings from standard inverted as well as parametric indexes . There is one parametric index for each field (say, date of creation); it allows us to select only the documents matching a date specified in the query. Figure 6.1 illustrates the user's view of such a parametric search. Some of the fields may assume ordered values, such as dates; in the example query above, the year 1601 is one such field value. The search engine may support querying ranges on such ordered values; to this end, a structure like a B-tree may be used for the field's dictionary. Parametric search.In this example we have a collection with fields allowing us to select publications by zones such as Author and fields such as Language. Zones are similar to fields, except the contents of a zone can be arbitrary free text. Whereas a field may take on a relatively small set of values, a zone can be thought of as an arbitrary, unbounded amount of text. For instance, document titles and abstracts are generally treated as zones. We may build a separate inverted index for each zone of a document, to support queries such as ``find documents with merchant in the title and william in the author list and the phrase gentle rain in the body''. This has the effect of building an index that looks like Figure 6.2. Whereas the dictionary for a parametric index comes from a fixed vocabulary (the set of languages, or the set of dates), the dictionary for a zone index must structure whatever vocabulary stems from the text of that zone. In fact, we can reduce the size of the dictionary by encoding the zone in which a term occurs in the postings. In Figure 6.3 for instance, we show how occurrences of william in the title and author zones of various documents are encoded. Such an encoding is useful when the size of the dictionary is a concern (because we require the dictionary to fit in main memory). But there is another important reason why the encoding of Figure 6.3 is useful: the efficient computation of scores using a technique we will call weighted zone scoring. Weighted zone scoring. Thus far in Section 6.1 we have focused on retrieving documents based on Boolean queries on fields and zones. We now turn to a second application of zones and fields. Given a Boolean query q and a document d, weighted zone scoring assigns to the pair (q,d) a score in the interval [0,1], by computing a linear combination of zone scores, where each zone of the document contributes a Boolean value. More specifically, consider a set of documents each of which has zones. Let g 1, , g [0,1] such that {i=1}^ g i = 1. For 1 i , let s i be the Boolean score denoting a match (or absence thereof) between q and the ith zone. For instance, the Boolean score from a zone could be 1 if all the query term(s) occur in that zone, and zero otherwise; indeed, it could be any Boolean function that maps the presence of query terms in a zone to {0,1}. Then, the weighted zone score is defined to be. Weighted zone scoring is sometimes referred to also as ranked Boolean retrieval . Worked example. Consider the query shakespeare in a collection in which each document has three zones: author, title and body. The Boolean score function for a zone takes on the value 1 if the query term shakespeare is present in the zone, and zero otherwise. Weighted zone scoring in such a collection would require three weights g 1, g 2 and g 3, respectively corresponding to the author, title and body zones. Suppose we set g 1=0.2, g 2=0.3 and g 3=0.5 (so that the three weights add up to 1); this corresponds to an application in which a match in the author zone is least important to the overall score, the title zone somewhat more, and the body contributes even more. Thus if the term shakespeare were to appear in the title and body zones but not the author zone of a document, the score of this document would be 0.8. End worked example. How do we implement the computation of weighted zone scores? A simple approach would be to compute the score for each document in turn, adding in all the contributions from the various zones. However, we now show how we may compute weighted zone scores directly from inverted indexes. The algorithm of Figure 6.4 treats the case when the query q is a two-term query consisting of query terms q 1 and q 2, and the Boolean function is AND: 1 if both query terms are present in a zone and 0 otherwise. Following the description of the algorithm, we describe the extension to more complex queries and Boolean functions. The reader may have noticed the close similarity between this algorithm and that in Figure 1.6 . Indeed, they represent the same postings traversal, except that instead of merely adding a document to the set of results for a Boolean AND query, we now compute a score for each such document. Some literature refers to the array scores[] above as a set of accumulators . The reason for this will be clear as we consider more complex Boolean functions than the AND; thus we may assign a non-zero score to a document even if it does not contain all query terms. Learning weights. How do we determine the weights g i for weighted zone scoring? These weights could be specified by an expert (or, in principle, the user); but increasingly, these weights are ``learned'' using training examples that have been judged editorially. This latter methodology falls under a general class of approaches to scoring and ranking in information retrieval, known as machine-learned relevance . We provide a brief introduction to this topic here because weighted zone scoring presents a clean setting for introducing it; a complete development demands an understanding of machine learning and is deferred to Chapter 15 . We are provided with a set of training examples, each of which is a tuple consisting of a query q and a document d, together with a relevance judgment for d on q. In the simplest form, each relevance judgments is either Relevant or Non-relevant. More sophisticated implementations of the methodology make use of more nuanced judgments. The weights g i are then ``learned'' from these examples, in order that the learned scores approximate the relevance judgments in the training examples. For weighted zone scoring, the process may be viewed as learning a linear function of the Boolean match scores contributed by the various zones. The expensive component of this methodology is the labor-intensive assembly of user-generated relevance judgments from which to learn the weights, especially in a collection that changes frequently (such as the Web). We now detail a simple example that illustrates how we can reduce the problem of learning the weights g i to a simple optimization problem. We now consider a simple case of weighted zone scoring, where each document has a title zone and a body zone. Given a query q and a document d, we use the given Boolean match function to compute Boolean variables s T(d,q) and s B(d,q), depending on whether the title (respectively, body) zone of d matches query q. For instance, the algorithm in Figure 6.4 uses an AND of the query terms for this Boolean function. We will compute a score between 0 and 1 for each (document, query) pair using s T(d,q) and s B(d,q) by using a constant. We now describe how to determine the constant g from a set of training examples, each of which is a triple of the form ). In each training example, a given training document d j and a given training query q j are assessed by a human editor who delivers a relevance judgment r(d j, q j) that is either Relevant or Non-relevant. This is illustrated in Figure 6.5 , where seven training examples are shown. For each training example and s B(d j,q j) that we use to compute a score from (14). We now compare this computed score to the human relevance judgment for the same document-query pair (d j,q j); to this end, we will quantize each Relevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose that we define the error of the scoring function with weight. where we have quantized the editorial relevance judgment r(d j, q j) to 0 or 1. Then, the total error of a set of training examples is given by. The problem of learning the constant g from the given training examples then reduces to picking the value of g that minimizes the total error in (17). Picking the best value of g in (17) in the formulation of Section 6.1.3 reduces to the problem of minimizing a quadratic function of g over the interval [0,1]. This reduction is detailed in Section 6.1.3 . The optimal weight g. We begin by noting that for any training example j for which s T(d j,q j)=0 and s B(d j,q j)=1, the score computed by Equation 14 is 1-g. In similar fashion, we may write down the score computed by Equation 14 for the three other possible combinations of s T(d j,q j) and s B(d j,q j); this is summarized in Figure 6.6 . Let n {01r} (respectively, n {01n}) denote the number of training examples for which s T(d j,q j)=0 and s B(d j,q j)=1 and the editorial judgment is Relevant (respectively, Non-relevant). Then the contribution to the total error in Equation 17 from training examples for which s T(d j,q j)=0 and s B(d j,q j)=1. By writing in similar fashion the error contributions from training examples of the other three combinations of values for s T(d j,q j) and s B(d j,q j) (and extending the notation in the obvious manner), the total error corresponding to Equation 17. By differentiating Equation 19 with respect to g and setting the result to zero, it follows that the optimal value of g. """
iir 6 2,Term frequency and weighting,"Tf „ idf, Vector space model","TERM FREQUENCY,BAG OF WORDS,document frequency, inverse document frequency,document vector","Thus far, scoring has hinged on whether or not a query term is present in a zone within a document. We take the next logical step: a document or zone that mentions a query term more often has more to do with that query and therefore should receive a higher score. To motivate this, we recall the notion of a free text query introduced in Section 1.4 : a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators (such as Boolean operators). This query style, which is extremely popular on the web, views the query as simply a set of words. A plausible scoring mechanism then is to compute a score that is the sum, over the query terms, of the match scores between each query term and the document. Towards this end, we assign to each term in a document a weight for that term, that depends on the number of occurrences of the term in the document. We would like to compute a score between a query term t and a document d, based on the weight of t in d. The simplest approach is to assign the weight to be equal to the number of occurrences of term t in document d. This weighting scheme is referred to as term frequency and is denoted {tf} {t,d}, with the subscripts denoting the term and the document in order. For a document d, the set of weights determined by the {tf} weights above (or indeed any weighting function that maps the number of occurrences of t in d to a positive real value) may be viewed as a quantitative digest of that document. In this view of a document, known in the literature as the bag of words model , the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material (in contrast to Boolean retrieval). We only retain information on the number of occurrences of each term. Thus, the document ``Mary is quicker than John'' is, in this view, identical to the document ``John is quicker than Mary''. Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content. We will develop this intuition further in Section 6.3 . Before doing so we first study the question: are all words in a document equally important? Clearly not; in Section 2.2.2 we looked at the idea of stop words - words that we decide not to index at all, and therefore do not contribute in any way to retrieval and scoring. Inverse document frequency. Raw term frequency as above suffers from a critical problem: all terms are considered equally important when it comes to assessing relevancy on a query. In fact certain terms have little or no discriminating power in determining relevance. For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. To this end, we introduce a mechanism for attenuating the effect of terms that occur too often in the collection to be meaningful for relevance determination. An immediate idea is to scale down the term weights of terms with high collection frequency, defined to be the total number of occurrences of a term in the collection. The idea would be to reduce the {tf} weight of a term by a factor that grows with its collection frequency. Instead, it is more commonplace to use for this purpose the document frequency {df} t, defined to be the number of documents in the collection that contain a term t. This is because in trying to discriminate between documents for the purpose of scoring it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term. The reason to prefer df to cf is illustrated in Figure 6.7 , where a simple example shows that collection frequency (cf) and document frequency (df) can behave rather differently. In particular, the cf values for both try and insurance are roughly equal, but their df values differ significantly. Intuitively, we want the few documents that contain insurance to get a higher boost for a query on insurance than the many documents containing try get from a query on try. How is the document frequency df of a term used to scale its weight? Denoting as usual the total number of documents in a collection by N, we define the inverse document frequency of a term. Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low. Figure 6.8 gives an example of idf's in the Reuters collection of 806,791 documents; in this example logarithms are to the base 10. In fact, as we will see in Exercise 6.2.2 , the precise base of the logarithm is not material to ranking. We will give on page 11.3.3 a justification of the particular form in Equation 21. Tf-idf weighting. We now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document. The tf-idf weighting scheme assigns to term t a weight in document d. In other words, {tf-idf} {t,d} assigns to term t a weight in document d that is highest when t occurs many times within a small number of documents (thus lending high discriminating power to those documents); lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal); lowest when the term occurs in virtually all documents. At this point, we may view each document as a vector with one component corresponding to each term in the dictionary, together with a weight for each component that is given by (22). For dictionary terms that do not occur in a document, this weight is zero. This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3 . As a first step, we introduce the overlap score measure: the score of a document d is the sum, over all query terms, of the number of times each of the query terms occurs in d. We can refine this idea so that we add up not the number of occurrences of each query term t in d, but instead the tf-idf weight of each term in d. In Section 6.3 we will develop a more rigorous form of Equation 23."""
iir 6 3,The vector space model for scoring,"Tf „ idf, Vector space model","vector space model,cosine similarity, dot product, euclidean length, length normalization, term-document matrix, accumulator, term-at-a-time, document-at-a-time","In Section 6.2 we developed the notion of a document vector that captures the relative importance of the terms in a document. The representation of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval operations ranging from scoring documents on a query, document classification and document clustering. We first develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2 ) of queries as vectors in the same vector space as the document collection. We denote by {V}(d) the vector derived from document d, with one component in the vector for each dictionary term. Unless otherwise specified, the reader may assume that the components are computed using the tf-idf weighting scheme, although the particular weighting scheme is immaterial to the discussion that follows. The set of documents in a collection then may be viewed as a set of vectors in a vector space, in which there is one axis for each term. This representation loses the relative ordering of the terms in each document; recall our example from Section 6.2 , where we pointed out that the documents Mary is quicker than John and John is quicker than Mary are identical in such a bag of words representation. How do we quantify the similarity between two documents in this vector space? A first attempt might consider the magnitude of the vector difference between two document vectors. This measure suffers from a drawback: two documents with very similar content can have a significant vector difference simply because one is much longer than the other. Thus the relative distributions of terms may be identical in the two documents, but the absolute term frequencies of one may be far larger. To compensate for the effect of document length, the standard way of quantifying the similarity between two documents d 1 and d 2 is to compute the cosine similarity of their vector representations {V}(d 1) and {V}(d 2). where the numerator represents the dot product (also known as the inner product ) of the vectors {V}(d 1) and {V}(d 2), while the denominator is the product of their Euclidean lengths . The dot product {x} {y} of two vectors is defined as {i=1}^Mx iy i. Let {V}(d) denote the document vector for d, with M components {V} 1(d) {V} M(d). The Euclidean length of d is defined to be. The effect of the denominator of Equation 24 is thus to length-normalize the vectors {V}(d 1) and {V}(d 2) to unit vectors {v}(d 1)={V}(d 1)/{V}(d 1) and {v}(d 2)={V}(d 2)/{V}(d 2). We can then rewrite. Worked example. Consider the documents in Figure 6.9 . We now apply Euclidean normalization to the tf values from the table, for each of the three documents in the table. The quantity { {i=1}^M{V} i^2(d)} has the values 30.56, 46.84 and 41.30 respectively for Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these documents are shown in Figure 6.11 . End worked example. Thus, (25) can be viewed as the dot product of the normalized versions of the two document vectors. This measure is the cosine of the angle between the two vectors, shown in Figure 6.10 . What use is the similarity measure {sim}(d 1,d 2)? Given a document d (potentially one of the d i in the collection), consider searching for the documents in the collection most similar to d. Such a search is useful in a system where a user may identify a document and seek others like it - a feature available in the results lists of search engines as a more like this feature. We reduce the problem of finding the document(s) most similar to d to that of finding the d i with the highest dot products ({sim} values) {v}(d) {v}(d i). We could do this by computing the dot products between {v}(d) and each of {v}(d 1),,{v}(d N), then picking off the highest resulting {sim} values. Worked example. Figure 6.12 shows the number of occurrences of three terms (affection, jealous and gossip) in each of the following three novels: Jane Austen's Sense and Sensibility (SaS) and Pride and Prejudice (PaP) and Emily BrontÔäêë 's Wuthering Heights (WH). Of course, there are many other terms occurring in each of these novels. In this example we represent each of these novels as a unit vector in three dimensions, corresponding to these three terms (only); we use raw term frequencies here, with no idf multiplier. The resulting weights are as shown in Figure 6.13. Now consider the cosine similarities between pairs of the resulting three-dimensional vectors. A simple computation shows that sim({v}(SAS), {v}(PAP)) is 0.999, whereas sim({v}(SAS), {v}(WH)) is 0.888; thus, the two books authored by Austen (SaS and PaP) are considerably closer to each other than to BrontÔäêë 's Wuthering Heights. In fact, the similarity between the first two is almost perfect (when restricted to the three terms we consider). Here we have considered tf weights, but we could of course use other term weight functions. End worked example. Viewing a collection of N documents as a collection of vectors leads to a natural view of a collection as a term-document matrix and jealousy would under stemming be considered as a single dimension. This matrix view will prove to be useful in Chapter 18 . Queries as vectors. There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q= jealous gossip. This query turns into the unit vector {v}(q)=(0,0.707,0.707) on the three coordinates of Figures 6.12 and 6.13. The key idea now: to assign to each document d a score equal to the dot product. In the example of Figure 6.13, Wuthering Heights is the top-scoring document for this query with a score of 0.509, with Pride and Prejudice a distant second with a score of 0.085, and Sense and Sensibility last with a score of 0.074. This simple example is somewhat misleading: the number of dimensions in practice will be far larger than three: it will equal the vocabulary size M. To summarize, by viewing a query as a ``bag of words'', we are able to treat it as a very short document. As a consequence, we can use the cosine similarity between the query vector and a document vector as a measure of the score of the document for that query. The resulting scores can then be used to select the top-scoring documents for a query. A document may have a high cosine score for a query even if it does not contain all query terms. Note that the preceding discussion does not hinge on any specific weighting of terms in the document vector, although for the present we may think of them as either tf or tf-idf weights. In fact, a number of weighting schemes are possible for query as well as document vectors, as illustrated in Example 6.3.2 and developed further in Section 6.4 . Computing the cosine similarities between the query vector and each document vector in the collection, sorting the resulting scores and selecting the top K documents can be expensive -- a single similarity computation can entail a dot product in tens of thousands of dimensions, demanding tens of thousands of arithmetic operations. In Section 7.1 we study how to use an inverted index for this purpose, followed by a series of heuristics for improving on this. Computing vector scores. In a typical setting we have a collection of documents each represented by a vector, a free text query represented by a vector, and a positive integer K. We seek the K documents of the collection with the highest vector space scores on the given query. We now initiate the study of determining the K documents with the highest vector space scores for a query. Typically, we seek these K top documents in ordered by decreasing score; for instance many search engines use K=10 to retrieve and rank-order the first page of the ten best results. Here we give the basic algorithm for this computation; we develop a fuller treatment of efficient techniques and approximations in Chapter 7 . Figure 6.14 gives the basic algorithm for computing vector space scores. The array Length holds the lengths (normalization factors) for each of the N documents, whereas the array Scores holds the scores for each of the documents. When the scores are finally computed in Step 9, all that remains in Step 10 is to pick off the K documents with the highest scores. The outermost loop beginning Step 3 repeats the updating of Scores, iterating over each query term t in turn. In Step 5 we calculate the weight in the query vector for term t. Steps 6-8 update the score of each document by adding in the contribution from term t. This process of adding in contributions one query term at a time is sometimes known as term-at-a-time scoring or accumulation, and the N elements of the array Scores are therefore known as accumulators . For this purpose, it would appear necessary to store, with each postings entry, the weight {wf} {t,d} of term t in document d (we have thus far used either tf or tf-idf for this weight, but leave open the possibility of other functions to be developed in Section 6.4 ). In fact this is wasteful, since storing this weight may require a floating point number. Two ideas help alleviate this space problem. First, if we are using inverse document frequency , we need not precompute {idf} t; it suffices to store N/{df} t at the head of the postings for t. Second, we store the term frequency {tf} {t,d} for each postings entry. Finally, Step 12 extracts the top K scores - this requires a priority queue data structure, often implemented using a heap. Such a heap takes no more than 2N comparisons to construct, following which each of the K top scores can be extracted from the heap at a cost of O( N) comparisons. Note that the general algorithm of Figure 6.14 does not prescribe a specific implementation of how we traverse the postings lists of the various query terms; we may traverse them one term at a time as in the loop beginning at Step 3, or we could in fact traverse them concurrently as in Figure 1.6 . In such a concurrent postings traversal we compute the scores of one document at a time, so that it is sometimes called document-at-a-time scoring. We will say more about this in Section 7.1.5 ."""
iir 6 4,Variant tf-idf functions,,"SMOOTHING, PIVOTED DOCUMENT LENGTH NORMALIZATION, EUCLIDEAN DISTANCE","For assigning a weight for each term in each document, a number of alternatives to tf and tf-idf have been considered. We discuss some of the principal ones here; a more complete development is deferred to Chapter 11 . We will summarize these alternatives in Section 6.4.3. Sublinear tf scaling. It seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence. Accordingly, there has been considerable research into variants of term frequency that go beyond counting the number of occurrences of a term. A common modification is to use instead the logarithm of the term frequency, which assigns a weight. In this form, we may replace {tf} by some other function. Equation (23) can then be modified by replacing tf-idf by wf-idf as defined in (29). Maximum tf normalization. One well-studied technique is to normalize the tf weights of all terms occurring in a document by the maximum tf in that document. For each document d, let {tf} {}(d)= { d} {tf} {,d}, where ranges over all terms in d. Then, we compute a normalized term frequency for each term t in document d. where a is a value between 0 and 1 and is generally set to 0.4, although some early work used the value 0.5. The term a in (30) is a smoothing term whose role is to damp the contribution of the second term - which may be viewed as a scaling down of tf by the largest tf value in d. We will encounter smoothing further in Chapter 13 when discussing classification; the basic idea is to avoid a large swing in {ntf} {t,d} from modest changes in {tf} {t,d} (say from 1 to 2). The main idea of maximum tf normalization is to mitigate the following anomaly: we observe higher term frequencies in longer documents, merely because longer documents tend to repeat the same words over and over again. To appreciate this, consider the following extreme example: supposed we were to take a document d and create a new document d' by simply appending a copy of d to itself. While d' should be no more relevant to any query than d is, the use of (23) would assign it twice as high a score as d. Replacing {tf-idf} {t,d} in (23) by {ntf-idf} {t,d} eliminates the anomaly in this example. Maximum tf normalization does suffer from the following issues: The method is unstable in the following sense: a change in the stop word list can dramatically alter term weightings (and therefore ranking). Thus, it is hard to tune. A document may contain an outlier term with an unusually large number of occurrences of that term, not representative of the content of that document. More generally, a document in which the most frequent term appears roughly as often as many other terms should be treated differently from one with a more skewed distribution. Document and query weighting schemes. Equation 27 is fundamental to information retrieval systems that use any form of vector space scoring. Variations from one vector space scoring method to another hinge on the specific choices of weights in the vectors {V}(d) and {V}(q). Figure 6.15 lists some of the principal weighting schemes in use for each of {V}(d) and {V}(q), together with a mnemonic for representing a specific combination of weights; this system of mnemonics is sometimes called SMART notation, following the authors of an early text retrieval system. The mnemonic for representing a combination of weights takes the form ddd.qqq where the first triplet gives the term weighting of the document vector, while the second triplet gives the weighting in the query vector. The first letter in each triplet specifies the term frequency component of the weighting, the second the document frequency component, and the third the form of normalization used. It is quite common to apply different normalization functions to {V}(d) and {V}(q). For example, a very standard weighting scheme is lnc.ltc, where the document vector has log-weighted term frequency, no idf (for both effectiveness and efficiency reasons), and cosine normalization, while the query vector uses log-weighted term frequency, idf weighting, and cosine normalization. Pivoted normalized document length. In Section 6.3.1 we normalized each document vector by the Euclidean length of the vector, so that all document vectors turned into unit vectors. In doing so, we eliminated all information on the length of the original document; this masks some subtleties about longer documents. First, longer documents will - as a result of containing more terms - have higher tf values. Second, longer documents contain more distinct terms. These factors can conspire to raise the scores of longer documents, which (at least for some information needs) is unnatural. Longer documents can broadly be lumped into two categories: (1) verbose documents that essentially repeat the same content - in these, the length of the document does not alter the relative weights of different terms; (2) documents covering multiple different topics, in which the search terms probably match small segments of the document but not all of it - in this case, the relative weights of terms are quite different from a single short document that matches the query terms. Compensating for this phenomenon is a form of document length normalization that is independent of term and document frequencies. To this end, we introduce a form of normalizing the vector representations of documents in the collection, so that the resulting ``normalized'' documents are not necessarily of unit length. Then, when we compute the dot product score between a (unit) query vector and such a normalized document, the score is skewed to account for the effect of document length on relevance. This form of compensation for document length is known as pivoted document length normalization . Consider a document collection together with an ensemble of queries for that collection. Suppose that we were given, for each query q and for each document d, a Boolean judgment of whether or not d is relevant to the query q; in Chapter 8 we will see how to procure such a set of relevance judgments for a query ensemble and a document collection. Given this set of relevance judgments, we may compute a probability of relevance as a function of document length, averaged over all queries in the ensemble. The resulting plot may look like the curve drawn in thick lines in Figure 6.16 . To compute this curve, we bucket documents by length and compute the fraction of relevant documents in each bucket, then plot this fraction against the median document length of each bucket. (Thus even though the ``curve'' in Figure 6.16 appears to be continuous, it is in fact a histogram of discrete buckets of document length.). On the other hand, the curve in thin lines shows what might happen with the same documents and query ensemble if we were to use relevance as prescribed by cosine normalization Equation 27 - thus, cosine normalization has a tendency to distort the computed relevance vis-Ôäêë Ôäêë -vis the true relevance, at the expense of longer documents. The thin and thick curves crossover at a point p corresponding to document length p, which we refer to as the pivot length; dashed lines mark this point on the x- and y- axes. The idea of pivoted document length normalization would then be to ``rotate'' the cosine normalization curve counter-clockwise about p so that it more closely matches thick line representing the relevance vs. document length curve. As mentioned at the beginning of this section, we do so by using in Equation 27 a normalization factor for each document vector {V}(d) that is not the Euclidean length of that vector, but instead one that is larger than the Euclidean length for documents of length less than p, and smaller for longer documents. To this end, we first note that the normalizing term for {V}(d) in the denominator of Equation 27 is its Euclidean length, denoted {V}(d). In the simplest implementation of pivoted document length normalization, we use a normalization factor in the denominator that is linear in {V}(d), but one of slope <1 as in Figure 6.17 . In this figure, the x- axis represents {V}(d), while the y-axis represents possible normalization factors we can use. The thin line y=x depicts the use of cosine normalization. Notice the following aspects of the thick line representing pivoted length normalization: It is linear in the document length and has the form where {piv} is the cosine normalization value at which the two curves intersect. Its slope is a<1 and (3) it crosses the y=x line at piv. It has been argued that in practice, Equation 31 is well approximated by where u d is the number of unique terms in document d. Of course, pivoted document length normalization is not appropriate for all applications. For instance, in a collection of answers to frequently asked questions (say, at a customer service website), relevance may have little to do with document length. In other cases the dependency may be more complex than can be accounted for by a simple linear pivoted normalization. In such cases, document length can be used as a feature in the machine learning based scoring approach of Section 6.1.2 ."""
iir 7 1,Computing scores in a complete search system,,,"Chapter 6 developed the theory underlying term weighting in documents for the purposes of scoring, leading up to vector space models and the basic cosine scoring algorithm of SectionÔäêë 6.3.3 (pageÔäêë ). In this chapter we begin in Section 7.1 with heuristics for speeding up this computation; many of these heuristics achieve their speed at the risk of not finding quite the top documents matching the query. Some of these heuristics generalize beyond cosine scoring. With Section 7.1 in place, we have essentially all the components needed for a complete search engine. We therefore take a step back from cosine scoring, to the more general problem of computing scores in a search engine. In Section 7.2 we outline a complete search engine, including indexes and structures to support not only cosine scoring but also more general ranking factors such as query term proximity. We describe how all of the various pieces fit together in Section 7.2.4 . We conclude this chapter with Section 7.3 , where we discuss how the vector space model for free text queries interacts with common query operators. We begin by recapping the algorithm of Figure 6.14 . For a query such as q= jealous gossip, two observations are immediate: The unit vector {v}(q) has only two non-zero components. In the absence of any weighting for query terms, these non-zero components are equal - in this case, both equal 0.707. For the purpose of ranking the documents matching this query, we are really interested in the relative (rather than absolute) scores of the documents in the collection. To this end, it suffices to compute the cosine similarity from each document unit vector {v}(d) to {V}(q) (in which all non-zero components of the query vector are set to 1), rather than to the unit vector {v}(q). For any two documents d 1,d 2 (34) For any document d, the cosine similarity {V}(q) {v}(d) is the weighted sum, over all terms in the query q, of the weights of those terms in d. This in turn can be computed by a postings intersection exactly as in the algorithm of Figure 6.14 , with line 8 altered since we take w {t,q} to be 1 so that the multiply-add in that step becomes just an addition; the result is shown in Figure 7.1 . We walk through the postings in the inverted index for the terms in q, accumulating the total score for each document - very much as in processing a Boolean query, except we assign a positive score to each document that appears in any of the postings being traversed. As mentioned in Section 6.3.3 we maintain an idf value for each dictionary term and a tf value for each postings entry. This scheme computes a score for every document in the postings of any of the query terms; the total number of such documents may be considerably smaller than N. Figure 7.1: A faster algorithm for vector space scores. Given these scores, the final step before presenting results to a user is to pick out the K highest-scoring documents. While one could sort the complete set of scores, a better approach is to use a heap to retrieve only the top K documents in order. Where J is the number of documents with non-zero cosine scores, constructing such a heap can be performed in 2J comparison steps, following which each of the K highest scoring documents can be ``read off'' the heap with J comparison steps. Thus far, we have focused on retrieving precisely the K highest-scoring documents for a query. We now consider schemes by which we produce K documents that are likely to be among the K highest scoring documents for a query. In doing so, we hope to dramatically lower the cost of computing the K documents we output, without materially altering the user's perceived relevance of the top K results. Consequently, in most applications it suffices to retrieve K documents whose scores are very close to those of the K best. In the sections that follow we detail schemes that retrieve K such documents while potentially avoiding computing scores for most of the N documents in the collection. Such inexact top-K retrieval is not necessarily, from the user's perspective, a bad thing. The top K documents by the cosine measure are in any case not necessarily the K best for the query: cosine similarity is only a proxy for the user's perceived relevance. In Sections 7.1.2 -7.1.6 below, we give heuristics using which we are likely to retrieve K documents with cosine scores close to those of the top K documents. The principal cost in computing the output stems from computing cosine similarities between the query and a large number of documents. Having a large number of documents in contention also increases the selection cost in the final stage of culling the top K documents from a heap. We now consider a series of ideas designed to eliminate a large number of documents without computing their cosine scores. The heuristics have the following two-step scheme: Find a set A of documents that are contenders, where K < A N. A does not necessarily contain the K top-scoring documents for the query, but is likely to have many documents with scores near those of the top K. Return the K top-scoring documents in A. From the descriptions of these ideas it will be clear that many of them require parameters to be tuned to the collection and application at hand; pointers to experience in setting these parameters may be found at the end of this chapter. It should also be noted that most of these heuristics are well-suited to free text queries, but not for Boolean or phrase queries. For a multi-term query q, it is clear we only consider documents containing at least one of the query terms. We can take this a step further using additional heuristics: We only consider documents containing terms whose idf exceeds a preset threshold. Thus, in the postings traversal, we only traverse the postings for terms with high idf. This has a fairly significant benefit: the postings lists of low-idf terms are generally long; with these removed from contention, the set of documents for which we compute cosines is greatly reduced. One way of viewing this heuristic: low-idf terms are treated as stop words and do not contribute to scoring. For instance, on the query catcher in the rye, we only traverse the postings for catcher and rye. The cutoff threshold can of course be adapted in a query-dependent manner. We only consider documents that contain many (and as a special case, all) of the query terms. This can be accomplished during the postings traversal; we only compute scores for documents containing all (or many) of the query terms. A danger of this scheme is that by requiring all (or even many) query terms to be present in a document before considering it for cosine computation, we may end up with fewer than K candidate documents in the output. This issue will discussed further in Section 7.2.1 . The idea of champion lists (sometimes also called fancy lists or top docs) is to precompute, for each term t in the dictionary, the set of the r documents with the highest weights for t; the value of r is chosen in advance. For tf-idf weighting, these would be the r documents with the highest tf values for term t. We call this set of r documents the champion list for term t. Now, given a query q we create a set A as follows: we take the union of the champion lists for each of the terms comprising q. We now restrict cosine computation to only the documents in A. A critical parameter in this scheme is the value r, which is highly application dependent. Intuitively, r should be large compared with K, especially if we use any form of the index elimination described in Section 7.1.2 . One issue here is that the value r is set at the time of index construction, whereas K is application dependent and may not be available until the query is received; as a result we may (as in the case of index elimination) find ourselves with a set A that has fewer than K documents. There is no reason to have the same value of r for all terms in the dictionary; it could for instance be set to be higher for rarer terms. We now further develop the idea of champion lists, in the somewhat more general setting of static quality scores . In many search engines, we have available a measure of quality g(d) for each document d that is query-independent and thus static. This quality measure may be viewed as a number between zero and one. For instance, in the context of news stories on the web, g(d) may be derived from the number of favorable reviews of the story by web surfers. otherindexing provides further discussion on this topic, as does Chapter 21 in the context of web search. The net score for a document d is some combination of g(d) together with the query-dependent score induced (say) by (27). The precise combination may be determined by the learning methods of Section 6.1.2 , to be developed further in Section 15.4.1 ; but for the purposes of our exposition here, let us consider a simple sum: (35) In this simple form, the static quality g(d) and the query-dependent score from (24) have equal contributions, assuming each is between 0 and 1. Other relative weightings are possible; the effectiveness of our heuristics will depend on the specific relative weighting. First, consider ordering the documents in the postings list for each term by decreasing value of g(d). This allows us to perform the postings intersection algorithm of Figure 1.6 . In order to perform the intersection by a single pass through the postings of each query term, the algorithm of Figure 1.6 relied on the postings being ordered by document IDs. But in fact, we only required that all postings be ordered by a single common ordering; here we rely on the g(d) values to provide this common ordering. This is illustrated in Figure 7.2 , where the postings are ordered in decreasing order of g(d). A static quality-ordered index.In this example we assume that Doc1, Doc2 and Doc3 respectively have static quality scores g(1)=0.25, g(2)=0.5, g(3)=1. The first idea is a direct extension of champion lists: for a well-chosen value r, we maintain for each term t a global champion list of the r documents with the highest values for g(d)+{tf-idf} {t,d}. The list itself is, like all the postings lists considered so far, sorted by a common order (either by document IDs or by static quality). Then at query time, we only compute the net scores (35) for documents in the union of these global champion lists. Intuitively, this has the effect of focusing on documents likely to have large net scores. We conclude the discussion of global champion lists with one further idea. We maintain for each term t two postings lists consisting of disjoint sets of documents, each sorted by g(d) values. The first list, which we call high, contains the m documents with the highest tf values for t. The second list, which we call low, contains all other documents containing t. When processing a query, we first scan only the high lists of the query terms, computing net scores for any document on the high lists of all (or more than a certain number of) query terms. If we obtain scores for K documents in the process, we terminate. If not, we continue the scanning into the low lists, scoring documents in these postings lists. This idea is developed further in Section 7.2.1 . In all the postings lists described thus far, we order the documents consistently by some common ordering: typically by document ID but in Section 7.1.4 by static quality scores. As noted at the end of Section 6.3.3 , such a common ordering supports the concurrent traversal of all of the query terms' postings lists, computing the score for each document as we encounter it. Computing scores in this manner is sometimes referred to as document-at-a-time scoring. We will now introduce a technique for inexact top-K retrieval in which the postings are not all ordered by a common ordering, thereby precluding such a concurrent traversal. We will therefore require scores to be ``accumulated'' one term at a time as in the scheme of Figure 6.14 , so that we have term-at-a-time scoring. The idea is to order the documents d in the postings list of term t by decreasing order of {tf} {t,d}. Thus, the ordering of documents will vary from one postings list to another, and we cannot compute scores by a concurrent traversal of the postings lists of all query terms. Given postings lists ordered by decreasing order of {tf} {t,d}, two ideas have been found to significantly lower the number of documents for which we accumulate scores: (1) when traversing the postings list for a query term t, we stop after considering a prefix of the postings list - either after a fixed number of documents r have been seen, or after the value of {tf} {t,d} has dropped below a threshold; (2) when accumulating scores in the outer loop of Figure 6.14 , we consider the query terms in decreasing order of idf, so that the query terms likely to contribute the most to the final scores are considered first. This latter idea too can be adaptive at the time of processing a query: as we get to query terms with lower idf, we can determine whether to proceed based on the changes in document scores from processing the previous query term. If these changes are minimal, we may omit accumulation from the remaining query terms, or alternatively process shorter prefixes of their postings lists. These ideas form a common generalization of the methods introduced in Sections 7.1.2 -7.1.4 . We may also implement a version of static ordering in which each postings list is ordered by an additive combination of static and query-dependent scores. We would again lose the consistency of ordering across postings, thereby having to process query terms one at time accumulating scores for all documents as we go along. Depending on the particular scoring function, the postings list for a document may be ordered by other quantities than term frequency; under this more general setting, this idea is known as impact ordering. In cluster pruning we have a preprocessing step during which we cluster the document vectors. Then at query time, we consider only documents in a small number of clusters as candidates for which we compute cosine scores. Specifically, the preprocessing step is as follows: Pick {N} documents at random from the collection. Call these leaders. For each document that is not a leader, we compute its nearest leader. We refer to documents that are not leaders as followers. Intuitively, in the partition of the followers induced by the use of {N} randomly chosen leaders, the expected number of followers for each leader is N/{N} = {N}. Next, query processing proceeds as follows: Given a query q, find the leader L that is closest to q. This entails computing cosine similarities from q to each of the {N} leaders. The candidate set A consists of L together with its followers. We compute the cosine scores for all documents in this candidate set. The use of randomly chosen leaders for clustering is fast and likely to reflect the distribution of the document vectors in the vector space: a region of the vector space that is dense in documents is likely to produce multiple leaders and thus a finer partition into sub-regions. This illustrated in Figure 7.3 . Figure 7.3: Cluster pruning. Variations of cluster pruning introduce additional parameters b 1 and b 2, both of which are positive integers. In the pre-processing step we attach each follower to its b 1 closest leaders, rather than a single closest leader. At query time we consider the b 2 leaders closest to the query q. Clearly, the basic scheme above corresponds to the case b 1=b 2=1. Further, increasing b 1 or b 2 increases the likelihood of finding K documents that are more likely to be in the set of true top-scoring K documents, at the expense of more computation. We reiterate this approach when describing clustering in Chapter 16 (page 16.1 ). """
iir 7 2,Components of an information retrieval system,,"tiered indexes, proximity weighting, evidence accumulation","In this section we combine the ideas developed so far to describe a rudimentary search system that retrieves and scores documents. We first develop further ideas for scoring, beyond vector spaces. Following this, we will put together all of these elements to outline a complete system. Because we consider a complete system, we do not restrict ourselves to vector space retrieval in this section. Indeed, our complete system will have provisions for vector space as well as other query operators and forms of retrieval. In Section 7.3 we will return to how vector space queries interact with other query operators. Tiered indexes We mentioned in Section 7.1.2 that when using heuristics such as index elimination for inexact top-K retrieval, we may occasionally find ourselves with a set A of contenders that has fewer than K documents. A common solution to this issue is the user of tiered indexes , which may be viewed as a generalization of champion lists . We illustrate this idea in Figure 7.4 , where we represent the documents and terms of Figure 6.9 . In this example we set a tf threshold of 20 for tier 1 and 10 for tier 2, meaning that the tier 1 index only has postings entries with tf values exceeding 20, while the tier 2 index only has postings entries with tf values exceeding 10. In this example we have chosen to order the postings entries within a tier by document ID. Tiered indexes.If we fail to get K results from tier 1, query processing ``falls back'' to tier 2, and so on. Within each tier, postings are ordered by document ID. We mentioned in Section 7.1.2 that when using heuristics such as index elimination for inexact top-K retrieval, we may occasionally find ourselves with a set A of contenders that has fewer than K documents. A common solution to this issue is the user of tiered indexes , which may be viewed as a generalization of champion lists . We illustrate this idea in Figure 7.4 , where we represent the documents and terms of Figure 6.9 . In this example we set a tf threshold of 20 for tier 1 and 10 for tier 2, meaning that the tier 1 index only has postings entries with tf values exceeding 20, while the tier 2 index only has postings entries with tf values exceeding 10. In this example we have chosen to order the postings entries within a tier by document ID. Tiered indexes.If we fail to get K results from tier 1, query processing ``falls back'' to tier 2, and so on. Within each tier, postings are ordered by document ID. Especially for free text queries on the web (Chapter 19 ), users prefer a document in which most or all of the query terms appear close to each other, because this is evidence that the document has text focused on their query intent. Consider a query with two or more query terms, t 1,t 2,,t k. Let be the width of the smallest window in a document d that contains all the query terms, measured in the number of words in the window. For instance, if the document were to simply consist of the sentence The quality of mercy is not strained, the smallest window for the query strained mercy would be 4. Intuitively, the smaller that is, the better that d matches the query. In cases where the document does not contain all of the query terms, we can set to be some enormous number. We could also consider variants in which only words that are not stop words are considered in computing . Such proximity-weighted scoring functions are a departure from pure cosine similarity and closer to the ``soft conjunctive'' semantics that Google and other web search engines evidently use. How can we design such a proximity-weighted scoring function to depend on ? The simplest answer relies on a ``hand coding'' technique we introduce below in Section 7.2.3 . A more scalable approach goes back to Section 6.1.2 - we treat the integer as yet another feature in the scoring function, whose importance is assigned by machine learning, as will be developed further in Section 15.4.1 . Common search interfaces, particularly for consumer-facing search applications on the web, tend to mask query operators from the end user. The intent is to hide the complexity of these operators from the largely non-technical audience for such applications, inviting free text queries . Given such interfaces, how should a search equipped with indexes for various retrieval operators treat a query such as rising interest rates? More generally, given the various factors we have studied that could affect the score of a document, how should we combine these features? The answer of course depends on the user population, the query distribution and the collection of documents. Typically, a query parser is used to translate the user-specified keywords into a query with various operators that is executed against the underlying indexes. Sometimes, this execution can entail multiple queries against the underlying indexes; for example, the query parser may issue a stream of queries: Run the user-generated query string as a phrase query. Rank them by vector space scoring using as query the vector consisting of the 3 terms rising interest rates. If fewer than ten documents contain the phrase rising interest rates, run the two 2-term phrase queries rising interest and interest rates; rank these using vector space scoring, as well. If we still have fewer than ten results, run the vector space query consisting of the three individual query terms. Each of these steps (if invoked) may yield a list of scored documents, for each of which we compute a score. This score must combine contributions from vector space scoring, static quality, proximity weighting and potentially other factors - particularly since a document may appear in the lists from multiple steps. This demands an aggregate scoring function that accumulates evidence of a document's relevance from multiple sources. How do we devise a query parser and how do we devise the aggregate scoring function? The answer depends on the setting. In many enterprise settings we have application builders who make use of a toolkit of available scoring operators, along with a query parsing layer, with which to manually configure the scoring function as well as the query parser. Such application builders make use of the available zones, metadata and knowledge of typical documents and queries to tune the parsing and scoring. In collections whose characteristics change infrequently (in an enterprise application, significant changes in collection and query characteristics typically happen with infrequent events such as the introduction of new document formats or document management systems, or a merger with another company). Web search on the other hand is faced with a constantly changing document collection with new characteristics being introduced all the time. It is also a setting in which the number of scoring factors can run into the hundreds, making hand-tuned scoring a difficult exercise. To address this, it is becoming increasingly common to use machine-learned scoring, extending the ideas we introduced in Section 6.1.2 , as will be discussed further in Section 15.4.1 . We have now studied all the components necessary for a basic search system that supports free text queries as well as Boolean, zone and field queries. We briefly review how the various pieces fit together into an overall system; this is depicted in Figure 7.5 . A complete search system.Data paths are shown primarily for a free text query. In this figure, documents stream in from the left for parsing and linguistic processing (language and format detection, tokenization and stemming). The resulting stream of tokens feeds into two modules. First, we retain a copy of each parsed document in a document cache. This will enable us to generate results snippets : snippets of text accompanying each document in the results list for a query. This snippet tries to give a succinct explanation to the user of why the document matches the query. The automatic generation of such snippets is the subject of Section 8.7 . A second copy of the tokens is fed to a bank of indexers that create a bank of indexes including zone and field indexes that store the metadata for each document, (tiered) positional indexes, indexes for spelling correction and other tolerant retrieval, and structures for accelerating inexact top-K retrieval. A free text user query (top center) is sent down to the indexes both directly and through a module for generating spelling-correction candidates. As noted in Chapter 3 the latter may optionally be invoked only when the original query fails to retrieve enough results. Retrieved documents (dark arrow) are passed to a scoring module that computes scores based on machine-learned ranking (MLR), a technique that builds on Section 6.1.2 (to be further developed in Section 15.4.1 ) for scoring and ranking documents. Finally, these ranked documents are rendered as a results page."""
iir 7 3,Vector space scoring and query operator interaction,Evaluation measures (information retrieval),,"We introduced the vector space model as a paradigm for free text queries. We conclude this chapter by discussing how the vector space scoring model relates to the query operators we have studied in earlier chapters. The relationship should be viewed at two levels: in terms of the expressiveness of queries that a sophisticated user may pose, and in terms of the index that supports the evaluation of the various retrieval methods. In building a search engine, we may opt to support multiple query operators for an end user. In doing so we need to understand what components of the index can be shared for executing various query operators, as well as how to handle user queries that mix various query operators. Vector space scoring supports so-called free text retrieval, in which a query is specified as a set of words without any query operators connecting them. It allows documents matching the query to be scored and thus ranked, unlike the Boolean, wildcard and phrase queries studied earlier. Classically, the interpretation of such free text queries was that at least one of the query terms be present in any retrieved document. However more recently, web search engines such as Google have popularized the notion that a set of terms typed into their query boxes (thus on the face of it, a free text query) carries the semantics of a conjunctive query that only retrieves documents containing all or most query terms. Clearly a vector space index can be used to answer Boolean queries, as long as the weight of a term t in the document vector for d is non-zero whenever t occurs in d. The reverse is not true, since a Boolean index does not by default maintain term weight information. There is no easy way of combining vector space and Boolean queries from a user's standpoint: vector space queries are fundamentally a form of evidence accumulation, where the presence of more query terms in a document adds to the score of a document. Boolean retrieval on the other hand, requires a user to specify a formula for selecting documents through the presence (or absence) of specific combinations of keywords, without inducing any relative ordering among them. Mathematically, it is in fact possible to invoke so-called p-norms to combine Boolean and vector space queries, but we know of no system that makes use of this fact. Wildcard and vector space queries require different indexes, except at the basic level that both can be implemented using postings and a dictionary (e.g., a dictionary of trigrams for wildcard queries). If a search engine allows a user to specify a wildcard operator as part of a free text query (for instance, the query rom* restaurant), we may interpret the wildcard component of the query as spawning multiple terms in the vector space (in this example, rome and roman would be two such terms) all of which are added to the query vector. The vector space query is then executed as usual, with matching documents being scored and ranked; thus a document containing both rome and roma is likely to be scored higher than another containing only one of them. The exact score ordering will of course depend on the relative weights of each term in matching documents. The representation of documents as vectors is fundamentally lossy: the relative order of terms in a document is lost in the encoding of a document as a vector. Even if we were to try and somehow treat every biword as a term (and thus an axis in the vector space), the weights on different axes not independent: for instance the phrase German shepherd gets encoded in the axis german shepherd, but immediately has a non-zero weight on the axes german and shepherd. Further, notions such as idf would have to be extended to such biwords. Thus an index built for vector space retrieval cannot, in general, be used for phrase queries. Moreover, there is no way of demanding a vector space score for a phrase query -- we only know the relative weights of each term in a document. On the query german shepherd, we could use vector space retrieval to identify documents heavy in these two terms, with no way of prescribing that they occur consecutively. Phrase retrieval, on the other hand, tells us of the existence of the phrase german shepherd in a document, without any indication of the relative frequency or weight of this phrase. While these two retrieval paradigms (phrase and vector space) consequently have different implementations in terms of indexes and retrieval algorithms, they can in some cases be combined usefully, as in the three-step example of query parsing in Section 7.2.3 ."""
iir 8,Evaluation in information retrieval,Evaluation measures (information retrieval),,"We have seen in the preceding chapters many alternatives in designing an IR system. How do we know which of these techniques are effective in which applications? Should we use stop lists? Should we stem? Should we use inverse document frequency weighting? Information retrieval has developed as a highly empirical discipline, requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections. In this chapter we begin with a discussion of measuring the effectiveness of IR systems (Section 8.1 ) and the test collections that are most often used for this purpose (Section 8.2 ). We then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results (Section 8.3 ). This includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate. We then extend these notions and develop further measures for evaluating ranked retrieval results (Section 8.4 ) and discuss developing reliable and informative test collections (Section 8.5 ). We then step back to introduce the notion of user utility, and how it is approximated by the use of document relevance (Section 8.6 ). The key utility measure is user happiness. Speed of response and the size of the index are factors in user happiness. It seems reasonable to assume that relevance of results is the most important factor: blindingly fast, useless answers do not make a user happy. However, user perceptions do not always coincide with system designers' notions of quality. For example, user happiness commonly depends very strongly on user interface design issues, including the layout, clarity, and responsiveness of the user interface, which are independent of the quality of the results returned. We touch on other measures of the quality of a system, in particular the generation of high-quality result summary snippets, which strongly influence user utility, but are not measured in the basic relevance ranking paradigm (Section 8.7 ). """
iir 8 1,Information retrieval system evaluation,Evaluation measures (information retrieval),"RELEVANCE,GOLD STANDARD,GROUND TRUTH,INFORMATION NEED,DEVELOPMENT TEST COLLECTION","To measure ad hoc information retrieval effectiveness in the standard way, we need a test collection consisting of three things: A document collection A test suite of information needs, expressible as queries A set of relevance judgments, standardly a binary assessment of either relevant or nonrelevant for each query-document pair. The standard approach to information retrieval system evaluation revolves around the notion of relevant and nonrelevant documents. With respect to a user information need, a document in the test collection is given a binary classification as either relevant or nonrelevant. This decision is referred to as the gold standard or ground truth judgment of relevance. The test document collection and suite of information needs have to be of a reasonable size: you need to average performance over fairly large test sets, as results are highly variable over different documents and information needs. As a rule of thumb, 50 information needs has usually been found to be a sufficient minimum. Relevance is assessed relative to an , not a query. For example, an information need might be: Information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine. This might be translated into a query such as: wine and red and white and heart and attack and effective A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query. This distinction is often misunderstood in practice, because the information need is not overt. But, nevertheless, an information need is present. If a user types python into a web search engine, they might be wanting to know where they can purchase a pet python. Or they might be wanting information on the programming language Python. From a one word query, it is very difficult for a system to know what the information need is. But, nevertheless, the user has one, and can judge the returned results on the basis of their relevance to it. To evaluate a system, we require an overt expression of an information need, which can be used for judging returned documents as relevant or nonrelevant. At this point, we make a simplification: relevance can reasonably be thought of as a scale, with some documents highly relevant and others marginally so. But for the moment, we will use just a binary decision of relevance. We discuss the reasons for using binary relevance judgments and alternatives in Section 8.5.1 . Many systems contain various weights (often known as parameters) that can be adjusted to tune system performance. It is wrong to report results on a test collection which were obtained by tuning these parameters to maximize performance on that collection. That is because such tuning overstates the expected performance of the system, because the weights will be set to maximize performance on one particular set of queries rather than for a random sample of queries. In such cases, the correct procedure is to have one or more development test collections , and to tune the parameters on the development test collection. The tester then runs the system with those weights on the test collection and reports the results on that collection as an unbiased estimate of performance. """
iir 8 2,Standard test collections,Evaluation measures (information retrieval),"CRANFIELD,TREC,GOV2,NTCIR,cross-language information retrieval, clef, reuters,20 newsgroups","Here is a list of the most standard test collections and evaluation series. We focus particularly on test collections for ad hoc information retrieval system evaluation, but also mention a couple of similar test collections for text classification. The Cranfield collection. This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs. Text Retrieval Conference (TREC) . The U.S. National Institute of Standards and Technology (NIST) has run a large IR test bed evaluation series since 1992. Within this framework, there have been many tracks over a range of different test collections, but the best known test collections are the ones used for the TREC Ad Hoc track during the first 8 TREC evaluations between 1992 and 1999. In total, these test collections comprise 6 CDs containing 1.89 million documents (mainly, but not exclusively, newswire articles) and relevance judgments for 450 information needs, which are called topics and specified in detailed text passages. Individual test collections are defined over different subsets of this data. The early TRECs each consisted of 50 information needs, evaluated over different but overlapping sets of documents. TRECs 6-8 provide 150 information needs over about 528,000 newswire and Foreign Broadcast Information Service articles. This is probably the best subcollection to use in future work, because it is the largest and the topics are more consistent. Because the test document collections are so large, there are no exhaustive relevance judgments. Rather, NIST assessors' relevance judgments are available only for the documents that were among the top k returned for some system which was entered in the TREC evaluation for which the information need was developed. In more recent years, NIST has done evaluations on larger document collections, including the 25 million page GOV2 web page collection. From the beginning, the NIST test document collections were orders of magnitude larger than anything available to researchers previously and GOV2 is now the largest Web collection easily available for research purposes. Nevertheless, the size of GOV2 is still more than 2 orders of magnitude smaller than the current size of the document collections indexed by the large web search companies. NII Test Collections for IR Systems ( NTCIR ). The NTCIR project has built various test collections of similar sizes to the TREC collections, focusing on East Asian language and cross-language information retrieval , where queries are made in one language over a document collection containing documents in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-en.html Cross Language Evaluation Forum ( CLEF ). This evaluation series has concentrated on European languages and cross-language information retrieval. See: http://www.clef-campaign.org/ and Reuters-RCV1. For text classification, the most used test collection has been the Reuters-21578 collection of 21578 newswire articles; see Chapter 13 , page 13.6 . More recently, Reuters released the much larger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents; see Chapter 4 , page 4.2 . Its scale and rich annotation makes it a better basis for future research. 20 Newsgroups . This is another widely used text classification collection, collected by Ken Lang. It consists of 1000 articles from each of 20 Usenet newsgroups (the newsgroup name being regarded as the category). After the removal of duplicate articles, as it is usually used, it contains 18941 articles. """
iir 8 3,Evaluation of unranked retrieval sets,Evaluation measures (information retrieval),"precision, recall, accuracy, f measure","Given these ingredients, how is system effectiveness measured? The two most frequent and basic measures for information retrieval effectiveness are precision and recall. These are first defined for the simple case where an IR system returns a set of documents for a query. We will see later how to extend these notions to ranked retrieval situations. Precision (P) is the fraction of retrieved documents that are relevant (41) However, using an even weighting is not the only choice. Values of < 1 emphasize precision, while values of > 1 emphasize recall. For example, a value of =3 or =5 might be used if recall is to be emphasized. Recall, precision, and the F measure are inherently measures between 0 and 1, but they are also very commonly written as percentages, on a scale between 0 and 100. Graph comparing the harmonic mean to other means.The graph shows a slice through the calculation of various means of precision and recall for the fixed recall value of 70%. The harmonic mean is always less than either the arithmetic or geometric mean, and often quite close to the minimum of the two numbers. When the precision is also 70%, all the measures coincide. Why do we use a harmonic mean rather than the simpler average (arithmetic mean)? Recall that we can always get 100% recall by just returning all documents, and therefore we can always get a 50% arithmetic mean by the same process. This strongly suggests that the arithmetic mean is an unsuitable measure to use. In contrast, if we assume that 1 document in 10,000 is relevant to the query, the harmonic mean score of this strategy is 0.02%. The harmonic mean is always less than or equal to the arithmetic mean and the geometric mean. When the values of two numbers differ greatly, the harmonic mean is closer to their minimum than to their arithmetic mean; see Figure 8.1 . """
iir 8 4,Evaluation of ranked retrieval results,Evaluation measures (information retrieval),"PRECISION-RECALL curve, interpolated precision, 11-POINT INTERPOLATED AVERAGE PRECISION, MEAN AVERAGE PRECISION, PRECISION AT k, R-PRECISION , BREAK-EVEN POINT, ROC CURVE, SENSITIVITY, SPECIFICITY, CUMULATIVE GAIN, NORMALIZED DISCOUNTED CUMULATIVE GAIN, NDCG, DICE COEFFICIENT","Precision, recall, and the F measure are set-based measures. They are computed using unordered sets of documents. We need to extend these measures (or to define new measures) if we are to evaluate the ranked retrieval results that are now standard with search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top k retrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve , such as the one shown in Figure 8.2 . Precision-recall curves have a distinctive saw-tooth shape: if the (k+1)^{th} document retrieved is nonrelevant then recall is the same as for the top k documents, but precision has dropped. If it is relevant, then both precision and recall increase, and the curve jags up and to the right. It is often useful to remove these jiggles and the standard way to do this is with an interpolated precision: the interpolated precision p {interp} at a certain recall level r is defined as the highest precision found for any recall level r' r : The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher). Interpolated precision is shown by a thinner line in Figure 8.2 . With this definition, the interpolated precision at a recall of 0 is well-defined (Exercise 8.4 ). Examining the entire precision-recall curve is very informative, but there is often a desire to boil this information down to a few numbers, or perhaps even a single number. The traditional way of doing this (used for instance in the first 8 TREC Ad Hoc evaluations) is the 11-point interpolated average precision . For each information need, the interpolated precision is measured at the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. For the precision-recall curve in Figure 8.2 , these 11 values are shown in Table 8.1 . For each recall level, we then calculate the arithmetic mean of the interpolated precision at that recall level for each information need in the test collection. A composite precision-recall curve showing 11 points can then be graphed. Figure 8.3 shows an example graph of such results from a representative good system at TREC 8. Averaged 11-point precision/recall graph across 50 queries for a representative TREC system.The Mean Average Precision for this system is 0.2553. In recent years, other measures have become more common. Most standard among the TREC community is Mean Average Precision (MAP), which provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. For a single information need, Average Precision is the average of the precision value obtained for the set of top k documents existing after each relevant document is retrieved, and this value is then averaged over information needs. That is, if the set of relevant documents for an information need q j Q is } and R {jk} is the set of ranked retrieval results from the top result until you get to document d k , then When a relevant document is not retrieved at all,[*]the precision value in the above equation is taken to be 0. For a single information need, the average precision approximates the area under the uninterpolated precision-recall curve, and so the MAP is roughly the average area under the precision-recall curve for a set of queries. Using MAP, fixed recall levels are not chosen, and there is no interpolation. The MAP value for a test collection is the arithmetic mean of average precision values for individual information needs. (This has the effect of weighting each information need equally in the final reported number, even if many documents are relevant to some queries whereas very few are relevant to other queries.) Calculated MAP scores normally vary widely across information needs when measured within a single system, for instance, between 0.1 and 0.7. Indeed, there is normally more agreement in MAP for an individual information need across systems than for MAP scores for different information needs for the same system. This means that a set of test information needs must be large and diverse enough to be representative of system effectiveness across different queries. The above measures factor in precision at all recall levels. For many prominent applications, particularly web search, this may not be germane to users. What matters is rather how many good results there are on the first page or the first three pages. This leads to measuring precision at fixed low levels of retrieved results, such as 10 or 30 documents. This is referred to as ``Precision at k '', for example ``Precision at 10''. It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at k . An alternative, which alleviates this problem, is R-precision . It requires having a set of known relevant documents Rel , from which we calculate the precision of the top Rel documents returned. (The set Rel may be incomplete, such as when Rel is formed by creating relevance judgments for the pooled top k results of particular systems in a set of experiments.) R-precision adjusts for the size of the set of relevant documents: A perfect system could score 1 on this metric for each query, whereas, even a perfect system could only achieve a precision at 20 of 0.4 if there were only 8 documents in the collection relevant to an information need. Averaging this measure across queries thus makes more sense. This measure is harder to explain to naive users than Precision at k but easier to explain than MAP. If there are Rel relevant documents for a query, we examine the top Rel results of a system, and find that r are relevant, then by definition, not only is the precision (and hence R-precision) r/ Rel , but the recall of this result set is also r/ Rel . Thus, R-precision turns out to be identical to the break-even point , another measure which is sometimes used, defined in terms of this equality relationship holding. Like Precision at k , R-precision describes only one point on the precision-recall curve, rather than attempting to summarize effectiveness across the curve, and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal F-measure) or a retrieval level of interest to a particular application (Precision at k ). Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on the curve. Figure 8.4: The ROC curve corresponding to the precision-recall curve in Figure 8.2 . . Another concept sometimes used in evaluation is an ROC curve . (``ROC'' stands for ``Receiver Operating Characteristics'', but knowing that doesn't help most people.) An ROC curve plots the true positive rate or sensitivity against the false positive rate or ( 1 - ). Here, sensitivity is just another term for recall. The false positive rate is given by fp/(fp+tn) . Figure 8.4 shows the ROC curve corresponding to the precision-recall curve in Figure 8.2 . An ROC curve always goes from the bottom left to the top right of the graph. For a good system, the graph climbs steeply on the left side. For unranked result sets, specificity , given by tn/(fp + tn) , was not seen as a very useful notion. Because the set of true negatives is always so large, its value would be almost 1 for all information needs (and, correspondingly, the value of the false positive rate would be almost 0). That is, the ``interesting'' part of Figure 8.2 is 0 < < 0.4 , a part which is compressed to a small corner of Figure 8.4 . But an ROC curve could make sense when looking over the full retrieval spectrum, and it provides another way of looking at the data. In many fields, a common aggregate measure is to report the area under the ROC curve, which is the ROC analog of MAP. Precision-recall curves are sometimes loosely referred to as ROC curves. This is understandable, but not accurate. A final approach that has seen increasing adoption, especially when employed with machine learning approaches to ranking svm-ranking is measures of cumulative gain , and in particular normalized discounted cumulative gain ( NDCG ). NDCG is designed for situations of non-binary notions of relevance (cf. Section 8.5.1 ). Like precision at k , it is evaluated over some number k of top search results. For a set of queries Q , let R(j,d) be the relevance score assessors gave to document d for query j . Then, where Z {kj} is a normalization factor calculated to make it so that a perfect ranking's NDCG at k for query j is 1. For queries for which k' < k documents are retrieved, the last summation is done up to k' . "
iir 8 5,Assessing relevance,Evaluation measures (information retrieval),"POOLING, KAPPA STATISTIC, MARGINAL","To properly evaluate a system, your test information needs must be germane to the documents in the test document collection, and appropriate for predicted usage of the system. These information needs are best designed by domain experts. Using random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distribution of information needs. Given information needs and documents, you need to collect relevance assessments. This is a time-consuming and expensive process involving human beings. For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained. For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query. The most standard approach is pooling , where relevance is assessed over a subset of the collection that is formed from the top k documents returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process. Observed proportion of the times the judges agreed P(A) = (300+70)/400 = 370/400 = 0.925 Pooled marginals P(nonrelevant) = (80+90)/(400+400) = 170/800 = 0.2125 P(relevant) = (320+310)/(400+400) = 630/800 = 0.7878 Probability that the two judges agreed by chance P(E) = P(nonrelevant)^2 + P(relevant)^2 = 0.2125^2 + 0.7878^2 = 0.665 Kappa statistic = (P(A) - P(E))/(1-P(E)) = (0.925 - 0.665)/(1 - 0.665) = 0.776 A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query. Rather, humans and their relevance judgments are quite idiosyncratic and variable. But this is not a problem to be solved: in the final analysis, the success of an IR system depends on how good it is at satisfying the needs of these idiosyncratic humans, one information need at a time. Nevertheless, it is interesting to consider and measure how much agreement between judges there is on relevance judgments. In the social sciences, a common measure for agreement between judges is the kappa statistic . It is designed for categorical judgments and corrects a simple agreement rate for the rate of chance agreement. where $P(A)$ is the proportion of the times the judges agreed, and $P(E)$ is the proportion of the times they would be expected to agree by chance. There are choices in how the latter is estimated: if we simply say we are making a two-class decision and assume nothing more, then the expected chance agreement rate is 0.5. However, normally the class distribution assigned is skewed, and it is usual to use marginal statistics to calculate expected agreement.[*]There are still two ways to do it depending on whether one pools the marginal distribution across judges or uses the marginals for each judge separately; both forms have been used, but we present the pooled version because it is more conservative in the presence of systematic differences in assessments across judges. The calculations are shown in Table 8.2 . The kappa value will be 1 if two judges always agree, 0 if they agree only at the rate given by chance, and negative if they are worse than random. If there are more than two judges, it is normal to calculate an average pairwise kappa value. As a rule of thumb, a kappa value above 0.8 is taken as good agreement, a kappa value between 0.67 and 0.8 is taken as fair agreement, and agreement below 0.67 is seen as data providing a dubious basis for an evaluation, though the precise cutoffs depend on the purposes for which the data will be used. Interjudge agreement of relevance has been measured within the TREC evaluations and for medical IR collections. Using the above rules of thumb, the level of agreement normally falls in the range of ``fair'' (0.67-0.8). The fact that human agreement on a binary relevance judgment is quite modest is one reason for not requiring more fine-grained relevance labeling from the test set creator. To answer the question of whether IR evaluation results are valid despite the variation of individual assessors' judgments, people have experimented with evaluations taking one or the other of two judges' opinions as the gold standard. The choice can make a considerable absolute difference to reported scores, but has in general been found to have little impact on the relative effectiveness ranking of either different systems or variants of a single system which are being compared for effectiveness. """
iir 8 5 1,Critiques and justifications of the concept of relevance,Evaluation measures (information retrieval),MARGINAL RELEVANCE,"The advantage of system evaluation, as enabled by the standard model of relevant and nonrelevant documents, is that we have a fixed setting in which we can vary IR systems and system parameters to carry out comparative experiments. Such formal testing is much less expensive and allows clearer diagnosis of the effect of changing system parameters than doing user studies of retrieval effectiveness. Indeed, once we have a formal measure that we have confidence in, we can proceed to optimize effectiveness by machine learning methods, rather than tuning parameters by hand. Of course, if the formal measure poorly describes what users actually want, doing this will not be effective in improving user satisfaction. Our perspective is that, in practice, the standard formal measures for IR evaluation, although a simplification, are good enough, and recent work in optimizing formal evaluation measures in IR has succeeded brilliantly. There are numerous examples of techniques developed in formal evaluation settings, which improve effectiveness in operational settings, such as the development of document length normalization methods within the context of TREC ( and 11.4.3 ) and machine learning methods for adjusting parameter weights in scoring (Section 6.1.2 ). That is not to say that there are not problems latent within the abstractions used. The relevance of one document is treated as independent of the relevance of other documents in the collection. (This assumption is actually built into most retrieval systems - documents are scored against queries, not against each other - as well as being assumed in the evaluation methods.) Assessments are binary: there aren't any nuanced assessments of relevance. Relevance of a document to an information need is treated as an absolute, objective decision. But judgments of relevance are subjective, varying across people, as we discussed above. In practice, human assessors are also imperfect measuring instruments, susceptible to failures of understanding and attention. We also have to assume that users' information needs do not change as they start looking at retrieval results. Any results based on one collection are heavily skewed by the choice of collection, queries, and relevance judgment set: the results may not translate from one domain to another or to a different user population. Some of these problems may be fixable. A number of recent evaluations, including INEX, some TREC tracks, and NTCIR have adopted an ordinal notion of relevance with documents divided into 3 or 4 classes, distinguishing slightly relevant documents from highly relevant documents. See Section 10.4 for a detailed discussion of how this is implemented in the INEX evaluations. One clear problem with the relevance-based assessment that we have presented is the distinction between relevance and marginal relevance : whether a document still has distinctive usefulness after the user has looked at certain other documents (Carbonell and Goldstein, 1998). Even if a document is highly relevant, its information can be completely redundant with other documents which have already been examined. The most extreme case of this is documents that are duplicates - a phenomenon that is actually very common on the World Wide Web - but it can also easily occur when several documents provide a similar precis of an event. In such circumstances, marginal relevance is clearly a better measure of utility to the user. Maximizing marginal relevance requires returning documents that exhibit diversity and novelty. One way to approach measuring this is by using distinct facts or entities as evaluation units. This perhaps more directly measures true utility to the user but doing this makes it harder to create a test collection. """
iir 8 6,A broader perspective: System quality and user utility,Evaluation measures (information retrieval)," clickthrough log analysis, clickstream mining","Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies. These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface. In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility. System issues Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies. These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface. In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility. There are many practical benchmarks on which to rate an information retrieval system beyond its retrieval quality. These include: How fast does it index, that is, how many documents per hour does it index for a certain distribution over document lengths? (cf. Chapter 4 ) How fast does it search, that is, what is its latency as a function of index size? How expressive is its query language? How fast is it on complex queries? How large is its document collection, in terms of the number of documents or the collection having information distributed across a broad range of topics? All these criteria apart from query language expressiveness are straightforwardly measurable: we can quantify the speed or size. Various kinds of feature checklists can make query language expressiveness semi-precise. User utility What we would really like is a way of quantifying aggregate user happiness, based on the relevance, speed, and user interface of a system. One part of this is understanding the distribution of people we wish to make happy, and this depends entirely on the setting. For a web search engine, happy search users are those who find what they want. One indirect measure of such users is that they tend to return to the same engine. Measuring the rate of return of users is thus an effective metric, which would of course be more effective if you could also measure how much these users used other search engines. But advertisers are also users of modern web search engines. They are happy if customers click through to their sites and then make purchases. On an eCommerce web site, a user is likely to be wanting to purchase something. Thus, we can measure the time to purchase, or the fraction of searchers who become buyers. On a shopfront web site, perhaps both the user's and the store owner's needs are satisfied if a purchase is made. Nevertheless, in general, we need to decide whether it is the end user's or the eCommerce site owner's happiness that we are trying to optimize. Usually, it is the store owner who is paying us. For an ``enterprise'' (company, government, or academic) intranet search engine, the relevant metric is more likely to be user productivity: how much time do users spend looking for information that they need. There are also many other practical criteria concerning such matters as information security, which we mentioned in Section 4.6 . User happiness is elusive to measure, and this is part of why the standard methodology uses the proxy of relevance of search results. The standard direct way to get at user satisfaction is to run user studies, where people engage in tasks, and usually various metrics are measured, the participants are observed, and ethnographic interview techniques are used to get qualitative information on satisfaction. User studies are very useful in system design, but they are time consuming and expensive to do. They are also difficult to do well, and expertise is required to design the studies and to interpret the results. We will not discuss the details of human usability testing here. Refining a deployed system If an IR system has been built and is being used by a large number of users, the system's builders can evaluate possible changes by deploying variant versions of the system and recording measures that are indicative of user satisfaction with one variant vs. others as they are being used. This method is frequently used by web search engines. The most common version of this is A/B testing , a term borrowed from the advertising industry. For such a test, precisely one thing is changed between the current system and a proposed system, and a small proportion of traffic (say, 1-10% of users) is randomly directed to the variant system, while most users use the current system. For example, if we wish to investigate a change to the ranking algorithm, we redirect a random sample of users to a variant system and evaluate measures such as the frequency with which people click on the top result, or any result on the first page. (This particular analysis method is referred to as clickthrough log analysis or clickstream mining . It is further discussed as a method of implicit feedback in Section 9.1.7 .) The basis of A/B testing is running a bunch of single variable tests (either in sequence or in parallel): for each test only one parameter is varied from the control (the current live system). It is therefore easy to see whether varying each parameter has a positive or negative effect. Such testing of a live system can easily and cheaply gauge the effect of a change on users, and, with a large enough user base, it is practical to measure even very small positive and negative effects. In principle, more analytic power can be achieved by varying multiple things at once in an uncorrelated (random) way, and doing standard multivariate statistical analysis, such as multiple linear regression. In practice, though, A/B testing is widely used, because A/B tests are easy to deploy, easy to understand, and easy to explain to management. """
iir 8 7,Results snippets,Relevance feedback," SNIPPET,STATIC SUMMARY, DYNAMIC SUMMARY, TEXT SUMMARIZATION, KEYWORD-IN-CONTEXT","Having chosen or ranked the documents matching a query, we wish to present a results list that will be informative to the user. In many cases the user will not want to examine all the returned documents and so we want to make the results list informative enough that the user can do a final ranking of the documents for themselves based on relevance to their information need.[*]The standard way of doing this is to provide a snippet , a short summary of the document, which is designed so as to allow the user to decide its relevance. Typically, the snippet consists of the document title and a short summary, which is automatically extracted. The question is how to design the summary so as to maximize its usefulness to the user. The two basic kinds of summaries are static , which are always the same regardless of the query, and dynamic (or query-dependent), which are customized according to the user's information need as deduced from a query. Dynamic summaries attempt to explain why a particular document was retrieved for the query at hand. A static summary is generally comprised of either or both a subset of the document and metadata associated with the document. The simplest form of summary takes the first two sentences or 50 words of a document, or extracts particular zones of a document, such as the title and author. Instead of zones of a document, the summary can instead use metadata associated with the document. This may be an alternative way to provide an author or date, or may include elements which are designed to give a summary, such as the description metadata which can appear in the meta element of a web HTML page. This summary is typically extracted and cached at indexing time, in such a way that it can be retrieved and presented quickly when displaying search results, whereas having to access the actual document content might be a relatively expensive operation. There has been extensive work within natural language processing (NLP) on better ways to do text summarization . Most such work still aims only to choose sentences from the original document to present and concentrates on how to select good sentences. The models typically combine positional factors, favoring the first and last paragraphs of documents and the first and last sentences of paragraphs, with content factors, emphasizing sentences with key terms, which have low document frequency in the collection as a whole, but high frequency and good distribution across the particular document being returned. In sophisticated NLP approaches, the system synthesizes sentences for a summary, either by doing full text generation or by editing and perhaps combining sentences used in the document. For example, it might delete a relative clause or replace a pronoun with the noun phrase that it refers to. This last class of methods remains in the realm of research and is seldom used for search results: it is easier, safer, and often even better to just use sentences from the original document. Dynamic summaries display one or more ``windows'' on the document, aiming to present the pieces that have the most utility to the user in evaluating the document with respect to their information need. Usually these windows contain one or several of the query terms, and so are often referred to as keyword-in-context ( ) snippets, though sometimes they may still be pieces of the text such as the title that are selected for their query-independent information value just as in the case of static summarization. Dynamic summaries are generated in conjunction with scoring. If the query is found as a phrase, occurrences of the phrase in the document will be shown as the summary. If not, windows within the document that contain multiple query terms will be selected. Commonly these windows may just stretch some number of words to the left and right of the query terms. This is a place where NLP techniques can usefully be employed: users prefer snippets that read well because they contain complete phrases. Dynamic summaries are generally regarded as greatly improving the usability of IR systems, but they present a complication for IR system design. A dynamic summary cannot be precomputed, but, on the other hand, if a system has only a positional index, then it cannot easily reconstruct the context surrounding search engine hits in order to generate such a dynamic summary. This is one reason for using static summaries. The standard solution to this in a world of large and cheap disk drives is to locally cache all the documents at index time (notwithstanding that this approach raises various legal, information security and control issues that are far from resolved) as shown in Figure 7.5 . Then, a system can simply scan a document which is about to appear in a displayed results list to find snippets containing the query words. Beyond simply access to the text, producing a good KWIC snippet requires some care. Given a variety of keyword occurrences in a document, the goal is to choose fragments which are: (i) maximally informative about the discussion of those terms in the document, (ii) self-contained enough to be easy to read, and (iii) short enough to fit within the normally strict constraints on the space available for summaries. Generating snippets must be fast since the system is typically generating many snippets for each query that it handles. Rather than caching an entire document, it is common to cache only a generous but fixed size prefix of the document, such as perhaps 10,000 characters. For most common, short documents, the entire document is thus cached, but huge amounts of local storage will not be wasted on potentially vast documents. Summaries of documents whose length exceeds the prefix size will be based on material in the prefix only, which is in general a useful zone in which to look for a document summary anyway. If a document has been updated since it was last processed by a crawler and indexer, these changes will be neither in the cache nor in the index. In these circumstances, neither the index nor the summary will accurately reflect the current contents of the document, but it is the differences between the summary and the actual document content that will be more glaringly obvious to the end user. """
iir 9,Relevance feedback and query expansion,Relevance feedback,SYNONYMY,"In most collections, the same concept may be referred to using different words. This issue, known as synonymy , has an impact on the recall of most information retrieval systems. For example, you would want a search for aircraft to match plane (but only for references to an airplane, not a woodworking plane), and for a search on thermodynamics to match references to heat in appropriate discussions. Users often attempt to address this problem themselves by manually refining a query, as was discussed in Section 1.4 ; in this chapter we discuss ways in which a system can help with query refinement, either fully automatically or with the user in the loop. The methods for tackling this problem split into two major classes: global methods and local methods. Global methods are techniques for expanding or reformulating query terms independent of the query and results returned from it, so that changes in the query wording will cause the new query to match other semantically similar terms. Global methods include: Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2 ) Query expansion via automatic thesaurus generation (Section 9.2.3 ) Techniques like spelling correction (discussed in Chapter 3 ) Local methods adjust a query relative to the documents that initially appear to match the query. The basic methods here are: Relevance feedback (Section 9.1 ) Pseudo relevance feedback, also known as Blind relevance feedback (Section 9.1.6 ) (Global) indirect relevance feedback (Section 9.1.7 ) In this chapter, we will mention all of these approaches, but we will concentrate on relevance feedback, which is one of the most used and most successful approaches. """
iir 9 1,Relevance feedback and pseudo relevance feedback,Relevance feedback,"RELEVANCE FEEDBACK, ROCCHIO ALGORITHM, IDE DEC-HI, PSEUDO RELEVANCE FEEDBACK,BLIND RELEVANCE FEEDBACK, IMPLICIT RELEVANCE FEEDBACK, CLICKSTREAM MINING"," The idea of relevance feedback ( ) is to involve the user in the retrieval process so as to improve the final result set. In particular, the user gives feedback on the relevance of documents in an initial set of results. The basic procedure is: The user issues a (short, simple) query. The system returns an initial set of retrieval results. The user marks some returned documents as relevant or nonrelevant. The system computes a better representation of the information need based on the user feedback. The system displays a revised set of retrieval results. Relevance feedback can go through one or more iterations of this sort. The process exploits the idea that it may be difficult to formulate a good query when you don't know the collection well, but it is easy to judge particular documents, and so it makes sense to engage in iterative query refinement of this sort. In such a scenario, relevance feedback can also be effective in tracking a user's evolving information need: seeing some documents may lead users to refine their understanding of the information they are seeking. (b) Relevance feedback searching over images.(a) The user views the initial query results for a query of bike, selects the first, third and fourth result in the top row and the fourth result in the bottom row as relevant, and submits this feedback. (b) The users sees the revised result set. Precision is greatly improved. From http://nayana.ece.ucsb.edu/imsearch/imsearch.html(Newsam et al., 2001). Image search provides a good example of relevance feedback. Not only is it easy to see the results at work, but this is a domain where a user can easily have difficulty formulating what they want in words, but can easily indicate relevant or nonrelevant images. After the user enters an initial query for bike on the demonstration system at: http://nayana.ece.ucsb.edu/imsearch/imsearch.html the initial results (in this case, images) are returned. In Figure 9.1 (a), the user has selected some of them as relevant. These will be used to refine the query, while other displayed results have no effect on the reformulation. Figure 9.1 (b) then shows the new top-ranked results calculated after this round of relevance feedback. Figure 9.2 shows a textual IR example where the user wishes to find out about new applications of space satellites. This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis (Section 9.2 ). It has been found to improve performance in the TREC ad hoc task. See for example the results in Figure 9.5 . But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. Indirect relevance feedback We can also use indirect sources of evidence rather than explicit feedback on relevance as the basis for relevance feedback. This is often called implicit (relevance) feedback . Implicit feedback is less reliable than explicit feedback, but is more useful than pseudo relevance feedback, which contains no evidence of user judgments. Moreover, while users are often reluctant to provide explicit feedback, it is easy to collect implicit feedback in large quantities for a high volume system, such as a web search engine. On the web, DirectHit introduced the idea of ranking more highly documents that users chose to look at more often. In other words, clicks on links were assumed to indicate that the page was likely relevant to the query. This approach makes various assumptions, such as that the document summaries displayed in results lists (on whose basis users choose which documents to click on) are indicative of the relevance of these documents. In the original DirectHit search engine, the data about the click rates on pages was gathered globally, rather than being user or query specific. This is one form of the general area of clickstream mining . Today, a closely related approach is used in ranking the advertisements that match a web search query (Chapter 19 ). Summary Relevance feedback has been shown to be very effective at improving relevance of results. Its successful use requires queries for which the set of relevant documents is medium to large. Full relevance feedback is often onerous for the user, and its implementation is not very efficient in most IR systems. In many cases, other types of interactive retrieval may improve relevance by about as much with less work. Beyond the core ad hoc retrieval scenario, other uses of relevance feedback include: Following a changing information need (e.g., names of car models of interest change over time) Maintaining an information filter (e.g., for a news feed). Such filters are discussed further in Chapter 13 . Active learning (deciding which examples it is most useful to know the class of to reduce annotation costs). Exercises. Under what conditions would the modified query q m in Equation 49 be the same as the original query q 0? In all other cases, is q m closer than q 0 to the centroid of the relevant documents? Why is positive feedback likely to be more useful than negative feedback to an IR system? Why might only using one nonrelevant document be more effective than using several? Suppose that a user's initial query is cheap CDs cheap DVDs extremely cheap CDs. The user examines two documents, d 1 and d 2. She judges d 1, with the content CDs cheap software cheap CDs relevant and d 2 with content cheap thrills DVDs nonrelevant. Assume that we are using direct term frequency (with no scaling and no document frequency). There is no need to length-normalize vectors. Using Rocchio relevance feedback as in Equation 49 what would the revised query vector be after relevance feedback? Assume =1,=0.75,=0.25. Omar has implemented a relevance feedback web search system, where he is going to do relevance feedback based only on words in the title text returned for a page (for efficiency). The user is going to rank 3 results. The first user, Jinxing, queries for: banana slug and the top three titles returned are: banana slug Ariolimax columbianus Santa Cruz mountains banana slug Santa Cruz Campus Mascot Jinxing judges the first two documents relevant, and the third nonrelevant. Assume that Omar's search engine uses term frequency but no length normalization nor IDF. Assume that he is using the Rocchio relevance feedback mechanism, with = = = 1. Show the final revised query that would be run. (Please list the vector elements in alphabetical order.) """
iir 9 2,Global methods for query reformulation,Probabilistic relevance model,QUERY EXPANSION,"In this section we more briefly discuss three global methods for expanding a query: by simply aiding the user in doing so, by using a manual thesaurus, and through building a thesaurus automatically. Vocabulary tools for query reformulation Various user supports in the search process can help the user see how their searches are or are not working. This includes information about words that were omitted from the query because they were on stop lists, what words were stemmed to, the number of hits on each term or phrase, and whether words were dynamically turned into phrases. The IR system might also suggest search terms by means of a thesaurus or a controlled vocabulary. A user can also be allowed to browse lists of the terms that are in the inverted index, and thus find good terms that appear in the collection. Query expansion An example of query expansion in the interface of the Yahoo! web search engine in 2006.The expanded query suggestions appear just below the ``Search Results'' bar. In relevance feedback, users give additional input on documents (by marking documents in the results set as relevant or not), and this input is used to reweight the terms in the query for documents. In query expansion on the other hand, users give additional input on query words or phrases, possibly suggesting additional query terms. Some search engines (especially on the web) suggest related queries in response to a query; the users then opt to use one of these alternative query suggestions. Figure 9.6 shows an example of query suggestion options being presented in the Yahoo! web search engine. The central question in this form of query expansion is how to generate alternative or expanded queries for the user. The most common form of query expansion is global analysis, using some form of thesaurus. For each term t in a query, the query can be automatically expanded with synonyms and related words of t from the thesaurus. Use of a thesaurus can be combined with ideas of term weighting: for instance, one might weight added terms less than original query terms. The simplest way to compute a co-occurrence thesaurus is based on term-term similarities. We begin with a term-document matrix A, where each cell A {t,d} is a weighted count w {t,d} for term t and document d, with weighting so A has length-normalized rows. If we then calculate C = AA^T, then C {u,v} is a similarity score between terms u and v, with a larger number being better. Figure 9.8 shows an example of a thesaurus derived in basically this manner, but with an extra step of dimensionality reduction via Latent Semantic Indexing, which we discuss in Chapter 18 . While some of the thesaurus terms are good or at least suggestive, others are marginal or bad. The quality of the associations is typically a problem. Term ambiguity easily introduces irrelevant statistically correlated terms. For example, a query for Apple computer may expand to Apple red fruit computer. In general these thesauri suffer from both false positives and false negatives. Moreover, since the terms in the automatic thesaurus are highly correlated in documents anyway (and often the collection used to derive the thesaurus is the same as the one being indexed), this form of query expansion may not retrieve many additional documents. Query expansion is often effective in increasing recall. However, there is a high cost to manually producing a thesaurus and then updating it for scientific and terminological developments within a field. In general a domain-specific thesaurus is required: general thesauri and dictionaries give far too little coverage of the rich domain-particular vocabularies of most scientific fields. However, query expansion may also significantly decrease precision, particularly when the query contains ambiguous terms. For example, if the user searches for interest rate, expanding the query to interest rate fascinate evaluate is unlikely to be useful. Overall, query expansion is less successful than relevance feedback, though it may be as good as pseudo relevance feedback. It does, however, have the advantage of being much more understandable to the system user. """
iir 11,Probabilistic information retrieval,Probabilistic relevance model,,"During the discussion of relevance feedback in Section 9.1.2 , we observed that if we have some known relevant and nonrelevant documents, then we can straightforwardly start to estimate the probability of a term t appearing in a relevant document P(t R=1), and that this could be the basis of a classifier that decides whether documents are relevant or not. In this chapter, we more systematically introduce this probabilistic approach to IR, which provides a different formal basis for a retrieval model and results in different techniques for setting term weights. Users start with information needs, which they translate into query representations. Similarly, there are documents, which are converted into document representations (the latter differing at least by how text is tokenized, but perhaps containing fundamentally less information, as when a non-positional index is used). Based on these two representations, a system tries to determine how well documents satisfy information needs. In the Boolean or vector space models of IR, matching is done in a formally defined but semantically imprecise calculus of index terms. Given only a query, an IR system has an uncertain understanding of the information need. Given the query and document representations, a system has an uncertain guess of whether a document has content relevant to the information need. Probability theory provides a principled foundation for such reasoning under uncertainty. This chapter provides one answer as to how to exploit this foundation to estimate how likely it is that a document is relevant to an information need. There is more than one possible retrieval model which has a probabilistic basis. Here, we will introduce probability theory and the Probability Ranking Principle (Sections 11.1 -11.2 ), and then concentrate on the Binary Independence Model (Section 11.3 ), which is the original and still most influential probabilistic retrieval model. Finally, we will introduce related but extended methods which use term counts, including the empirically successful Okapi BM25 weighting scheme, and Bayesian Network models for IR (Section 11.4 ). In Chapter 12 , we then present the alternative probabilistic language modeling approach to IR, which has been developed with considerable success in recent years. """
iir 11 1,Review of basic probability theory,Probabilistic relevance model,"RANDOM VARIABLe, CHAIN RULE, PARTITION RULE, BAYESê RULE , PRIOR PROBABILITY, POSTERIOR PROBABILITY, ODDS","We hope that the reader has seen a little basic probability theory previously. We will give a very quick review; some references for further reading appear at the end of the chapter. A variable A represents an event (a subset of the space of possible outcomes). Equivalently, we can represent the subset via a random variable , which is a function from outcomes to real numbers; the subset is the domain over which the random variable A has a particular value. Often we will not know with certainty whether an event is true in the world. We can ask the probability of the event 0 P(A) 1. For two events A and B, the joint event of both events occurring is described by the joint probability P(A,B). The conditional probability P(A B) expresses the probability of event A given that event B occurred. The fundamental relationship between joint and conditional probabilities is given by the chain rule : Without making any assumptions, the probability of a joint event equals the probability of one of the events multiplied by the probability of the other event conditioned on knowing the first event happened. Writing P({A}) for the complement of an event, we similarly have: Probability theory also has a partition rule , which says that if an event B can be divided into an exhaustive set of disjoint subcases, then the probability of B is the sum of the probabilities of the subcases. A special case of this rule gives that: From these we can derive Bayes' Rule for inverting conditional probabilities: This equation can also be thought of as a way of updating probabilities. We start off with an initial estimate of how likely the event A is when we do not have any other information; this is the prior probability P(A). Bayes' rule lets us derive a posterior probability P(A B) after having seen the evidence B, based on the likelihood of B occurring in the two cases that A does or does not hold.[*] Finally, it is often useful to talk about the odds of an event, which provide a kind of multiplier for how probabilities change: """
iir 11 2,The Probability Ranking Principle,Probabilistic relevance model,"PROBABILITY RANKING PRINCIPLE, 1/0 LOSS, BAYES OPTIMAL DECISION RULE, BAYES RISK","The 1/0 loss case We assume a ranked retrieval setup as in Section 6.3 , where there is a collection of documents, the user issues a query, and an ordered list of documents is returned. We also assume a binary notion of relevance as in Chapter 8 . For a query q and a document d in the collection, let R {d,q} be an indicator random variable that says whether d is relevant with respect to a given query q. That is, it takes on a value of 1 when the document is relevant and 0 otherwise. In context we will often write just R for R {d,q}. Using a probabilistic model, the obvious order in which to present documents to the user is to rank documents by their estimated probability of relevance with respect to the information need: P(R=1 d,q). This is the basis of the Probability Ranking Principle (PRP) (van Rijsbergen, 1979, 113-114): ``If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose, the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data.'' In the simplest case of the PRP, there are no retrieval costs or other utility concerns that would differentially weight actions or errors. You lose a point for either returning a nonrelevant document or failing to return a relevant document (such a binary situation where you are evaluated on your accuracy is called 1/0 loss ). The goal is to return the best possible results as the top k documents, for any value of k the user chooses to examine. The PRP then says to simply rank all documents in decreasing order of P(R=1 d,q). If a set of retrieval results is to be returned, rather than an ordering, the Bayes Optimal Decision Rule , the decision which minimizes the risk of loss, is to simply return documents that are more likely relevant than nonrelevant: Theorem. The PRP is optimal, in the sense that it minimizes the expected loss (also known as the Bayes risk ) under 1/0 loss. End theorem. The proof can be found in Ripley (1996). However, it requires that all probabilities are known correctly. This is never the case in practice. Nevertheless, the PRP still provides a very useful foundation for developing models of IR. The PRP with retrieval costs Suppose, instead, that we assume a model of retrieval costs. Let C 1 be the cost of not retrieving a relevant document and C 0 the cost of retrieval of a nonrelevant document. Then the Probability Ranking Principle says that if for a specific document d and for all documents d' not yet retrieved (62) then d is the next document to be retrieved. Such a model gives a formal framework where we can model differential costs of false positives and false negatives and even system performance issues at the modeling stage, rather than simply at the evaluation stage, as we did in Section 8.6 . However, we will not further consider loss/utility models in this chapter. """
iir 11 3,The Binary Independence Model,Probabilistic relevance model," BINARY independence model, naive bayes assumption, retrieval status value, odds ratio, RELATIVE FREQUENCY, MAXIMUM LIKELIHOOD estimate, MLE, SMOOTHING, PSEUDOCOUNTS, BAYESIAN PRIOR, MAXIMUM A POSTERIORI, MAP"," The Binary Independence Model (BIM) we present in this section is the model that has traditionally been used with the PRP. It introduces some simple assumptions, which make estimating the probability function P(R d,q) practical. Here, ``binary'' is equivalent to Boolean: documents and queries are both represented as binary term incidence vectors. That is, a document d is represented by the vector {x} = (x 1, , x M) where x t = 1 if term t is present in document d and x t = 0 if t is not present in d. With this representation, many possible documents have the same vector representation. Similarly, we represent q by the incidence vector {q} (the distinction between q and {q} is less central since commonly q is in the form of a set of words). ``Independence'' means that terms are modeled as occurring in documents independently. The model recognizes no association between terms. This assumption is far from correct, but it nevertheless often gives satisfactory results in practice; it is the ``naive'' assumption of Naive Bayes models, discussed further in Section 13.4 . Indeed, the Binary Independence Model is exactly the same as the multivariate Bernoulli Naive Bayes model presented in Section 13.3 . In a sense this assumption is equivalent to an assumption of the vector space model, where each term is a dimension that is orthogonal to all other terms. We will first present a model which assumes that the user has a single step information need. As discussed in Chapter 9 , seeing a range of results might let the user refine their information need. Fortunately, as mentioned there, it is straightforward to extend the Binary Independence Model so as to provide a framework for relevance feedback, and we present this model in Section 11.3.4 . To make a probabilistic retrieval strategy precise, we need to estimate how terms in documents contribute to relevance, specifically, we wish to know how term frequency, document frequency, document length, and other statistics that we can compute influence judgments about document relevance, and how they can be reasonably combined to estimate the probability of document relevance. We then order documents by decreasing estimated probability of relevance. We assume here that the relevance of each document is independent of the relevance of other documents. As we noted in Section 8.5.1 , this is incorrect: the assumption is especially harmful in practice if it allows a system to return duplicate or near duplicate documents. Under the BIM, we model the probability P(R d,q) that a document is relevant via the probability in terms of term incidence vectors P(R{x}, {q}). Then, using Bayes rule, we have: P(R=1{x}, {q}) = {P({x} R=1, {q})P(R=1{q})}{P({x}{q})} (63) P(R=0{x}, {q}) = {P({x} R=0, {q})P(R=0{q})}{P({x}{q})} (64) Here, P({x} R=1,{q}) and P({x} R=0,{q}) are the probability that if a relevant or nonrelevant, respectively, document is retrieved, then that document's representation is {x}. You should think of this quantity as defined with respect to a space of possible documents in a domain. How do we compute all these probabilities? We never know the exact probabilities, and so we have to use estimates: Statistics about the actual document collection are used to estimate these probabilities. P(R=1{q}) and P(R=0{q}) indicate the prior probability of retrieving a relevant or nonrelevant document respectively for a query {q}. Again, if we knew the percentage of relevant documents in the collection, then we could use this number to estimate P(R=1{q}) and P(R=0{q}). Since a document is either relevant or nonrelevant to a query, we must have that: Deriving a ranking function for query terms Given a query q, we wish to order returned documents by descending P(R=1 d,q). Under the BIM, this is modeled as ordering by P(R=1{x},{q}). Rather than estimating this probability directly, because we are interested only in the ranking of documents, we work with some other quantities which are easier to compute and which give the same ordering of documents. In particular, we can rank documents by their odds of relevance (as the odds of relevance is monotonic with the probability of relevance). This makes things easier, because we can ignore the common denominator in Rxq-bayes, giving: The left term in the rightmost expression of Equation 66 is a constant for a given query. Since we are only ranking documents, there is thus no need for us to estimate it. The right-hand term does, however, require estimation, and this initially appears to be difficult: How can we accurately estimate the probability of an entire term incidence vector occurring? It is at this point that we make the Naive Bayes conditional independence assumption that the presence or absence of a word in a document is independent of the presence or absence of any other word (given the query): So: Henceforth, let p t = P(x t=1 R=1,{q}) be the probability of a term appearing in a document relevant to the query, and u t = P(x t = 1 R=0,{q}) be the probability of a term appearing in a nonrelevant document. These quantities can be visualized in the following contingency table where the columns add to 1: Let us make an additional simplifying assumption that terms not occurring in the query are equally likely to occur in relevant and nonrelevant documents: that is, if q t = 0 then p t = u t. (This assumption can be changed, as when doing relevance feedback in Section 11.3.4 .) Then we need only consider terms in the products that appear in the query, and so, The left product is over query terms found in the document and the right product is over query terms not found in the document. We can manipulate this expression by including the query terms found in the document into the right product, but simultaneously dividing through by them in the left product, so the value is unchanged. Then we have: The left product is still over query terms found in the document, but the right product is now over all query terms. That means that this right product is a constant for a particular query, just like the odds O(R{q}). So the only quantity that needs to be estimated to rank documents for relevance to a query is the left product. We can equally rank documents by the logarithm of this term, since log is a monotonic function. The resulting quantity used for ranking is called the Retrieval Status Value (RSV) in this model: So everything comes down to computing the RSV. Define c t: The c t terms are log odds ratios for the terms in the query. We have the odds of the term appearing if the document is relevant (p t/(1-p t)) and the odds of the term appearing if the document is nonrelevant (u t/(1-u t)). The odds ratio is the ratio of two such odds, and then we finally take the log of that quantity. The value will be 0 if a term has equal odds of appearing in relevant and nonrelevant documents, and positive if it is more likely to appear in relevant documents. The c t quantities function as term weights in the model, and the document score for a query is RSV d = {x t=q t=1} c t. Operationally, we sum them in accumulators for query terms appearing in documents, just as for the vector space model calculations discussed in Section 7.1 . We now turn to how we estimate these c t quantities for a particular collection and query. Probability estimates in theory For each term t, what would these c t numbers look like for the whole collection? odds-ratio-ct-contingency gives a contingency table of counts of documents in the collection, where t is the number of documents that contain term t: To avoid the possibility of zeroes (such as if every or no relevant document has a particular term) it is fairly standard to add {1}{2} to each of the quantities in the center 4 terms of odds-ratio-ct-contingency, and then to adjust the marginal counts (the totals) accordingly (so, the bottom right cell totals N+2). Then we have: Adding {1}{2} in this way is a simple form of smoothing. For trials with categorical outcomes (such as noting the presence or absence of a term), one way to estimate the probability of an event from data is simply to count the number of times an event occurred divided by the total number of trials. This is referred to as the relative frequency of the event. Estimating the probability as the relative frequency is the maximum likelihood estimate (or MLE ), because this value makes the observed data maximally likely. However, if we simply use the MLE, then the probability given to events we happened to see is usually too high, whereas other events may be completely unseen and giving them as a probability estimate their relative frequency of 0 is both an underestimate, and normally breaks our models, since anything multiplied by 0 is 0. Simultaneously decreasing the estimated probability of seen events and increasing the probability of unseen events is referred to as smoothing . One simple way of smoothing is to add a number to each of the observed counts. These pseudocounts correspond to the use of a uniform distribution over the vocabulary as a Bayesian prior , following Equation 59. We initially assume a uniform distribution over events, where the size of denotes the strength of our belief in uniformity, and we then update the probability based on observed events. Since our belief in uniformity is weak, we use = {1}{2}. This is a form of maximum a posteriori ( MAP ) estimation, where we choose the most likely point value for probabilities based on the prior and the observed evidence, following Equation 59. We will further discuss methods of smoothing estimated counts to give probability models in Section 12.2.2 ; the simple method of adding {1}{2} to each observed count will do for now. Probability estimates in practice Under the assumption that relevant documents are a very small percentage of the collection, it is plausible to approximate statistics for nonrelevant documents by statistics from the whole collection. Under this assumption, u t (the probability of term occurrence in nonrelevant documents for a query) is t/N and (83) we see that we are now adding the two log scaled components rather than multiplying them. Exercises. Work through the derivation of Equation 74 from and 3()I . What are the differences between standard vector space tf-idf weighting and the BIM probabilistic retrieval model (in the case where no document relevance information is available)? Let X t be a random variable indicating whether the term t appears in a document. Suppose we have R relevant documents in the document collection and that X t = 1 in s of the documents. Take the observed data to be just these observations of X t for each document in R. Show that the MLE for the parameter p t = P(X t=1 R=1,{q}), that is, the value for p t which maximizes the probability of the observed data, is p t = s/ R. Describe the differences between vector space relevance feedback and probabilistic relevance feedback. """
iir 11 4,An appraisal and some extensions,Language model,"BM25 WEIGHTS, OKAPI WEIGHTING, BAYESIAN NETWORKS"," An appraisal of probabilistic models Probabilistic methods are one of the oldest formal models in IR. Already in the 1970s they were held out as an opportunity to place IR on a firmer theoretical footing, and with the resurgence of probabilistic methods in computational linguistics in the 1990s, that hope has returned, and probabilistic methods are again one of the currently hottest topics in IR. Traditionally, probabilistic IR has had neat ideas but the methods have never won on performance. Getting reasonable approximations of the needed probabilities for a probabilistic IR model is possible, but it requires some major assumptions. In the BIM these are: a Boolean representation of documents/queries/relevance term independence terms not in the query don't affect the outcome document relevance values are independent It is perhaps the severity of the modeling assumptions that makes achieving good performance difficult. A general problem seems to be that probabilistic models either require partial relevance information or else only allow for deriving apparently inferior term weighting models. Things started to change in the 1990s when the BM25 weighting scheme, which we discuss in the next section, showed very good performance, and started to be adopted as a term weighting scheme by many groups. The difference between ``vector space'' and ``probabilistic'' IR systems is not that great: in either case, you build an information retrieval scheme in the exact same way that we discussed in Chapter 7 . For a probabilistic IR system, it's just that, at the end, you score queries not by cosine similarity and tf-idf in a vector space, but by a slightly different formula motivated by probability theory. Indeed, sometimes people have changed an existing vector-space IR system into an effectively probabilistic system simply by adopted term weighting formulas from probabilistic models. In this section, we briefly present three extensions of the traditional probabilistic model, and in the next chapter, we look at the somewhat different probabilistic language modeling approach to IR. Tree-structured dependencies between terms ...endent on a term x k if there is an arrow x k x i.}{figure} Some of the assumptions of the BIM can be removed. For example, we can remove the assumption that terms are independent. This assumption is very far from true in practice. A case that particularly violates this assumption is term pairs like Hong and Kong, which are strongly dependent. But dependencies can occur in various complex configurations, such as between the set of terms New, York, England, City, Stock, Exchange, and University. van Rijsbergen (1979) proposed a simple, plausible model which allowed a tree structure of term dependencies, as in Figure 11.1 . In this model each term can be directly dependent on only one other term, giving a tree structure of dependencies. When it was invented in the 1970s, estimation problems held back the practical success of this model, but the idea was reinvented as the Tree Augmented Naive Bayes model by Friedman and Goldszmidt (1996), who used it with some success on various machine learning data sets. Okapi BM25: a non-binary model The BIM was originally designed for short catalog records and abstracts of fairly consistent length, and it works reasonably in these contexts, but for modern full-text search collections, it seems clear that a model should pay attention to term frequency and document length, as in Chapter 6 . The BM25 weighting scheme , often called Okapi weighting , after the system in which it was first implemented, was developed as a way of building a probabilistic model sensitive to these quantities while not introducing too many additional parameters into the model (Sp rck Jones et al., 2000). We will not develop the full theory behind the model here, but just present a series of forms that build up to the standard form now used for document scoring. The simplest score for document d is just idf weighting of the query terms present, as in Equation 76: Sometimes, an alternative version of idf is used. If we start with the formula in Equation 75 but in the absence of relevance feedback information we estimate that S = s = 0, then we get an alternative idf formulation as follows: This variant behaves slightly strangely: if a term occurs in over half the documents in the collection then this model gives a negative term weight, which is presumably undesirable. But, assuming the use of a stop list, this normally doesn't happen, and the value for each summand can be given a floor of 0. We can improve on Equation 84 by factoring in the frequency of each term and document length: Here, {td} is the frequency of term t in document d, and L d and L {ave} are the length of document d and the average document length for the whole collection. The variable k 1 is a positive tuning parameter that calibrates the document term frequency scaling. A k 1 value of 0 corresponds to a binary model (no term frequency), and a large value corresponds to using raw term frequency. b is another tuning parameter (0 b 1) which determines the scaling by document length: b = 1 corresponds to fully scaling the term weight by the document length, while b = 0 corresponds to no length normalization. If the query is long, then we might also use similar weighting for query terms. This is appropriate if the queries are paragraph long information needs, but unnecessary for short queries. with {tq} being the frequency of term t in the query q, and k 3 being another positive tuning parameter that this time calibrates term frequency scaling of the query. In the equation presented, there is no length normalization of queries (it is as if b = 0 here). Length normalization of the query is unnecessary because retrieval is being done with respect to a single fixed query. The tuning parameters of these formulas should ideally be set to optimize performance on a development test collection (see page 8.1 ). That is, we can search for values of these parameters that maximize performance on a separate development test collection (either manually or with optimization methods such as grid search or something more advanced), and then use these parameters on the actual test collection. In the absence of such optimization, experiments have shown reasonable values are to set k 1 and k 3 to a value between 1.2 and 2 and b = 0.75. If we have relevance judgments available, then we can use the full form of smoothed-rf in place of the approximation (N/ t) introduced in prob-idf: RSV d = {t q} [[{( VR t + {1}{... .../(N - t - VR + VR t + {1}{2})} Here, VR t, NVR t, and VR are used as in Section 11.3.4 . The first part of the expression reflects relevance feedback (or just idf weighting if no relevance information is available), the second implements document term frequency and document length scaling, and the third considers term frequency in the query. Rather than just providing a term weighting method for terms in a user's query, relevance feedback can also involve augmenting the query (automatically or with manual review) with some (say, 10-20) of the top terms in the known-relevant documents as ordered by the relevance factor {c} t from Equation 75, and the above formula can then be used with such an augmented query vector {q}. The BM25 term weighting formulas have been used quite widely and quite successfully across a range of collections and search tasks. Especially in the TREC evaluations, they performed well and were widely adopted by many groups. See Sp rck Jones et al. (2000) for extensive motivation and discussion of experimental results. Bayesian network approaches to IR Turtle and Croft (1989;1991) introduced into information retrieval the use of Bayesian networks (Jensen and Jensen, 2001), a form of probabilistic graphical model. We skip the details because fully introducing the formalism of Bayesian networks would require much too much space, but conceptually, Bayesian networks use directed graphs to show probabilistic dependencies between variables, as in Figure 11.1 , and have led to the development of sophisticated algorithms for propagating influence so as to allow learning and inference with arbitrary knowledge within arbitrary directed acyclic graphs. Turtle and Croft used a sophisticated network to better model the complex dependencies between a document and a user's information need. The model decomposes into two parts: a document collection network and a query network. The document collection network is large, but can be precomputed: it maps from documents to terms to concepts. The concepts are a thesaurus-based expansion of the terms appearing in the document. The query network is relatively small but a new network needs to be built each time a query comes in, and then attached to the document network. The query network maps from query terms, to query subexpressions (built using probabilistic or ``noisy'' versions of AND and OR operators), to the user's information need. The result is a flexible probabilistic network which can generalize various simpler Boolean and probabilistic models. Indeed, this is the primary case of a statistical ranked retrieval model that naturally supports structured query operators. The system allowed efficient large-scale retrieval, and was the basis of the InQuery text retrieval system, built at the University of Massachusetts. This system performed very well in TREC evaluations and for a time was sold commercially. On the other hand, the model still used various approximations and independence assumptions to make parameter estimation and computation possible. There has not been much follow-on work along these lines, but we would note that this model was actually built very early on in the modern era of using Bayesian networks, and there have been many subsequent developments in the theory, and the time is perhaps right for a new generation of Bayesian network-based information retrieval systems. Language models for information retrieval A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. This approach thus provides a different realization of some of the basic ideas for document ranking which we saw in Section 6.2 . Instead of overtly modeling the probability P(R=1 q,d) of relevance of a document d to a query q, as in the traditional probabilistic approach to IR (Chapter 11 ), the basic language modeling approach instead builds a probabilistic language model M d from each document d, and ranks documents based on the probability of the model generating the query: P(q M d). In this chapter, we first introduce the concept of language models (Section 12.1 ) and then describe the basic and most commonly used language modeling approach to IR, the Query Likelihood Model (Section 12.2 ). After some comparisons between the language modeling approach and other approaches to IR (Section 12.3 ), we finish by briefly describing various extensions to the language modeling approach (Section 12.4 ). """
iir 12,Language models for information retrieval,Language model,," A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. This approach thus provides a different realization of some of the basic ideas for document ranking which we saw in Section 6.2 . Instead of overtly modeling the probability P(R=1 q,d) of relevance of a document d to a query q, as in the traditional probabilistic approach to IR (Chapter 11 ), the basic language modeling approach instead builds a probabilistic language model M d from each document d, and ranks documents based on the probability of the model generating the query: P(q M d). In this chapter, we first introduce the concept of language models (Section 12.1 ) and then describe the basic and most commonly used language modeling approach to IR, the Query Likelihood Model (Section 12.2 ). After some comparisons between the language modeling approach and other approaches to IR (Section 12.3 ), we finish by briefly describing various extensions to the language modeling approach (Section 12.4 ). """
iir 12 1 1,Finite automata and language models,Language model,"GENERATIVE MODEL, language, LANGUAGE MODEL, LIKELIHOOD RATIO","What do we mean by a document model generating a query? A traditional generative model of a language, of the kind familiar from formal language theory, can be used either to recognize or to generate strings. For example, the finite automaton shown in Figure 12.1 can generate strings that include the examples shown. The full set of strings that can be generated is called the language of the automaton.[*] If instead each node has a probability distribution over generating different terms, we have a language model. The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary. That is, for a language model M over an alphabet : One simple kind of language model is equivalent to a probabilistic finite automaton consisting of just a single node with a single probability distribution over producing different terms, so that {t V} P(t) = 1, as shown in Figure 12.2 . After generating each word, we decide whether to stop or to loop around and then produce another word, and so the model also requires a probability of stopping in the finishing state. Such a model places a probability distribution over any sequence of words. By construction, it also provides a model for generating text according to its distribution. Worked example. To find the probability of a word sequence, we just multiply the probabilities which the model gives to each word in the sequence, together with the probability of continuing or stopping after producing each word. For example, As you can see, the probability of a particular string/document, is usually a very small number! Here we stopped after generating frog the second time. The first line of numbers are the term emission probabilities, and the second line gives the probability of continuing or stopping after generating each word. An explicit stop probability is needed for a finite automaton to be a well-formed language model according to Equation 90. Nevertheless, most of the time, we will omit to include STOP and (1- {{stop}}) probabilities (as do most other authors). To compare two models for a data set, we can calculate their likelihood ratio , which results from simply dividing the probability of the data according to one model by the probability of the data according to the other model. Providing that the stop probability is fixed, its inclusion will not alter the likelihood ratio that results from comparing the likelihood of two language models generating a string. Hence, it will not alter the ranking of documents.[*] Nevertheless, formally, the numbers will no longer truly be probabilities, but only proportional to probabilities. See Exercise 12.1.3 . End worked example. Figure 12.3: Partial specification of two unigram language models. and we see that P(s M 1)> P(s M 2). We present the formulas here in terms of products of probabilities, but, as is common in probabilistic applications, in practice it is usually best to work with sums of log probabilities (cf. page 13.2 ). End worked example. """
iir 12 1 2,Types of language models,Language model,"UNIGRAM LANGUAGE model, BIGRAM LANGUAGE model","How do we build probabilities over sequences of terms? We can always use the chain rule from Equation 56 to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events: (96) and even more complex grammar-based language models such as probabilistic context-free grammars. Such models are vital for tasks like speech recognition , spelling correction , and machine translation , where you need the probability of a term conditioned on surrounding context. However, most language-modeling work in IR has used unigram language models. IR is not the place where you most immediately need complex language models, since IR does not directly depend on the structure of sentences to the extent that other tasks like speech recognition do. Unigram models are often sufficient to judge the topic of a text. Moreover, as we shall see, IR language models are frequently estimated from a single document and so it is questionable whether there is enough training data to do more. Losses from data sparseness (see the discussion on page 13.2 ) tend to outweigh any gains from richer models. This is an example of the bias-variance tradeoff (cf. secbiasvariance): With limited training data, a more constrained model tends to perform better. In addition, unigram models are more efficient to estimate and apply than higher-order models. Nevertheless, the importance of phrase and proximity queries in IR in general suggests that future work should make use of more sophisticated language models, and some has begun to lmir-refs. Indeed, making this move parallels the model of van Rijsbergen in Chapter 11 (page 11.4.2 ). """
iir 12 1 3,Multinomial distributions over words,Language model,MULTINOMIAL DISTRIBUTION,"Under the unigram language model the order of words is irrelevant, and so such models are often called ``bag of words'' models, as discussed in Chapter 6 (page 6.2 ). Even though there is no conditioning on preceding context, this model nevertheless still gives the probability of a particular ordering of terms. However, any other ordering of this bag of terms will have the same probability. So, really, we have a multinomial distribution over words. So long as we stick to unigram models, the language model name and motivation could be viewed as historical rather than necessary. We could instead just refer to the model as a multinomial model. From this perspective, the equations presented above do not present the multinomial probability of a bag of words, since they do not sum over all possible orderings of those words, as is done by the multinomial coefficient (the first term on the right-hand side) in the standard presentation of a multinomial model: (97) Here, L d = {1 i M} {t i,d} is the length of document d, M is the size of the term vocabulary, and the products are now over the terms in the vocabulary, not the positions in the document. However, just as with STOP probabilities, in practice we can also leave out the multinomial coefficient in our calculations, since, for a particular bag of words, it will be a constant, and so it has no effect on the likelihood ratio of two different models generating a particular bag of words. Multinomial distributions also appear in Section 13.2 . The fundamental problem in designing language models is that we do not know what exactly we should use as the model M d. However, we do generally have a sample of text that is representative of that model. This problem makes a lot of sense in the original, primary uses of language models. For example, in speech recognition, we have a training sample of (spoken) text. But we have to expect that, in the future, users will use different words and in different sequences, which we have never observed before, and so the model has to generalize beyond the observed data to allow unknown words and sequences. This interpretation is not so clear in the IR case, where a document is finite and usually fixed. The strategy we adopt in IR is as follows. We pretend that the document d is only a representative sample of text drawn from a model distribution, treating it like a fine-grained topic. We then estimate a language model from this sample, and use that model to calculate the probability of observing any word sequence, and, finally, we rank documents according to their probability of generating the query. """
iir 12 2 1,The query likelihood model,Language model,"QUERY LIKELIHOOD model, LINEAR interpolation, BAYESIAN SMOOTHING","Using query likelihood language models in IR Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model . In it, we construct from each document d in the collection a language model M d. Our goal is to rank documents by P(d q), where the probability of a document is interpreted as the likelihood that it is relevant to the query. Using Bayes rule (as introduced in probirsec), we have: (99) where, again K q = L d!/( {t 1,d}! {t 2,d}! {t M,d}!) is the multinomial coefficient for the query q, which we will henceforth ignore, since it is a constant for a particular query. For retrieval based on a language model (henceforth LM ), we treat the generation of queries as a random process. The approach is to Infer a LM for each document. Estimate P(q M {d i}), the probability of generating the query according to each of these document models. Rank the documents according to these probabilities. The intuition of the basic model is that the user has a prototype document in mind, and generates a query based on words that appear in this document. Often, users have a reasonable idea of terms that are likely to occur in documents of interest and they will choose query terms that distinguish these documents from others in the collection.[*]Collection statistics are an integral part of the language model, rather than being used heuristically as in many other approaches. """
iir 12 2 2,Estimating the query generation probability,Language model,,"In this section we describe how to estimate P(q M d). The probability of producing the query given the LM M d of document d using maximum likelihood estimation ( MLE ) and the unigram assumption is: where M d is the language model of document d, {t,d} is the (raw) term frequency of term t in document d, and L d is the number of tokens in document d. That is, we just count up how often each word occurred, and divide through by the total number of words in the document d. This is the same method of calculating an MLE as we saw in Section 11.3.2 , but now using a multinomial over word counts. The classic problem with using language models is one of estimation (the {{x}} symbol on the P's is used above to stress that the model is estimated): terms appear very sparsely in documents. In particular, some words will not have appeared in the document at all, but are possible words for the information need, which the user may have used in the query. If we estimate {P}(t M d) = 0 for a term missing from a document d, then we get a strict conjunctive semantics: documents will only give a query non-zero probability if all of the query terms appear in the document. Zero probabilities are clearly a problem in other uses of language models, such as when predicting the next word in a speech recognition application, because many words will be sparsely represented in the training data. It may seem rather less clear whether this is problematic in an IR application. This could be thought of as a human-computer interface issue: vector space systems have generally preferred more lenient matching, though recent web search developments have tended more in the direction of doing searches with such conjunctive semantics. Regardless of the approach here, there is a more general problem of estimation: occurring words are also badly estimated; in particular, the probability of words occurring once in the document is normally overestimated, since their one occurrence was partly by chance. The answer to this (as we saw in probtheory) is smoothing. But as people have come to understand the LM approach better, it has become apparent that the role of smoothing in this model is not only to avoid zero probabilities. The smoothing of terms actually implements major parts of the term weighting component (Exercise 12.2.3 ). It is not just that an unsmoothed model has conjunctive semantics; an unsmoothed model works badly because it lacks parts of the term weighting component. Thus, we need to smooth probabilities in our document language models: to discount non-zero probabilities and to give some probability mass to unseen words. There's a wide space of approaches to smoothing probability distributions to deal with this problem. In Section 11.3.2 , we already discussed adding a number (1, 1/2, or a small ) to the observed counts and renormalizing to give a probability distribution.[*]In this section we will mention a couple of other smoothing methods, which involve combining observed counts with a more general reference probability distribution. The general approach is that a non-occurring term should be possible in a query, but its probability should be somewhat close to but no more likely than would be expected by chance from the whole collection. That is, if {t,d} = 0 then where t is the raw count of the term in the collection, and T is the raw size (number of tokens) of the entire collection. A simple idea that works well in practice is to use a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire collection: where 0 < < 1 and M c is a language model built from the entire document collection. This mixes the probability from the document with the general collection frequency of the word. Such a model is referred to as a linear interpolation language model.[*]Correctly setting is important to the good performance of this model. An alternative is to use a language model built from the whole collection as a prior distribution in a Bayesian updating process (rather than a uniform distribution, as we saw in Section 11.3.2 ). We then get the following equation: Both of these smoothing methods have been shown to perform well in IR experiments; we will stick with the linear interpolation smoothing method for the rest of this section. While different in detail, they are both conceptually similar: in both cases the probability estimate for a word present in the document combines a discounted MLE and a fraction of the estimate of its prevalence in the whole collection, while for words not present in a document, the estimate is just a fraction of the estimate of the prevalence of the word in the whole collection. The role of smoothing in LMs for IR is not simply or principally to avoid estimation problems. This was not clear when the models were first proposed, but it is now understood that smoothing is essential to the good properties of the models. The reason for this is explored in Exercise 12.2.3 . The extent of smoothing in these two models is controlled by the and parameters: a small value of or a large value of means more smoothing. This parameter can be tuned to optimize performance using a line search (or, for the linear interpolation model, by other methods, such as the expectation maximimization algorithm; see modelclustering). The value need not be a constant. One approach is to make the value a function of the query size. This is useful because a small amount of smoothing (a ``conjunctive-like'' search) is more suitable for short queries, while a lot of smoothing is more suitable for long queries. To summarize, the retrieval ranking for a query q under the basic LM for IR we have been considering is given by: This equation captures the probability that the document that the user had in mind was in fact d. Worked example. Suppose the document collection contains two documents: d 1: Xyzzy reports a profit but revenue is down d 2: Quorus narrows quarter loss but revenue decreases further The model will be MLE unigram models from the documents and collection, mixed with = 1/2. Suppose the query is revenue down. Then: So, the ranking is d 1 > d 2. End worked example. """
iir 12 2 3,Ponte and Croft's Experiments,Language model,,"Ponte and Croft (1998) present the first experiments on the language modeling approach to information retrieval. Their basic approach is the model that we have presented until now. However, we have presented an approach where the language model is a mixture of two multinomials, much as in (Miller et al., 1999, Hiemstra, 2000) rather than Ponte and Croft's multivariate Bernoulli model. The use of multinomials has been standard in most subsequent work in the LM approach and experimental results in IR, as well as evidence from text classification which we consider in Section 13.3 , suggests that it is superior. Ponte and Croft argued strongly for the effectiveness of the term weights that come from the language modeling approach over traditional tf-idf weights. We present a subset of their results in Figure 12.4 where they compare tf-idf to language modeling by evaluating TREC topics 202-250 over TREC disks 2 and 3. The queries are sentence-length natural language queries. The language modeling approach yields significantly better results than their baseline tf-idf based term weighting approach. And indeed the gains shown here have been extended in subsequent work."""
iir 12 3,Language modeling versus other approaches in IR,Language model,,"The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech and language processing. As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used, and hence the performance of the model. The major issue is estimation of the document model, such as choices of how to smooth it effectively. The model has achieved very good retrieval results. Compared to other probabilistic approaches, such as the BIM from Chapter 11 , the main difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the BIM approach). But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss. The LM approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the tools and methods of language modeling from speech and natural language processing. The resulting model is mathematically precise, conceptually simple, computationally tractable, and intuitively appealing. This seems similar to the situation with XML retrieval (Chapter 10 ): there the approaches that assume queries and documents are objects of the same type are also among the most successful. On the other hand, like all IR models, you can also raise objections to the model. The assumption of equivalence between document and information need representation is unrealistic. Current LM approaches use very simple models of language, usually unigram models. Without an explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences. It also seems necessary to move beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators. Subsequent work in the LM approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch between the query language and the document language. The model has significant relations to traditional tf-idf models. Term frequency is directly represented in tf-idf models, and much recent work has recognized the importance of document length normalization. The effect of doing a mixture of document generation probability with collection generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents. In most concrete realizations, the models share treating terms as if they were independent. On the other hand, the intuitions are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term frequency and document length are used differ. If you are concerned mainly with performance numbers, recent work has shown the LM approach to be very effective in retrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is perhaps still insufficient evidence that its performance so greatly exceeds that of a well-tuned traditional vector space retrieval system as to justify changing an existing implementation. """
iir 12 4,Extended language modeling approaches,Document classification,"DOCUMENT LIKELIHOOD MODEL, KULLBACK-LEIBLER DIVERGENCE, TRANSLATION MODEL","In this section we briefly mention some of the work that extends the basic language modeling approach. There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model M d generating the query, you can look at the probability of a query language model M q generating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less text available to estimate a language model based on the query text, and so the model will be worse estimated, and will have to depend more on being smoothed with some other language model. On the other hand, it is easy to see how to incorporate relevance feedback into such a model: you can expand the query with terms taken from relevant documents in the usual way and hence update the language model M q (Zhai and Lafferty, 2001a). Indeed, with appropriate modeling choices, this approach leads to the BIM model of Chapter 11 . The relevance model of Lavrenko and Croft (2001) is an instance of a document likelihood model, which incorporates pseudo-relevance feedback into a language modeling approach. It achieves very strong empirical results. Figure 12.5: Three ways of developing the language modeling approach: (a) query likelihood, (b) document likelihood, and (c) model comparison. Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other. Lafferty and Zhai (2001) lay out these three ways of thinking about the problem, which we show in Figure 12.5 , and develop a general risk minimization approach for document retrieval. For instance, one way to model the risk of returning a document d as relevant to a query q is to use the Kullback-Leibler (KL) divergence between their respective language models: KL divergence is an asymmetric divergence measure originating in information theory, which measures how bad the probability distribution M q is at modeling M d (Manning and SchÔäêë tze, 1999, Cover and Thomas, 1991). Lafferty and Zhai (2001) present results suggesting that a model comparison approach outperforms both query-likelihood and document-likelihood approaches. One disadvantage of using KL divergence as a ranking function is that scores are not comparable across queries. This does not matter for ad hoc retrieval, but is important in other applications such as topic tracking. Kraaij and Spitters (2003) suggest an alternative proposal which models similarity as a normalized log-likelihood ratio (or, equivalently, as a difference between cross-entropies). Basic LMs do not address issues of alternate expression, that is, synonymy, or any deviation in use of language between queries and documents. Berger and Lafferty (1999) introduce translation models to bridge this query-document gap. A translation model lets you generate query words not in a document by translation to alternate terms with similar meaning. This also provides a basis for performing cross-language IR. We assume that the translation model can be represented by a conditional probability distribution T() between vocabulary terms. The form of the translation query generation model is then: The term P(v M d) is the basic document language model, and the term T(t v) performs translation. This model is clearly more computationally intensive and we need to build a translation model. The translation model is usually built using separate resources (such as a traditional thesaurus or bilingual dictionary or a statistical machine translation system's translation dictionary), but can be built using the document collection if there are pieces of text that naturally paraphrase or summarize other pieces of text. Candidate examples are documents and their titles or abstracts, or documents and anchor-text pointing to them in a hypertext environment. Building extended LM approaches remains an active area of research. In general, translation models, relevance feedback models, and model comparison approaches have all been demonstrated to improve performance over the basic query likelihood LM. """
iir 13,Text classification and Naive Bayes,Document classification,"STANDING QUERY, CLASSIFICATION, ROUTING, FILTERING, TEXT CLASSIFICATION, SENTIMENT DETECTION, EMAIL SORTING, VERTICAL SEARCH ENGINE, RULES IN TEXT CLASSIFICATION, STATISTICAL TEXT CLASSIFICATION, LABELING","Thus far, this book has mainly discussed the process of ad hoc retrieval , where users have transient information needs that they try to address by posing one or more queries to a search engine. However, many users have ongoing information needs. For example, you might need to track developments in multicore computer chips. One way of doing this is to issue the query multicore and computer and chip against an index of recent newswire articles each morning. In this and the following two chapters we examine the question: How can this repetitive task be automated? To this end, many systems support standing queries . A standing query is like any other query except that it is periodically executed on a collection to which new documents are incrementally added over time. If your standing query is just multicore and computer and chip, you will tend to miss many relevant new articles which use other terms such as multicore processors. To achieve good recall, standing queries thus have to be refined over time and can gradually become quite complex. In this example, using a Boolean search engine with stemming, you might end up with a query like (multicore or multi-core) and (chip or processor or microprocessor). To capture the generality and scope of the problem space to which standing queries belong, we now introduce the general notion of a classification problem. Given a set of classes, we seek to determine which class(es) a given object belongs to. In the example, the standing query serves to divide new newswire articles into the two classes: documents about multicore computer chips and documents not about multicore computer chips. We refer to this as two-class classification. Classification using standing queries is also called routing or filtering and will be discussed further in Section 15.3.1 . A class need not be as narrowly focused as the standing query multicore computer chips. Often, a class is a more general subject area like China or coffee. Such more general classes are usually referred to as topics , and the classification task is then called text classification , text categorization , topic classification , or topic spotting . An example for China appears in Figure 13.1 . Standing queries and topics differ in their degree of specificity, but the methods for solving routing, filtering, and text classification are essentially the same. We therefore include routing and filtering under the rubric of text classification in this and the following chapters. The notion of classification is very general and has many applications within and beyond information retrieval (IR). For instance, in computer vision, a classifier may be used to divide images into classes such as landscape, portrait, and neither. We focus here on examples from information retrieval such as: Several of the preprocessing steps necessary for indexing as discussed in Chapter 2 : detecting a document's encoding (ASCII, Unicode UTF-8 etc; page 2.1.1 ); word segmentation (Is the white space between two letters a word boundary or not? page 24 ) ; truecasing (page 2.2.3 ); and identifying the language of a document (page 2.5 ). The automatic detection of spam pages (which then are not included in the search engine index). The automatic detection of sexually explicit content (which is included in search results only if the user turns an option such as SafeSearch off). Sentiment detection or the automatic classification of a movie or product review as positive or negative. An example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems. Personal email sorting . A user may have folders like talk announcements, electronic bills, email from family and friends, and so on, and may want a classifier to classify each incoming email and automatically move it to the appropriate folder. It is easier to find messages in sorted folders than in a very large inbox. The most common case of this application is a spam folder that holds all suspected spam messages. Topic-specific or vertical search. Vertical search engines restrict searches to a particular topic. For example, the query computer science on a vertical search engine for the topic China will return a list of Chinese computer science departments with higher precision and recall than the query computer science China on a general purpose search engine. This is because the vertical search engine does not include web pages in its index that contain the term china in a different sense (e.g., referring to a hard white ceramic), but does include relevant pages even if they do not explicitly mention the term China. Finally, the ranking function in ad hoc information retrieval can also be based on a document classifier as we will explain in Section 15.4 . This list shows the general importance of classification in IR. Most retrieval systems today contain multiple components that use some form of classifier. The classification task we will use as an example in this book is text classification. A computer is not essential for classification. Many classification tasks have traditionally been solved manually. Books in a library are assigned Library of Congress categories by a librarian. But manual classification is expensive to scale. The multicore computer chips example illustrates one alternative approach: classification by the use of standing queries - which can be thought of as rules - most commonly written by hand. As in our example (multicore or multi-core) and (chip or processor or microprocessor), rules are sometimes equivalent to Boolean expressions. A rule captures a certain combination of keywords that indicates a class. Hand-coded rules have good scaling properties, but creating and maintaining them over time is labor intensive. A technically skilled person (e.g., a domain expert who is good at writing regular expressions) can create rule sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly; however, it can be hard to find someone with this specialized skill. Apart from manual classification and hand-crafted rules, there is a third approach to text classification, namely, machine learning-based text classification. It is the approach that we focus on in the next several chapters. In machine learning, the set of rules or, more generally, the decision criterion of the text classifier, is learned automatically from training data. This approach is also called statistical text classification if the learning method is statistical. In statistical text classification, we require a number of good example documents (or training documents) for each class. The need for manual classification is not eliminated because the training documents come from a person who has labeled them - where labeling refers to the process of annotating each document with its class. But labeling is arguably an easier task than writing rules. Almost anybody can look at a document and decide whether or not it is related to China. Sometimes such labeling is already implicitly part of an existing workflow. For instance, you may go through the news articles returned by a standing query each morning and give relevance feedback (cf. Chapter 9 ) by moving the relevant articles to a special folder like multicore-processors. We begin this chapter with a general introduction to the text classification problem including a formal definition (Section 13.1 ); we then cover Naive Bayes, a particularly simple and effective classification method (Sections 13.2-13.4). All of the classification algorithms we study represent documents in high-dimensional spaces. To improve the efficiency of these algorithms, it is generally desirable to reduce the dimensionality of these spaces; to this end, a technique known as feature selection is commonly applied in text classification as discussed in Section 13.5 . Section 13.6 covers evaluation of text classification. In the following chapters, Chapters 14 15 , we look at two other families of classification methods, vector space classifiers and support vector machines. """
iir 13 1,The text classification problem,"Document classification,Naive Bayes classifier","DOCUMENT SPACE class, TRAINING SET, LEARNING METHOD, CLASSIFIER, SUPERVISED LEARNING, TEST SET"," In text classification, we are given a description {X} of a document, where {X} is the document space ; and a fixed set of classes {C} = . Classes are also called categories or labels . Typically, the document space {X} is some type of high-dimensional space, and the classes are human defined for the needs of an application, as in the examples China and documents that talk about multicore computer chips above. We are given a training set of labeled documents , where {X} {C}. For example: for the one-sentence document Beijing joins the World Trade Organization and the class (or label) China. Using a learning method or learning algorithm , we then wish to learn a classifier or classification function that maps documents to classes: This type of learning is called supervised learning because a supervisor (the human who defines the classes and labels training documents) serves as a teacher directing the learning process. We denote the supervised learning method by Gamma and write Gamma() = . The learning method Gamma takes the training set as input and returns the learned classification function . Most names for learning methods Gamma are also used for classifiers . We talk about the Naive Bayes (NB) learning method Gamma when we say that ``Naive Bayes is robust,'' meaning that it can be applied to many different learning problems and is unlikely to produce classifiers that fail catastrophically. But when we say that ``Naive Bayes had an error rate of 20%,'' we are describing an experiment in which a particular NB classifier (which was produced by the NB learning method) had a 20% error rate in an application. Figure 13.1 shows an example of text classification from the Reuters-RCV1 collection, introduced in Section 4.2 , page 4.2 . There are six classes (UK, China, ..., sports), each with three training documents. We show a few mnemonic words for each document's content. The training set provides some typical examples for each class, so that we can learn the classification function . Once we have learned , we can apply it to the test set (or test data ), for example, the new document first private Chinese airline whose class is unknown. In Figure 13.1 , the classification function assigns the new document to class () = China, which is the correct assignment. The classes in text classification often have some interesting structure such as the hierarchy in Figure 13.1 . There are two instances each of region categories, industry categories, and subject area categories. A hierarchy can be an important aid in solving a classification problem; see Section 15.3.2 for further discussion. Until then, we will make the assumption in the text classification chapters that the classes form a set with no subset relationships between them. Definition eqn:gammadef stipulates that a document is a member of exactly one class. This is not the most appropriate model for the hierarchy in Figure 13.1 . For instance, a document about the 2008 Olympics should be a member of two classes: the China class and the sports class. This type of classification problem is referred to as an any-of problem and we will return to it in Section 14.5 . For the time being, we only consider one-of problems where a document is a member of exactly one class. Our goal in text classification is high accuracy on test data or new data - for example, the newswire articles that we will encounter tomorrow morning in the multicore chip example. It is easy to achieve high accuracy on the training set (e.g., we can simply memorize the labels). But high accuracy on the training set in general does not mean that the classifier will work well on new data in an application. When we use the training set to learn a classifier for test data, we make the assumption that training data and test data are similar or from the same distribution. We defer a precise definition of this notion to Section 14.6 . """
iir 13 2,Naive Bayes text classification,"Document classification,Naive Bayes classifier","MULTINOMIAL NAIVE BAYES, MAXIMUM A POSTERIORI CLASS, SPARSENESS, ADD-ONE SMOOTHING","The first supervised learning method we introduce is the multinomial Naive Bayes or multinomial NB model, a probabilistic learning method. The probability of a document d being in class c is computed as where P( c) is the conditional probability of term occurring in a document of class c.[*]We interpret P( c) as a measure of how much evidence contributes that c is the correct class. P(c) is the prior probability of a document occurring in class c. If a document's terms do not provide clear evidence for one class versus another, we choose the one that has a higher prior probability. For example, In text classification, our goal is to find the best class for the document. The best class in NB classification is the most likely or maximum a posteriori ( MAP ) class c {map}: We write {P} for P because we do not know the true values of the parameters P() and P( ), but estimate them from the training set as we will see in a moment. In Equation 114, many conditional probabilities are multiplied, one for each position 1 n d. This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable; Equation 115 has a simple interpretation. Each conditional parameter {P}( ) is a weight that indicates how good an indicator is for . Similarly, the prior {P}() is a weight that indicates the relative frequency of . More frequent classes are more likely to be the correct class than infrequent classes. The sum of log prior and term weights is then a measure of how much evidence there is for the document being in the class, and Equation 115 selects the class for which we have the most evidence. We will initially work with this intuitive interpretation of the multinomial NB model and defer a formal derivation to Section 13.4 . How do we estimate the parameters {P}() and {P}( )? We first try the maximum likelihood estimate (MLE; probtheory), which is simply the relative frequency and corresponds to the most likely value of each parameter given the training data. For the priors this estimate is: {P}() = {N c}{N}, (116) where N c is the number of documents in class and N is the total number of documents. We estimate the conditional probability {P}( c) as the relative frequency of term in documents belonging to class c: where T {c} is the number of occurrences of in training documents from class c, including multiple occurrences of a term in a document. We have made the positional independence assumption here, which we will discuss in more detail in the next section: T {c} is a count of occurrences in all positions k in the documents in the training set. Thus, we do not compute different estimates for different positions and, for example, if a word occurs twice in a document, in positions k 1 and k 2, then The problem with the MLE estimate is that it is zero for a term-class combination that did not occur in the training data. If the term WTO in the training data only occurred in China documents, then the MLE estimates for the other classes, for example UK, will be zero: Now, the one-sentence document Britain is a member of the WTO will get a conditional probability of zero for UK because we are multiplying the conditional probabilities for all terms in Equation 113. Clearly, the model should assign a high probability to the UK class because the term Britain occurs. The problem is that the zero probability for WTO cannot be ``conditioned away,'' no matter how strong the evidence for the class UK from other features. The estimate is 0 because of sparseness : The training data are never large enough to represent the frequency of rare events adequately, for example, the frequency of WTO occurring in UK documents. Figure 13.2: Naive Bayes algorithm (multinomial model): Training and testing. where B= V is the number of terms in the vocabulary. Add-one smoothing can be interpreted as a uniform prior (each term occurs once for each class) that is then updated as evidence from the training data comes in. Note that this is a prior probability for the occurrence of a term as opposed to the prior probability of a class which we estimate in Equation 116 on the document level. We have now introduced all the elements we need for training and applying an NB classifier. The complete algorithm is described in Figure 13.2 . Worked example. For the example in Table 13.1 , the multinomial parameters we need to classify the test document are the priors {P}(c) = 3/4 and {P}({c}) = 1/4 and the following conditional probabilities: The denominators are (8+6) and (3+6) because the lengths of text c and text {{c}} are 8 and 3, respectively, and because the constant B in Equation 119 is 6 as the vocabulary consists of six terms. We then get: Thus, the classifier assigns the test document to c = China. The reason for this classification decision is that the three occurrences of the positive indicator Chinese in d 5 outweigh the occurrences of the two negative indicators Japan and Tokyo. End worked example. What is the time complexity of NB? The complexity of computing the parameters is Theta({C} V) because the set of parameters consists of {C} V conditional probabilities and {C} priors. The preprocessing necessary for computing the parameters (extracting the vocabulary, counting terms, etc.) can be done in one pass through the training data. The time complexity of this component is therefore Theta( L {ave}), where is the number of documents and L {ave} is the average length of a document. We use Theta( L {ave}) as a notation for Theta(T) here, where T is the length of the training collection. This is nonstandard; Theta(.) is not defined for an average. We prefer expressing the time complexity in terms of and L {ave} because these are the primary statistics used to characterize training collections. The time complexity of APPLYMULTINOMIALNB in Figure 13.2 is Theta({C} L {a}). L {a} and M {a} are the numbers of tokens and types, respectively, in the test document . APPLYMULTINOMIALNB can be modified to be Theta( L {a}+{C} M {a}) (Exercise 13.6 ). Finally, assuming that the length of test documents is bounded, Theta( L {a}+{C} M {a})= Theta({C} M {a}) because L {a} < b C M {a} for a fixed constant b.[*] Table 13.2 summarizes the time complexities. In general, we have {C} V < L {ave}, so both training and testing complexity are linear in the time it takes to scan the data. Because we have to look at the data at least once, NB can be said to have optimal time complexity. Its efficiency is one reason why NB is a popular text classification method. """
iir 13 2 1,Relation to multinomial unigram language model,"Document classification,Naive Bayes classifier",,"The multinomial NB model is formally identical to the multinomial unigram language model (Section 12.2.1 , page 12.2.1 ). In particular, Equation 113 is a special case of Equation 104 from page 12.2.1 , which we repeat here for =1: P(d q) P(d) {t q} P(t M d). (120) The document d in text classification (Equation 113) takes the role of the query in language modeling (Equation 120) and the classes c in text classification take the role of the documents d in language modeling. We used Equation 120 to rank documents according to the probability that they are relevant to the query q. In NB classification, we are usually only interested in the top-ranked class. We also used MLE estimates in Section 12.2.2 and encountered the problem of zero estimates owing to sparse data (page 12.2.2 ); but instead of add-one smoothing, we used a mixture of two distributions to address the problem there. Add-one smoothing is closely related to add-{1}{2} smoothing in Section 11.3.4 . """
iir 13 3,The Bernoulli model,Document classification,BERNOULLI MODEL," There are two different ways we can set up an NB classifier. The model we introduced in the previous section is the multinomial model . It generates one term from the vocabulary in each position of the document, where we assume a generative model that will be discussed in more detail in Section 13.4 (see also page 12.1.1 ). An alternative to the multinomial model is the multivariate Bernoulli model or Bernoulli model . It is equivalent to the binary independence model of Section 11.3 (page [*]), which generates an indicator for each term of the vocabulary, either 1 indicating presence of the term in the document or 0 indicating absence. Figure 13.3 presents training and testing algorithms for the Bernoulli model. The Bernoulli model has the same time complexity as the multinomial model. The different generation models imply different estimation strategies and different classification rules. The Bernoulli model estimates {P}() as the fraction of documents of class that contain term (Figure 13.3 , TRAINBERNOULLINB, line 8). In contrast, the multinomial model estimates {P}() as the fraction of tokens or fraction of positions in documents of class that contain term (Equation 119). When classifying a test document, the Bernoulli model uses binary occurrence information, ignoring the number of occurrences, whereas the multinomial model keeps track of multiple occurrences. As a result, the Bernoulli model typically makes many mistakes when classifying long documents. For example, it may assign an entire book to the class China because of a single occurrence of the term China. The models also differ in how nonoccurring terms are used in classification. They do not affect the classification decision in the multinomial model; but in the Bernoulli model the probability of nonoccurrence is factored in when computing P(c d) (Figure 13.3 , APPLYBERNOULLINB, Line 7). This is because only the Bernoulli NB model models absence of terms explicitly. Worked example. Applying the Bernoulli model to the example in Table 13.1 , we have the same estimates for the priors as before: {P}(c) = 3/4, {P}({c}) = 1/4. The conditional probabilities are: The denominators are (3+2) and (1+2) because there are three documents in c and one document in {c} and because the constant B in Equation 119 is 2 - there are two cases to consider for each term, occurrence and nonoccurrence. The scores of the test document for the two classes are and, analogously, Thus, the classifier assigns the test document to {c} = not-China. When looking only at binary occurrence and not at term frequency, Japan and Tokyo are indicators for {c} (2/3>1/5) and the conditional probabilities of Chinese for c and {c} are not different enough (4/5 vs. 2/3) to affect the classification decision. End worked example. Properties of Naive Bayes To gain a better understanding of the two models and the assumptions they make, let us go back and examine how we derived their classification rules in Chapters 11 12 . We decide class membership of a document by assigning it to the class with the probability (cf. probtheory), which we compute as follows: c {map} = { {C}} P( d) (121) = { {C}} {P(d)} (122) = { {C}} P(d) P(), (123) where Bayes' rule (Equation 59, page 59 ) is applied in (122) and we drop the denominator in the last step because P(d) is the same for all classes and does not affect the argmax. We can interpret Equation 123 as a description of the generative process we assume in Bayesian text classification. To generate a document, we first choose class with probability P() (top nodes in and 13.5 ). The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution P(d): { Multinomial} P(d) = P( 1,, ,, {n d} ) (124) { Bernoulli} P(d) = P( e 1,,e i,,e M ), (125) where 1,, {n d} is the sequence of terms as it occurs in d (minus terms that were excluded from the vocabulary) and e 1,, e i, ,e M is a binary vector of dimensionality M that indicates for each term whether it occurs in d or not. It should now be clearer why we introduced the document space {X} in Equation 112 when we defined the classification problem. A critical step in solving a text classification problem is to choose the document representation. 1,, {n d} and e 1,,e M are two different document representations. In the first case, {X} is the set of all term sequences (or, more precisely, sequences of term tokens). In the second case, {X} is ^M. We cannot use and 125 for text classification directly. For the Bernoulli model, we would have to estimate 2^M {C} different parameters, one for each possible combination of M values e i and a class. The number of parameters in the multinomial case has the same order of magnitude.[*]This being a very large quantity, estimating these parameters reliably is infeasible. To reduce the number of parameters, we make the Naive Bayes conditional independence assumption . We assume that attribute values are independent of each other given the class: { Multinomial} P(d) = P( 1,, {n d} ) =... ...1 n d} P(X = ) (126) { Bernoulli} P(d) = P( e 1,,e M ) = {1 i M} P(U i=e i c). (127) We have introduced two random variables here to make the two different generative models explicit. is the random variable for position in the document and takes as values terms from the vocabulary. P( =) is the probability that in a document of class the term will occur in position . i is the random variable for vocabulary term i and takes as values 0 (absence) and 1 (presence). {P}( i=1) is the probability that in a document of class the term i will occur - in any position and possibly multiple times. We illustrate the conditional independence assumption in and 13.5 . The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes. The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing. In reality, the conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. But as we will discuss shortly, NB models perform well despite the conditional independence assumption. Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position in the document. The position of a term in a document by itself does not carry information about the class. Although there is a difference between China sues France and France sues China, the occurrence of China in position 1 versus position 3 of the document is not useful in NB classification because we look at each term separately. The conditional independence assumption commits us to this way of processing the evidence. Also, if we assumed different term distributions for each position , we would have to estimate a different set of parameters for each . The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on. This again causes problems in estimation owing to data sparseness. For these reasons, we make a second independence assumption for the multinomial model, positional independence : The conditional probabilities for a term are the same independent of position in the document. (128) for all positions 1, 2, terms and classes . Thus, we have a single distribution of terms that is valid for all positions i and we can use as its symbol.[*]Positional independence is equivalent to adopting the bag of words model, which we introduced in the context of ad hoc retrieval in Chapter 6 (page 6.2 ). With conditional and positional independence assumptions, we only need to estimate Theta(M {C}) parameters P( ) (multinomial model) or P(e i) (Bernoulli model), one for each term-class combination, rather than a number that is at least exponential in M, the size of the vocabulary. The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude. To summarize, we generate a document in the multinomial model (Figure 13.4 ) by first picking a class C= with P() where C is a random variable taking values from {C} as values. Next we generate term in position with P(X = ) for each of the n d positions of the document. The X all have the same distribution over terms for a given . In the example in Figure 13.4 , we show the generation of 1, 2, 3, 4, 5 = {Beijing}, {and}, {Taipei}, {join}, {WTO} , corresponding to the one-sentence document Beijing and Taipei join WTO. For a completely specified document generation model, we would also have to define a distribution P(n d) over lengths. Without it, the multinomial model is a token generation model rather than a document generation model. We generate a document in the Bernoulli model (Figure 13.5 ) by first picking a class C= with P() and then generating a binary indicator e i for each term i of the vocabulary ( 1 i M). In the example in Figure 13.5 , we show the generation of e 1,e 2,e 3,e 4,e 5,e 6 = 0,1,0,1,1,1 , corresponding, again, to the one-sentence document Beijing and Taipei join WTO where we have assumed that and is a stop word. We compare the two models in Table 13.3 , including estimation equations and decision rules. Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class. This is hardly ever true for terms in documents. In many cases, the opposite is true. The pairs hong and kong or london and english in Figure 13.7 are examples of highly dependent terms. In addition, the multinomial model makes an assumption of positional independence. The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence. This bag-of-words model discards all information that is communicated by the order of words in natural language sentences. How can NB be a good text classifier when its model of natural language is so oversimplified? The answer is that even though the probability estimates of NB are of low quality, its classification decisions are surprisingly good. Consider a document d with true probabilities P(c 1 d)= 0.6 and P(c 2 d)= 0.4 as shown in Table 13.4 . Assume that d contains many terms that are positive indicators for c 1 and many terms that are negative indicators for c 2. Thus, when using the multinomial model in Equation 126, {P}(c 1) {1 n d} {P}( c 1) will be much larger than {P}(c 2) {1 n d} {P}( c 2) (0.00099 vs. 0.00001 in the table). After division by 0.001 to get well-formed probabilities for P(c d), we end up with one estimate that is close to 1.0 and one that is close to 0.0. This is common: The winning class in NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. But the classification decision is based on which class gets the highest score. It does not matter how accurate the estimates are. Despite the bad estimates, NB estimates a higher probability for c 1 and therefore assigns to the correct class in Table 13.4 . Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification. It excels if there are many equally important features that jointly contribute to the classification decision. It is also somewhat robust to noise features (as defined in the next section) and concept drift - the gradual change over time of the concept underlying a class like US president from Bill Clinton to George W. Bush (see Section 13.7 ). Classifiers like kNN knn can be carefully tuned to idiosyncratic properties of a particular time period. This will then hurt them when documents in the following time period have slightly different properties. The Bernoulli model is particularly robust with respect to concept drift. We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms. The most important indicators for a class are less likely to change. Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift. NB's main strength is its efficiency: Training and classification can be accomplished with one pass over the data. Because it combines efficiency with good accuracy it is often used as a baseline in text classification research. It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited. Table 13.5: A set of documents for which the NB independence assumptions are problematic. (1) He moved from London, Ontario, to London, England. (2) He moved from London, England, to London, Ontario. (3) He moved from England to London, Ontario. In this book, we discuss NB as a classifier for text. The independence assumptions do not hold for text. However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for data where the independence assumptions do hold. A variant of the multinomial model An alternative formalization of the represents each document d as an M-dimensional vector of counts {t 1,d},, {t M,d} where {t i,d} is the term frequency of t i in d. P(d) is then computed as follows (cf. Equation 99, page 12.2.1 ); (129) Note that we have omitted the multinomial factor. See Equation 99 (page 99 ). Equation 129 is equivalent to the sequence model in Equation 113 as P(X=t i c)^{ {t i,d}}=1 for terms that do not occur in d ( {t i,d}=0) and a term that occurs {t i,d} 1 times will contribute {t i,d} factors both in Equation 113 and in Equation 129. Exercises. Which of the documents in Table 13.5 have identical and different bag of words representations for (i) the Bernoulli model (ii) the multinomial model? If there are differences, describe them. The rationale for the positional independence assumption is that there is no useful information in the fact that a term occurs in position of a document. Find exceptions. Consider formulaic documents with a fixed document structure. Table 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the difference. """
iir 13 4,Properties of Naive Bayes,Document classification,"CONDITIONAL INDEPENDENCE ASSUMPTION, RANDOM VARIABLE X, RANDOM VARIABLE U, POSITIONAL INDEPENDENCE, RANDOM VARIABLE C, CONCEPT DRIFT, OPTIMAL CLASSIFIER"," To gain a better understanding of the two models and the assumptions they make, let us go back and examine how we derived their classification rules in Chapters 11 12 . We decide class membership of a document by assigning it to the class with the probability (cf. probtheory), which we compute as follows: c {map} = { {C}} P( d) (121) = { {C}} {P(d)} (122) = { {C}} P(d) P(), (123) where Bayes' rule (Equation 59, page 59 ) is applied in (122) and we drop the denominator in the last step because P(d) is the same for all classes and does not affect the argmax. We can interpret Equation 123 as a description of the generative process we assume in Bayesian text classification. To generate a document, we first choose class with probability P() (top nodes in and 13.5 ). The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution P(d): { Multinomial} P(d) = P( 1,, ,, {n d} ) (124) { Bernoulli} P(d) = P( e 1,,e i,,e M ), (125) where 1,, {n d} is the sequence of terms as it occurs in d (minus terms that were excluded from the vocabulary) and e 1,, e i, ,e M is a binary vector of dimensionality M that indicates for each term whether it occurs in d or not. It should now be clearer why we introduced the document space {X} in Equation 112 when we defined the classification problem. A critical step in solving a text classification problem is to choose the document representation. 1,, {n d} and e 1,,e M are two different document representations. In the first case, {X} is the set of all term sequences (or, more precisely, sequences of term tokens). In the second case, {X} is ^M. We cannot use and 125 for text classification directly. For the Bernoulli model, we would have to estimate 2^M {C} different parameters, one for each possible combination of M values e i and a class. The number of parameters in the multinomial case has the same order of magnitude.[*]This being a very large quantity, estimating these parameters reliably is infeasible. To reduce the number of parameters, we make the Naive Bayes conditional independence assumption . We assume that attribute values are independent of each other given the class: { Multinomial} P(d) = P( 1,, {n d} ) =... ...1 n d} P(X = ) (126) { Bernoulli} P(d) = P( e 1,,e M ) = {1 i M} P(U i=e i c). (127) We have introduced two random variables here to make the two different generative models explicit. is the random variable for position in the document and takes as values terms from the vocabulary. P( =) is the probability that in a document of class the term will occur in position . i is the random variable for vocabulary term i and takes as values 0 (absence) and 1 (presence). {P}( i=1) is the probability that in a document of class the term i will occur - in any position and possibly multiple times. Figure 13.4: The multinomial NB model. Figure 13.5: The Bernoulli NB model. We illustrate the conditional independence assumption in and 13.5 . The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes. The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing. In reality, the conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. But as we will discuss shortly, NB models perform well despite the conditional independence assumption. Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position in the document. The position of a term in a document by itself does not carry information about the class. Although there is a difference between China sues France and France sues China, the occurrence of China in position 1 versus position 3 of the document is not useful in NB classification because we look at each term separately. The conditional independence assumption commits us to this way of processing the evidence. Also, if we assumed different term distributions for each position , we would have to estimate a different set of parameters for each . The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on. This again causes problems in estimation owing to data sparseness. For these reasons, we make a second independence assumption for the multinomial model, positional independence : The conditional probabilities for a term are the same independent of position in the document. (128) for all positions 1, 2, terms and classes . Thus, we have a single distribution of terms that is valid for all positions i and we can use as its symbol.[*]Positional independence is equivalent to adopting the bag of words model, which we introduced in the context of ad hoc retrieval in Chapter 6 (page 6.2 ). With conditional and positional independence assumptions, we only need to estimate Theta(M {C}) parameters P( ) (multinomial model) or P(e i) (Bernoulli model), one for each term-class combination, rather than a number that is at least exponential in M, the size of the vocabulary. The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude. To summarize, we generate a document in the multinomial model (Figure 13.4 ) by first picking a class C= with P() where C is a random variable taking values from {C} as values. Next we generate term in position with P(X = ) for each of the n d positions of the document. The X all have the same distribution over terms for a given . In the example in Figure 13.4 , we show the generation of 1, 2, 3, 4, 5 = {Beijing}, {and}, {Taipei}, {join}, {WTO} , corresponding to the one-sentence document Beijing and Taipei join WTO. For a completely specified document generation model, we would also have to define a distribution P(n d) over lengths. Without it, the multinomial model is a token generation model rather than a document generation model. We generate a document in the Bernoulli model (Figure 13.5 ) by first picking a class C= with P() and then generating a binary indicator e i for each term i of the vocabulary ( 1 i M). In the example in Figure 13.5 , we show the generation of e 1,e 2,e 3,e 4,e 5,e 6 = 0,1,0,1,1,1 , corresponding, again, to the one-sentence document Beijing and Taipei join WTO where we have assumed that and is a stop word. We compare the two models in Table 13.3 , including estimation equations and decision rules. Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class. This is hardly ever true for terms in documents. In many cases, the opposite is true. The pairs hong and kong or london and english in Figure 13.7 are examples of highly dependent terms. In addition, the multinomial model makes an assumption of positional independence. The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence. This bag-of-words model discards all information that is communicated by the order of words in natural language sentences. How can NB be a good text classifier when its model of natural language is so oversimplified? The answer is that even though the probability estimates of NB are of low quality, its classification decisions are surprisingly good. Consider a document d with true probabilities P(c 1 d)= 0.6 and P(c 2 d)= 0.4 as shown in Table 13.4 . Assume that d contains many terms that are positive indicators for c 1 and many terms that are negative indicators for c 2. Thus, when using the multinomial model in Equation 126, {P}(c 1) {1 n d} {P}( c 1) will be much larger than {P}(c 2) {1 n d} {P}( c 2) (0.00099 vs. 0.00001 in the table). After division by 0.001 to get well-formed probabilities for P(c d), we end up with one estimate that is close to 1.0 and one that is close to 0.0. This is common: The winning class in NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. But the classification decision is based on which class gets the highest score. It does not matter how accurate the estimates are. Despite the bad estimates, NB estimates a higher probability for c 1 and therefore assigns to the correct class in Table 13.4 . Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification. It excels if there are many equally important features that jointly contribute to the classification decision. It is also somewhat robust to noise features (as defined in the next section) and concept drift - the gradual change over time of the concept underlying a class like US president from Bill Clinton to George W. Bush (see Section 13.7 ). Classifiers like kNN knn can be carefully tuned to idiosyncratic properties of a particular time period. This will then hurt them when documents in the following time period have slightly different properties. The Bernoulli model is particularly robust with respect to concept drift. We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms. The most important indicators for a class are less likely to change. Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift. NB's main strength is its efficiency: Training and classification can be accomplished with one pass over the data. Because it combines efficiency with good accuracy it is often used as a baseline in text classification research. It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited. In this book, we discuss NB as a classifier for text. The independence assumptions do not hold for text. However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for data where the independence assumptions do hold. """
iir 13 4 1, A variant of the multinomial model,"Document classification,Feature selection",,"A variant of the multinomial model An alternative formalization of the represents each document d as an M-dimensional vector of counts {t 1,d},, {t M,d} where {t i,d} is the term frequency of t i in d. P(d) is then computed as follows (cf. Equation 99, page 12.2.1 ); (129) Note that we have omitted the multinomial factor. See Equation 99 (page 99 ). Equation 129 is equivalent to the sequence model in Equation 113 as P(X=t i c)^{ {t i,d}}=1 for terms that do not occur in d ( {t i,d}=0) and a term that occurs {t i,d} 1 times will contribute {t i,d} factors both in Equation 113 and in Equation 129."""
iir 13 5,Feature selection,Document classification,"feature selection, noise feature, overfitting","Feature selection is the process of selecting a subset of the terms occurring in the training set and using only this subset as features in text classification. Feature selection serves two main purposes. First, it makes training and applying a classifier more efficient by decreasing the size of the effective vocabulary. This is of particular importance for classifiers that, unlike NB, are expensive to train. Second, feature selection often increases classification accuracy by eliminating noise features. A noise feature is one that, when added to the document representation, increases the classification error on new data. Suppose a rare term, say arachnocentric, has no information about a class, say China, but all instances of arachnocentric happen to occur in China documents in our training set. Then the learning method might produce a classifier that misassigns test documents containing arachnocentric to China. Such an incorrect generalization from an accidental property of the training set is called overfitting . Figure: Basic feature selection algorithm for selecting the best features. We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features). It may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification, but when discussing the bias-variance tradeoff in Section 14.6 (page [*]), we will see that weaker models are often preferable when limited training data are available. The basic feature selection algorithm is shown in Figure 13.6 . For a given class c, we compute a utility measure A(,c) for each term of the vocabulary and select the terms that have the highest values of A(,c). All other terms are discarded and not used in classification. We will introduce three different utility measures in this section: mutual information, A(,c) = I( ;C c); the ^2 test, A(,c) = X^2(t,c); and frequency, A(,c) = N(,c). Of the two NB models, the Bernoulli model is particularly sensitive to noise features. A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low. This section mainly addresses feature selection for two-class classification tasks like China versus not-China. Section 13.5.5 briefly discusses optimizations for systems with more than two classes."""
iir 13 5 1,Mutual information,Document classification,MUTUAL INFORMATION," A common feature selection method is to compute A(,c) as the expected mutual information (MI) of term and class c.[*] MI measures how much information the presence/absence of a term contributes to making the correct classification decision on c. Formally: I(;C) = {e } {e c } P(... ...rd,C=e c) 2 {P(=e ,C=e c)} {P(=e )P(C=e c)}, (130) where is a random variable that takes values e =1 (the document contains term ) and e =0 (the document does not contain ), as defined on page 13.4 , and C is a random variable that takes values e c=1 (the document is in class c) and e c=0 (the document is not in class c). We write and C c if it is not clear from context which term and class c we are referring to. For MLEs of the probabilities, Equation 130 is equivalent to Equation 131: I(;C) = { {11}}{N} 2 {N {11}}{... ...01}}{N} 2 {N {01}}{ {0.} {.1}} (131) +, { {10}}{N} 2 {N {10}... ...00}}{N} 2 {N {00}}{ {0.} {.0}} (132) where the are counts of documents that have the values of e and e c that are indicated by the two subscripts. For example, {10} is the number of documents that contain (e =1) and are not in c (e c=0). {1.} = {10}+ {11} is the number of documents that contain (e =1) and we count documents independent of class membership ( e c ). N= {00}+ {01}+ {10}+ {11} is the total number of documents. An example of one of the MLE estimates that transform Equation 130 into Equation 131 is P(=1,C=1)= {11}/N. Worked example. Consider the class poultry and the term export in Reuters-RCV1. The counts of the number of documents with the four possible combinations of indicator values are as follows: e c=e {{poultry}}=1 e c = e {{poultry}}=0 e =e {{export}} = 1 {11}=49 {10} = 27{,}652 e =e {{export}} = 0 {01} = 141 {00}=774{,}106 After plugging these values into Equation 131 we get: End worked example. To select terms 1,, for a given class, we use the feature selection algorithm in Figure 13.6 : We compute the utility measure as A(,c)=I(U ,C c) and select the terms with the largest values. Mutual information measures how much information - in the information-theoretic sense - a term contains about the class. If a term's distribution is the same in the class as it is in the collection as a whole, then I(;C) = 0. MI reaches its maximum value if the term is a perfect indicator for class membership, that is, if the term is present in a document if and only if the document is in the class. Figure 13.7: Features with high mutual information scores for six Reuters-RCV1 classes. Figure 13.7 shows terms with high mutual information scores for the six classes in Figure 13.1 .[*] The selected terms (e.g., london, uk, british for the class UK) are of obvious utility for making classification decisions for their respective classes. At the bottom of the list for UK we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding whether the document is in the class. As you might expect, keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier's accuracy. Figure 13.8: Effect of feature set size on accuracy for multinomial and Bernoulli models. Such an accuracy increase can be observed in Figure 13.8 , which shows F 1 as a function of vocabulary size after feature selection for Reuters-RCV1.[*] Comparing F 1 at 132,776 features (corresponding to selection of all features) and at 10-100 features, we see that MI feature selection increases F 1 by about 0.1 for the multinomial model and by more than 0.2 for the Bernoulli model. For the Bernoulli model, F 1 peaks early, at ten features selected. At that point, the Bernoulli model is better than the multinomial model. When basing a classification decision on only a few features, it is more robust to consider binary occurrence only. For the multinomial model (MI feature selection), the peak occurs later, at 100 features, and its effectiveness recovers somewhat at the end when we use all features. The reason is that the multinomial takes the number of occurrences into account in parameter estimation and classification and therefore better exploits a larger number of features than the Bernoulli model. Regardless of the differences between the two methods, using a carefully selected subset of the features results in better effectiveness than using all features."""
iir 13 5 2,chi2 Feature selection,Document classification," 2 Feature selection, INDEPENDENCE, STATISTICAL SIGNIFICANCE","Chi2 Feature selection Another popular feature selection method is ^2 . In statistics, the ^2 test is applied to test the independence of two events, where two events A and B are defined to be independent if P(AB) = P(A)P(B) or, equivalently, P(A B)=P(A) and P(B A)=P(B). In feature selection, the two events are occurrence of the term and occurrence of the class. We then rank terms with respect to the following quantity: X^2(,,c) = {e } ... ...} {( {e e c}-E {e e c})^2}{E {e e c}} (133) where e and e c are defined as in Equation 130. is the observed frequency in and E the expected frequency. For example, E {11} is the expected frequency of and c occurring together in a document assuming that term and class are independent. Worked example. We first compute E {11} for the data in Example 13.5.1: E {11} = N P() P(c) = N { {11}+ {10}}{N} { {11}+ {01}}{N} (134) = N {49+141}{N} {49+27652}{N} 6.6 (135) where N is the total number of documents as before. We compute the other E {e e c} in the same way: e {{poultry}}=1 e {{poultry}}=0 e {{export}} = 1 {11}=49 E {11} 6.6 {10} = 27{,}652 E {10} 27{,}694.4 e {{export}} = 0 {01} = 141 E {01} 183.4 {00}=774{,}106 E {00} 774{,}063.6 Plugging these values into Equation 133, we get a X^2 value of 284: X^2(,,c) = {e } ... ...servationo {e e c}-E {e e c})^2}{E {e e c}} 284 (136) End worked example. X^2 is a measure of how much expected counts E and observed counts deviate from each other. A high value of X^2 indicates that the hypothesis of independence, which implies that expected and observed counts are similar, is incorrect. In our example, X^2 284 > 10.83. Based on Table 13.6 , we can reject the hypothesis that poultry and export are independent with only a 0.001 chance of being wrong.[*]Equivalently, we say that the outcome X^{.5pt2} 284 > 10.83 is statistically significant at the 0.001 level. If the two events are dependent, then the occurrence of the term makes the occurrence of the class more likely (or less likely), so it should be helpful as a feature. This is the rationale of ^2 feature selection. An arithmetically simpler way of computing X^{.5pt2} is the following: (137) This is equivalent to Equation 133 (Exercise 13.6 )."""
iir 13 5 3,Frequency-based feature selection,Document classification,,"A third feature selection method is frequency-based feature selection , that is, selecting the terms that are most common in the class. Frequency can be either defined as document frequency (the number of documents in the class c that contain the term ) or as collection frequency (the number of tokens of that occur in documents in c). Document frequency is more appropriate for the Bernoulli model, collection frequency for the multinomial model. Frequency-based feature selection selects some frequent terms that have no specific information about the class, for example, the days of the week (Monday, Tuesday, ...), which are frequent across classes in newswire text. When many thousands of features are selected, then frequency-based feature selection often does well. Thus, if somewhat suboptimal accuracy is acceptable, then frequency-based feature selection can be a good alternative to more complex methods. However, Figure 13.8 is a case where frequency-based feature selection performs a lot worse than MI and ^2 and should not be used. """
iir 13 5 4,Feature selection for multiple classifiers,Document classification,," In an operational system with a large number of classifiers, it is desirable to select a single set of features instead of a different one for each classifier. One way of doing this is to compute the X^2 statistic for an n 2 table where the columns are occurrence and nonoccurrence of the term and each row corresponds to one of the classes. We can then select the terms with the highest X^2 statistic as before. More commonly, feature selection statistics are first computed separately for each class on the two-class classification task c versus {c} and then combined. One combination method computes a single figure of merit for each feature, for example, by averaging the values A(,c) for feature , and then selects the features with highest figures of merit. Another frequently used combination method selects the top / n features for each of n classifiers and then combines these n sets into one global feature set. Classification accuracy often decreases when selecting common features for a system with n classifiers as opposed to n different sets of size . But even if it does, the gain in efficiency owing to a common document representation may be worth the loss in accuracy . """
iir 13 5 5,Comparison of feature selection methods,"Document classification,Evaluation of binary classifiers",GREEDY FEATURE selection," Mutual information and ^2 represent rather different feature selection methods. The independence of term and class c can sometimes be rejected with high confidence even if carries little information about membership of a document in c. This is particularly true for rare terms. If a term occurs once in a large collection and that one occurrence is in the poultry class, then this is statistically significant. But a single occurrence is not very informative according to the information-theoretic definition of information. Because its criterion is significance, ^2 selects more rare terms (which are often less reliable indicators) than mutual information. But the selection criterion of mutual information also does not necessarily select the terms that maximize classification accuracy. Despite the differences between the two methods, the classification accuracy of feature sets selected with ^2 and MI does not seem to differ systematically. In most text classification problems, there are a few strong indicators and many weak indicators. As long as all strong indicators and a large number of weak indicators are selected, accuracy is expected to be good. Both methods do this. Figure 13.8 compares MI and ^2 feature selection for the multinomial model. Peak effectiveness is virtually the same for both methods. ^2 reaches this peak later, at 300 features, probably because the rare, but highly significant features it selects initially do not cover all documents in the class. However, features selected later (in the range of 100-300) are of better quality than those selected by MI. All three methods - MI, ^2 and frequency based - are greedy methods. They may select features that contribute no incremental information over previously selected features. In Figure 13.7 , kong is selected as the seventh term even though it is highly correlated with previously selected hong and therefore redundant. Although such redundancy can negatively impact accuracy, non-greedy methods (see Section 13.7 for references) are rarely used in text classification due to their computational cost. """
iir 13 6,Evaluation of text classification,Document classification,"TWO-CLASS CLASSIFIER, MODAPTE SPLIT, EFFECTIVENESS, PERFORMANCE , EFFICIENCY, MACROAVERAGING, MICROAVERAGING, DECISION trees, DEVELOPMENT SET, HELD-OUT DATA, INFORMATION GAIN"," Historically, the classic Reuters-21578 collection was the main benchmark for text classification evaluation. This is a collection of 21,578 newswire articles, originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text classification system. It is much smaller than and predates the Reuters-RCV1 collection discussed in Chapter 4 (page 4.2 ). The articles are assigned classes from a set of 118 topic categories. A document may be assigned several classes or none, but the commonest case is single assignment (documents with at least one class received an average of 1.24 classes). The standard approach to this any-of problem (Chapter 14 , page 14.5 ) is to learn 118 two-class classifiers, one for each class, where the two-class classifier for class is the classifier for the two classes and its complement {}. For each of these classifiers, we can measure recall, precision, and accuracy. In recent work, people almost invariably use the ModApte split , which includes only documents that were viewed and assessed by a human indexer, and comprises 9,603 training documents and 3,299 test documents. The distribution of documents in classes is very uneven, and some work evaluates systems on only documents in the ten largest classes. They are listed in Table 13.7 . A typical document with topics is shown in Figure 13.9 . In Section 13.1 , we stated as our goal in text classification the minimization of classification error on test data. Classification error is 1.0 minus classification accuracy, the proportion of correct decisions, a measure we introduced in Section 8.3 (page 8.3 ). This measure is appropriate if the percentage of documents in the class is high, perhaps 10% to 20% and higher. But as we discussed in Section 8.3 , accuracy is not a good measure for ``small'' classes because always saying no, a strategy that defeats the purpose of building a classifier, will achieve high accuracy. The always-no classifier is 99% accurate for a class with relative frequency 1%. For small classes, precision, recall and F 1 are better measures. We will use effectiveness as a generic term for measures that evaluate the quality of classification decisions, including precision, recall, F 1, and accuracy. Performance refers to the computational efficiency of classification and IR systems in this book. However, many researchers mean effectiveness, not efficiency of text classification when they use the term performance. Figure 13.9: A sample document from the Reuters-21578 collection. When we process a collection with several two-class classifiers (such as Reuters-21578 with its 118 classes), we often want to compute a single aggregate measure that combines the measures for individual classifiers. There are two methods for doing this. Macroaveraging computes a simple average over classes. Microaveraging pools per-document decisions across classes, and then computes an effectiveness measure on the pooled contingency table. Table 13.8 gives an example. The differences between the two methods can be large. Macroaveraging gives equal weight to each class, whereas microaveraging gives equal weight to each per-document classification decision. Because the F 1 measure ignores true negatives and its magnitude is mostly determined by the number of true positives, large classes dominate small classes in microaveraging. In the example, microaveraged precision (0.83) is much closer to the precision of c 2 (0.9) than to the precision of c 1 (0.5) because c 2 is five times larger than c 1. Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection. To get a sense of effectiveness on small classes, you should compute macroaveraged results. In one-of classification (more-than-two-classes), microaveraged F 1 is the same as accuracy (Exercise 13.6 ). Table 13.9 gives microaveraged and macroaveraged effectiveness of Naive Bayes for the ModApte split of Reuters-21578. To give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15 ), one of the most effective classifiers, but also one that is more expensive to train than NB. NB has a microaveraged F 1 of 80%, which is 9% less than the SVM (89%), a 10% relative decrease (row ``micro-avg-L (90 classes)''). So there is a surprisingly small effectiveness penalty for its simplicity and efficiency. However, on small classes, some of which only have on the order of ten positive examples in the training set, NB does much worse. Its macroaveraged F 1 is 13% below the SVM, a 22% relative decrease (row ``macro-avg (90 classes)'' ). The table also compares NB with the other classifiers we cover in this book: Rocchio and kNN. In addition, we give numbers for decision trees , an important classification method we do not cover. The bottom part of the table shows that there is considerable variation from class to class. For instance, NB beats kNN on ship, but is much worse on money-fx. Comparing parts (a) and (b) of the table, one is struck by the degree to which the cited papers' results differ. This is partly due to the fact that the numbers in (b) are break-even scores (cf. page 8.4 ) averaged over 118 classes, whereas the numbers in (a) are true F 1 scores (computed without any knowledge of the test set) averaged over ninety classes. This is unfortunately typical of what happens when comparing different results in text classification: There are often differences in the experimental setup or the evaluation that complicate the interpretation of the results. These and other results have shown that the average effectiveness of NB is uncompetitive with classifiers like SVMs when trained and tested on independent and identically distributed ( i.i.d. ) data, that is, uniform data with all the good properties of statistical sampling. However, these differences may often be invisible or even reverse themselves when working in the real world where, usually, the training sample is drawn from a subset of the data to which the classifier will be applied, the nature of the data drifts over time rather than being stationary (the problem of concept drift we mentioned on page 13.4 ), and there may well be errors in the data (among other problems). Many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than NB. Our conclusion from the results in Table 13.9 is that, although most researchers believe that an SVM is better than kNN and kNN better than NB, the ranking of classifiers ultimately depends on the class, the document collection, and the experimental setup. In text classification, there is always more to know than simply which machine learning algorithm was used, as we further discuss in Section 15.3 (page [*]). When performing evaluations like the one in Table 13.9 , it is important to maintain a strict separation between the training set and the test set . We can easily make correct classification decisions on the test set by using information we have gleaned from the test set, such as the fact that a particular term is a good predictor in the test set (even though this is not the case in the training set). A more subtle example of using knowledge about the test set is to try a large number of values of a parameter (e.g., the number of selected features) and select the value that is best for the test set. As a rule, accuracy on new data - the type of data we will encounter when we use the classifier in an application - will be much lower than accuracy on a test set that the classifier has been tuned for. We discussed the same problem in ad hoc retrieval in Section 8.1 (page 8.1 ). In a clean statistical text classification experiment, you should never run any program on or even look at the test set while developing a text classification system. Instead, set aside a development set for testing while you develop your method. When such a set serves the primary purpose of finding a good value for a parameter, for example, the number of selected features, then it is also called held-out data . Train the classifier on the rest of the training set with different parameter values, and then select the value that gives best results on the held-out part of the training set. Ideally, at the very end, when all parameters have been set and the method is fully specified, you run one final experiment on the test set and publish the results. Because no information about the test set was used in developing the classifier, the results of this experiment should be indicative of actual performance in practice. This ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a period of several years. But it is nevertheless highly important to not look at the test data and to run systems on it as sparingly as possible. Beginners often violate this rule, and their results lose validity because they have implicitly tuned their system to the test data simply by running many variant systems and keeping the tweaks to the system that worked best on the test set. Exercises. Assume a situation where every document in the test collection has been assigned exactly one class, and that a classifier also assigns exactly one class to each document. This setup is called one-of classification more-than-two-classes. Show that in one-of classification (i) the total number of false positive decisions equals the total number of false negative decisions and (ii) microaveraged F 1 and accuracy are identical. The class priors in Figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class. Why? The function APPLYMULTINOMIALNB in Figure 13.2 has time complexity Theta( L {a}+{C} L {a}). How would you modify the function so that its time complexity is Theta( L {a}+{C} M {a})? Based on the data in Table 13.10 , (i) estimate a multinomial Naive Bayes classifier, (ii) apply the classifier to the test document, (iii) estimate a Bernoulli NB classifier, (iv) apply the classifier to the test document. You need not estimate parameters that you don't need for classifying the test document. Your task is to classify words as English or not English. Words are generated by a source with the following distribution: event word English? probability 1 ozb no 4/9 2 uzu no 4/9 3 zoo yes 1/18 4 bun yes 1/18 (i) Compute the parameters (priors and conditionals) of a multinomial NB classifier that uses the letters b, n, o, u, and z as features. Assume a training set that reflects the probability distribution of the source perfectly. Make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text classification. Compute parameters using smoothing, in which computed-zero probabilities are smoothed into probability 0.01, and computed-nonzero probabilities are untouched. (This simplistic smoothing may cause P(A) +P({A}) > 1. Solutions are not required to correct this.) (ii) How does the classifier classify the word zoo? (iii) Classify the word zoo using a multinomial classifier as in part (i), but do not make the assumption of positional independence. That is, estimate separate parameters for each position in a word. You only need to compute the parameters you need for classifying zoo. What are the values of I( ;C c) and X^2(,,c) if term and class are completely independent? What are the values if they are completely dependent? The feature selection method in Equation 130 is most appropriate for the Bernoulli model. Why? How could one modify it for the multinomial model? Features can also be selected according to information gain (IG), which is defined as: (138) where H is entropy, is the training set, and {t^{+}}, and {t^{-}} are the subset of with term t, and the subset of without term t, respectively. p A is the class distribution in (sub)collection A, e.g., p A(c)=0.25, p A({c} )=0.75 if a quarter of the documents in A are in class c. Show that mutual information and information gain are equivalent. Show that the two X^2 formulas ( and 137 ) are equivalent. In the ^2 example on page 13.5.2 we have {11}-E {11}= {10}-E {10}= {01}-E {01}= {00}-E {00}. Show that this holds in general. ^2 and mutual information do not distinguish between positively and negatively correlated features. Because most good text classification features are positively correlated (i.e., they occur more often in c than in {c}), one may want to explicitly rule out the selection of negative indicators. How would you do this?"""
iir 14,Vector space classification,"Document classification,Vector space model","CONTIGUITY HYPOTHESIS, PROTOTYPE","The document representation in Naive Bayes is a sequence of terms or a binary vector e 1,,e { V} ^ V. In this chapter we adopt a different representation for text classification, the vector space model, developed in Chapter 6 . It represents each document as a vector with one real-valued component, usually a tf-idf weight, for each term. Thus, the document space , the domain of the classification function , is ^{ V}. This chapter introduces a number of classification methods that operate on real-valued vectors. The basic hypothesis in using the vector space model for classification is the contiguity hypothesis . Contiguity hypothesis. Documents in the same class form a contiguous region and regions of different classes do not overlap. There are many classification tasks, in particular the type of text classification that we encountered in Chapter 13 , where classes can be distinguished by word patterns. For example, documents in the class China tend to have high values on dimensions like Chinese, Beijing, and Mao whereas documents in the class UK tend to have high values for London, British and Queen. Documents of the two classes therefore form distinct contiguous regions as shown in Figure 14.1 and we can draw boundaries that separate them and classify new documents. How exactly this is done is the topic of this chapter. Whether or not a set of documents is mapped into a contiguous region depends on the particular choices we make for the document representation: type of weighting, stop list etc. To see that the document representation is crucial, consider the two classes written by a group vs. written by a single person. Frequent occurrence of the first person pronoun I is evidence for the single-person class. But that information is likely deleted from the document representation if we use a stop list. If the document representation chosen is unfavorable, the contiguity hypothesis will not hold and successful vector space classification is not possible. The same considerations that led us to prefer weighted representations, in particular length-normalized tf-idf representations, in Chapters 6 7 also apply here. For example, a term with 5 occurrences in a document should get a higher weight than a term with one occurrence, but a weight 5 times larger would give too much emphasis to the term. Unweighted and unnormalized counts should not be used in vector space classification. We introduce two vector space classification methods in this chapter, Rocchio and kNN. Rocchio classification (Section 14.2 ) divides the vector space into regions centered on centroids or prototypes , one for each class, computed as the center of mass of all documents in the class. Rocchio classification is simple and efficient, but inaccurate if classes are not approximately spheres with similar radii. kNN or k nearest neighbor classification (Section 14.3 ) assigns the majority class of the k nearest neighbors to a test document. kNN requires no explicit training and can use the unprocessed training set directly in classification. It is less efficient than other classification methods in classifying documents. If the training set is large, then kNN can handle non-spherical and other complex classes better than Rocchio. A large number of text classifiers can be viewed as linear classifiers - classifiers that classify based on a simple linear combination of the features (Section 14.4 ). Such classifiers partition the space of features into regions separated by linear decision hyperplanes , in a manner to be detailed below. Because of the bias-variance tradeoff (Section 14.6 ) more complex nonlinear models are not systematically better than linear models. Nonlinear models have more parameters to fit on a limited amount of training data and are more likely to make mistakes for small and noisy data sets. When applying two-class classifiers to problems with more than two classes, there are one-of tasks - a document must be assigned to exactly one of several mutually exclusive classes - and any-of tasks - a document can be assigned to any number of classes as we will explain in Section 14.5 . Two-class classifiers solve any-of problems and can be combined to solve one-of problems."
iir 14 1,Document representations and measures of relatedness in vector spaces,"Document classification,Nearest centroid classifier",,"As in Chapter 6 , we represent documents as vectors in ^{ V} in this chapter. To illustrate properties of document vectors in vector classification, we will render these vectors as points in a plane as in the example in Figure 14.1 . In reality, document vectors are length-normalized unit vectors that point to the surface of a hypersphere. We can view the 2D planes in our figures as projections onto a plane of the surface of a (hyper-)sphere as shown in Figure 14.2 . Distances on the surface of the sphere and on the projection plane are approximately the same as long as we restrict ourselves to small areas of the surface and choose an appropriate projection (Exercise 14.1 ). Decisions of many vector space classifiers are based on a notion of distance, e.g., when computing the nearest neighbors in kNN classification. We will use Euclidean distance in this chapter as the underlying distance measure. We observed earlier (Exercise 6.4.4 , page [*] ) that there is a direct correspondence between cosine similarity and Euclidean distance for length-normalized vectors. In vector space classification, it rarely matters whether the relatedness of two documents is expressed in terms of similarity or distance. However, in addition to documents, centroids or averages of vectors also play an important role in vector space classification. Centroids are not length-normalized. For unnormalized vectors, dot product, cosine similarity and Euclidean distance all have different behavior in general (Exercise 14.8 ). We will be mostly concerned with small local regions when computing the similarity between a document and a centroid, and the smaller the region the more similar the behavior of the three measures is."
iir 14 2,Rocchio classification,"Document classification,K-nearest neighbors algorithm","DECISION BOUNDARY, ROCCHIO CLASSIFICATION, CENTROID, NORMAL VECTOR, MULTIMODAL CLASS","Figure 14.1 shows three classes, China, UK and Kenya, in a two-dimensional (2D) space. Documents are shown as circles, diamonds and X's. The boundaries in the figure, which we call decision boundaries , are chosen to separate the three classes, but are otherwise arbitrary. To classify a new document, depicted as a star in the figure, we determine the region it occurs in and assign it the class of that region - China in this case. Our task in vector space classification is to devise algorithms that compute good boundaries where ``good'' means high classification accuracy on data unseen during training. Perhaps the best-known way of computing good class boundaries is Rocchio classification , which uses centroids to define the boundaries. The centroid of a class c is computed as the vector average or center of mass of its members, where c is the set of documents in whose class is c: c = . We denote the normalized vector of d by (d) (Equation 25 , page 6.3.1 ). Three example centroids are shown as solid circles in Figure 14.3 . The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids. This set of points is always a line. The generalization of a line in M-dimensional space is a hyperplane, which we define as the set of points that satisfy. where is the M-dimensional normal vector [*] of the hyperplane and b is a constant. This definition of hyperplanes includes lines (any line in 2D can be defined by w 1x 1+w 2x 2=b) and 2-dimensional planes (any plane in 3D can be defined by w 1x 1+w 2x 2+w 3x 3=b). A line divides a plane in two, a plane divides 3-dimensional space in two, and hyperplanes divide higher-dimensional spaces in two. Thus, the boundaries of class regions in Rocchio classification are hyperplanes. The classification rule in Rocchio is to classify a point in accordance with the region it falls into. Equivalently, we determine the centroid () that the point is closest to and then assign it to . As an example, consider the star in Figure 14.3 . It is located in the China region of the space and Rocchio therefore assigns it to China. We show the Rocchio algorithm in pseudocode in Figure 14.4 . Worked example. Table 14.1 shows the tf-idf vector representations of the five documents in Table 13.1 (page 13.1 ), using the formula (1+ {10} {tf} {t,d}) {10} (4/{df} t) if {tf} {t,d} >0 (Equation 29, page 6.4.1 ). The two class centroids are c = 1/3 ( 1+ 2+ 3) and {} = 1/1 ( 4). The distances of the test document from the centroids are c - 5 1.15 and {} - 5 = 0.0. Thus, Rocchio assigns d 5 to . See Exercise 14.8 for how to compute and b. We can easily verify that this hyperplane separates the documents as desired. Thus, documents in c are above the hyperplane ( ^{T} > b) and documents in are below the hyperplane ( ^{T} < b). The assignment criterion in Figure 14.4 is Euclidean distance (APPLYROCCHIO, line 1). An alternative is cosine similarity. As discussed in Section 14.1 , the two assignment criteria will sometimes make different classification decisions. We present the Euclidean distance variant of Rocchio classification here because it emphasizes Rocchio's close correspondence to K-means clustering (Section 16.4 , page 16.4 ). Rocchio classification is a form of Rocchio relevance feedback (Section 9.1.1 , page 9.1.1 ). The average of the relevant documents, corresponding to the most important component of the Rocchio vector in relevance feedback (Equation 49, page 49 ), is the centroid of the ``class'' of relevant documents. We omit the query component of the Rocchio formula in Rocchio classification since there is no query in text classification. Rocchio classification can be applied to J>2 classes whereas Rocchio relevance feedback is designed to distinguish only two classes, relevant and nonrelevant. In addition to respecting contiguity, the classes in Rocchio classification must be approximate spheres with similar radii. In Figure 14.3 , the solid square just below the boundary between UK and Kenya is a better fit for the class UK since UK is more scattered than Kenya. But Rocchio assigns it to Kenya because it ignores details of the distribution of points in a class and only uses distance from the centroid for classification. The assumption of sphericity also does not hold in Figure 14.5 . We cannot represent the ``a'' class well with a single prototype because it has two clusters. Rocchio often misclassifies this type of multimodal class . A text classification example for multimodality is a country like Burma, which changed its name to Myanmar in 1989. The two clusters before and after the name change need not be close to each other in space. We also encountered the problem of multimodality in relevance feedback (Section 9.1.2 , page 9.1.3 ). Two-class classification is another case where classes are rarely distributed like spheres with similar radii. Most two-class classifiers distinguish between a class like China that occupies a small region of the space and its widely scattered complement. Assuming equal radii will result in a large number of false positives. Most two-class classification problems therefore require a modified decision rule of the form. for a positive constant b. As in Rocchio relevance feedback, the centroid of the negative documents is often not used at all, so that the decision criterion simplifies to (c) - () < b' for a positive constant b'. Table 14.2 gives the time complexity of Rocchio classification.[*] Adding all documents to their respective (unnormalized) centroid is ( L {ave}) (as opposed to ( V)) since we need only consider non-zero entries. Dividing each vector sum by the size of its class to compute the centroid is ( V). Overall, training time is linear in the size of the collection (cf. Exercise 13.2.1 ). Thus, Rocchio classification and Naive Bayes have the same linear training time complexity. In the next section, we will introduce another vector space classification method, kNN, that deals better with classes that have non-spherical, disconnected or other irregular shapes."
iir 14 3,k nearest neighbor,"Document classification,Multiclass classification","k NEAREST NEIGHBOR, CLASSIFICATION, VORONOI TESSELLATION, MEMORY-BASED LEARNING, BAYES ERROR RATE","Unlike Rocchio, k nearest neighbor or kNN classification determines the decision boundary locally. For 1NN we assign each document to the class of its closest neighbor. For kNN we assign each document to the majority class of its k closest neighbors where k is a parameter. The rationale of kNN classification is that, based on the contiguity hypothesis, we expect a test document d to have the same label as the training documents located in the local region surrounding d. Decision boundaries in 1NN are concatenated segments of the Voronoi tessellation as shown in Figure 14.6 . The Voronoi tessellation of a set of objects decomposes space into Voronoi cells, where each object's cell consists of all points that are closer to the object than to other objects. In our case, the objects are documents. The Voronoi tessellation then partitions the plane into convex polygons, each containing its corresponding document (and no other) as shown in Figure 14.6 , where a convex polygon is a convex region in 2-dimensional space bounded by lines. For general k in kNN, consider the region in the space for which the set of k nearest neighbors is the same. This again is a convex polygon and the space is partitioned into convex polygons , within each of which the set of k nearest neighbors is invariant (Exercise 14.8 ). 1NN is not very robust. The classification decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical. kNN for k>1 is more robust. It assigns documents to the majority class of their k closest neighbors, with ties broken randomly. There is a probabilistic version of this kNN classification algorithm. We can estimate the probability of membership in class c as the proportion of the k nearest neighbors in c. Figure 14.6 gives an example for k=3. Probability estimates for class membership of the star are ({circle class})=1/3, ({X class})=2/3, and ({diamond class})=0. The 3nn estimate ( 1({circle class})=1/3) and the 1nn estimate ( 1({circle class})=1) differ with 3nn preferring the X class and 1nn preferring the circle class . The parameter k in kNN is often chosen based on experience or knowledge about the classification problem at hand. It is desirable for k to be odd to make ties less likely. k=3 and k=5 are common choices, but much larger values between 50 and 100 are also used. An alternative way of setting the parameter is to select the k that gives best results on a held-out portion of the training set. We can also weight the ``votes'' of the k nearest neighbors by their cosine similarity. In this scheme, a class's score is computed. where S k(d) is the set of d's k nearest neighbors and I c(')=1 iff ' is in class c and 0 otherwise. We then assign the document to the class with the highest score. Weighting by similarities is often more accurate than simple voting. For example, if two classes have the same number of neighbors in the top k, the class with the more similar neighbors wins. Time complexity and optimality of kNN. Table 14.3 gives the time complexity of kNN. kNN has properties that are quite different from most other classification algorithms. Training a kNN classifier simply consists of determining k and preprocessing documents. In fact, if we preselect a value for k and do not preprocess, then kNN requires no training at all. In practice, we have to perform preprocessing steps like tokenization. It makes more sense to preprocess training documents once as part of the training phase rather than repeatedly every time we classify a new test document. Test time is ( M {ave} M {a}) for kNN. It is linear in the size of the training set as we need to compute the distance of each training document from the test document. Test time is independent of the number of classes J. kNN therefore has a potential advantage for problems with large J. In kNN classification, we do not perform any estimation of parameters as we do in Rocchio classification (centroids) or in Naive Bayes (priors and conditional probabilities). kNN simply memorizes all examples in the training set and then compares the test document to them. For this reason, kNN is also called memory-based learning or instance-based learning . It is usually desirable to have as much training data as possible in machine learning. But in kNN large training sets come with a severe efficiency penalty in classification. Can kNN testing be made more efficient than ( M {ave} M {a}) or, ignoring the length of documents, more efficient than ()? There are fast kNN algorithms for small dimensionality M (Exercise 14.8 ). There are also approximations for large M that give error bounds for specific efficiency gains (see Section 14.7 ). These approximations have not been extensively tested for text classification applications, so it is not clear whether they can achieve much better efficiency than () without a significant loss of accuracy. The reader may have noticed the similarity between the problem of finding nearest neighbors of a test document and ad hoc retrieval, where we search for the documents with the highest similarity to the query (Section 6.3.2 , page 6.3.2 ). In fact, the two problems are both k nearest neighbor problems and only differ in the relative density of (the vector of) the test document in kNN (10s or 100s of non-zero entries) versus the sparseness of (the vector of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries). We introduced the inverted index for efficient ad hoc retrieval in Section 1.1 (page 1.1 ). Is the inverted index also the solution for efficient kNN? An inverted index restricts a search to those documents that have at least one term in common with the query. Thus in the context of kNN, the inverted index will be efficient if the test document has no term overlap with a large number of training documents. Whether this is the case depends on the classification problem. If documents are long and no stop list is used, then less time will be saved. But with short documents and a large stop list, an inverted index may well cut the average test time by a factor of 10 or more. The search time in an inverted index is a function of the length of the postings lists of the terms in the query. Postings lists grow sublinearly with the length of the collection since the vocabulary increases according to Heaps' law - if the probability of occurrence of some terms increases, then the probability of occurrence of others must decrease. However, most new terms are infrequent. We therefore take the complexity of inverted index search to be (T) (as discussed in Section 2.4.2 , page 2.4.2 ) and, assuming average document length does not change over time, (T)=(). As we will see in the next chapter, kNN's effectiveness is close to that of the most accurate learning methods in text classification (Table 15.2 , page 15.2 ). A measure of the quality of a learning method is its Bayes error rate , the average error rate of classifiers learned by it for a particular problem. kNN is not optimal for problems with a non-zero Bayes error rate - that is, for problems where even the best possible classifier has a non-zero classification error. The error of 1NN is asymptotically (as the training set increases) bounded by twice the Bayes error rate. That is, if the optimal classifier has an error rate of x, then 1NN has an asymptotic error rate of less than 2x. This is due to the effect of noise - we already saw one example of noise in the form of noisy features in Section 13.5 (page 13.5 ), but noise can also take other forms as we will discuss in the next section. Noise affects two components of kNN: the test document and the closest training document. The two sources of noise are additive, so the overall error of 1NN is twice the optimal error rate. For problems with Bayes error rate 0, the error rate of 1NN will approach 0 as the size of the training set increases."
iir 14 4,Linear versus nonlinear classifiers,"Document classification,Multiclass classification","LINEAR CLASSIFIER, DECISION HYPERPLANE, CLASS BOUNDARY, NOISE DOCUMENT, LINEAR SEPARABILITY, NONLINEAR CLASSIFIER","In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers. To simplify the discussion, we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class membership by comparing a linear combination of the features to a threshold. In two dimensions, a linear classifier is a line. Five examples are shown in Figure 14.8 . These lines have the functional form w 1x 1+w 2x 2=b. The classification rule of a linear classifier is to assign a document to c if w 1x 1+w 2x 2>b and to if w 1x 1+w 2x 2 b. Here, (x 1, x 2)^{T} is the two-dimensional vector representation of the document and (w 1, w 2)^{T} is the parameter vector that defines (together with b) the decision boundary. An alternative geometric interpretation of a linear classifier is provided in Figure 15.7 .We can generalize this 2D linear classifier to higher dimensions by defining a hyperplane as we did in Equation 140. The assignment criterion then is: assign to c if ^{T} > b and to if ^{T} b. We call a hyperplane that we use as a linear classifier a decision hyperplane . The corresponding algorithm for linear classification in M dimensions is shown in Figure 14.9 . Linear classification at first seems trivial given the simplicity of this algorithm. However, the difficulty is in training the linear classifier, that is, in determining the parameters and b based on the training set. In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data. We now show that Rocchio and Naive Bayes are linear classifiers. To see this for Rocchio, observe that a vector is on the decision boundary if it has equal distance to the two class centroids. Some basic arithmetic shows that this corresponds to a linear classifier with normal vector. We can derive the linearity of Naive Bayes from its decision rule, which chooses the category c with the largest (c) (Figure 13.2 , page 13.2 ). n d is the number of tokens in the document that are part of the vocabulary. Denoting the complement category as , we obtain for the log odds. We choose class c if the odds are greater than 1 or, equivalently, if the log odds are greater than 0. It is easy to see that Equation 147 is an instance of Equation 144 for w i = [( i c)/( i)], x i = number of occurrences of t i in , and b = - [ (c) / ()] . Here, the index i, 1 i M, refers to terms of the vocabulary (not to positions in d as k does; cf. variantmultinomial) and and are M-dimensional vectors. So in log space, Naive Bayes is a linear classifier. Worked example. Table 14.4 defines a linear classifier for the category interest in Reuters-21578 (see Section 13.6 , page 13.6 ). We assign document 1 ``rate discount dlrs world'' to interest since ^{T} 1 = 0.67 1 + 0.46 1 + -0.71) 1 + (-0.35) 1 = 0.07 >0= b. We assign 2 ``prime dlrs'' to the complement class (not in interest) since ^{T} 2 = -0.01 b. For simplicity, we assume a simple binary vector representation in this example: 1 for occurring terms, 0 for non-occurring terms. End worked example. A linear problem with noise. In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese-English web pages are squares. The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows). Figure 14.10 is a graphical example of a linear problem, which we define to mean that the underlying distributions P(d c) and P(d) of the two classes are separated by a line. We call this separating line the class boundary . It is the ``true'' boundary of the two classes and we distinguish it from the decision boundary that the learning method computes to approximate the class boundary. As is typical in text classification, there are some noise documents in Figure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes. In Section 13.5 (page 13.5 ), we defined a noise feature as a misleading feature that, when included in the document representation, on average increases the classification error. Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error. Intuitively, the underlying distribution partitions the representation space into areas with mostly homogeneous class assignments. A document that does not conform with the dominant class in its area is a noise document. Noise documents are one reason why training a linear classifier is hard. If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data. More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading. If there exists a hyperplane that perfectly separates the two classes, then we call the two classes linearly separable . In fact, if linear separability holds, then there is an infinite number of linear separators (Exercise 14.4 ) as illustrated by Figure 14.8 , where the number of possible separating hyperplanes is infinite. Figure 14.8 illustrates another challenge in training a linear classifier. If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data. In general, some of these hyperplanes will do well on new data, some will not. An example of a nonlinear classifier is kNN. The nonlinearity of kNN is intuitively clear when looking at examples like Figure 14.6 . The decision boundaries of kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions. Figure 14.11 is another example of a nonlinear problem: there is no good linear separator between the distributions P(d c) and P(d) because of the circular ``enclave'' in the upper left part of the graph. Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough. If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."
iir 14 5,Classification with more than two classes,"Document classification,Bias „ variance tradeoff","ANY-OF CLASSIFICATION, ONE-OF CLASSIFICATION, CONFUSION MATRIX","We can extend two-class linear classifiers to J>2 classes. The method to use depends on whether the classes are mutually exclusive or not. Classification for classes that are not mutually exclusive is called any-of , multilabel , or multivalue classification . In this case, a document can belong to several classes simultaneously, or to a single class, or to none of the classes. A decision on one class leaves all options open for the others. It is sometimes said that the classes are independent of each other, but this is misleading since the classes are rarely statistically independent in the sense defined on page 13.5.2 . In terms of the formal definition of the classification problem in Equation 112 (page 112 ), we learn J different classifiers j in any-of classification, each returning either c j or j: j( ) . Solving an any-of classification task with linear classifiers is straightforward. Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. The decision of one classifier has no influence on the decisions of the other classifiers. The second type of classification with more than two classes is one-of classification . Here, the classes are mutually exclusive. Each document must belong to exactly one of the classes. One-of classification is also called multinomial , polytomous [*], multiclass , or single-label classification . Formally, there is a single classification function in one-of classification whose range is , i.e., ( ) . kNN is a (nonlinear) one-of classifier. True one-of problems are less common in text classification than any-of problems. With classes like UK, China, poultry, or coffee, a document can be relevant to many topics simultaneously - as when the prime minister of the UK visits China to talk about the coffee and poultry trade. Nevertheless, we will often make a one-of assumption, as we did in Figure 14.1 , even if classes are not really mutually exclusive. For the classification problem of identifying the language of a document, the one-of assumption is a good approximation as most text is written in only one language. In such cases, imposing a one-of constraint can increase the classifier's effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated. J hyperplanes do not divide ^{ V} into J distinct regions as illustrated in Figure 14.12 . Thus, we must use a combination method when using two-class linear classifiers for one-of classification. The simplest method is to rank classes and then select the top-ranked class. Geometrically, the ranking can be with respect to the distances from the J linear separators. Documents close to a class's separator are more likely to be misclassified, so the greater the distance from the separator, the more plausible it is that a positive classification decision is correct. Alternatively, we can use a direct measure of confidence to rank classes, e.g., probability of class membership. We can state this algorithm for one-of classification with linear classifiers as follows: Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. Assign the document to the class with the maximum score, the maximum confidence value, or the maximum probability. An important tool for analyzing the performance of a classifier for J>2 classes is the confusion matrix . The confusion matrix shows for each pair of classes 1,c 2, how many documents from c 1 were incorrectly assigned to c 2. In Table 14.5 , the classifier manages to distinguish the three financial classes money-fx, trade, and interest from the three agricultural classes wheat, corn, and grain, but makes many errors within these two groups. The confusion matrix can help pinpoint opportunities for improving the accuracy of the system. For example, to address the second largest error in Table 14.5 (14 in the row grain), one could attempt to introduce features that distinguish wheat documents from grain documents."
iir 14 6,The bias-variance tradeoff,Document clustering,"OPTIMAL CLASSIFIER, LEARNING ERROR, OPTIMAL LEARNING METHOD, VARIANCE, OVERFITTING, MEMORY CAPACITY, BIAS-VARIANCE TRADEOFF","Nonlinear classifiers are more powerful than linear classifiers. For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier. Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification? To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning. The tradeoff helps explain why there is no universally optimal learning method. Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem. Throughout this section, we use linear and nonlinear classifiers as prototypical examples of ``less powerful'' and ``more powerful'' learning, respectively. This is a simplification for a number of reasons. First, many nonlinear models subsume linear models as a special case. For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier. Second, there are nonlinear models that are less complex than linear models. For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier. Third, the complexity of learning is not really a property of the classifier because there are many aspects of learning (such as feature selection, cf. feature, regularization, and constraints such as margin maximization in Chapter 15 ) that make a learning method either more powerful or less powerful without affecting the type of classifier that is the final result of learning - regardless of whether that classifier is linear or nonlinear. We refer the reader to the publications listed in Section 14.7 for a treatment of the bias-variance tradeoff that takes into account these complexities. In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification. We first need to state our objective in text classification more precisely. In Section 13.1 (page [*]), we said that we want to minimize classification error on the test set. The implicit assumption was that training documents and test documents are generated according to the same underlying distribution. We will denote this distribution P( d,c) where d is the document and c its label or class. graphclassmodelbernoulligraph were examples of generative models that decompose P( d,c) into the product of P(c) and P(d c). typicallineartypicalnonlinear depict generative models for d,c with d ^2 and c ,{solid circle} }. In this section, instead of using the number of correctly classified test documents (or, equivalently, the error rate on test documents) as evaluation measure, we adopt an evaluation measure that addresses the inherent uncertainty of labeling. In many text classification problems, a given document representation can arise from documents belonging to different classes. This is because documents from different classes can be mapped to the same document representation. For example, the one-sentence documents China sues France and France sues China are mapped to the same document representation d' = } ,{} , {} } in a bag of words model. But only the latter document is relevant to the class c'= legal actions brought by France (which might be defined, for example, as a standing query by an international trade lawyer). To simplify the calculations in this section, we do not count the number of errors on the test set when evaluating a classifier, but instead look at how well the classifier estimates the conditional probability P(c d) of a document being in a class. In the above example, we might have P(c' d') = 0.5. Our goal in text classification then is to find a classifier such that, averaged over documents d, (d) is as close as possible to the true probability P(c d). We measure this using mean squared error. where A is the expectation with respect to P(d). The mean squared error term gives partial credit for decisions by that are close if not completely right. We define a classifier to be optimal for a distribution P() if it minimizes {MSE}(). Minimizing MSE is a desideratum for classifiers. We also need a criterion for learning methods. Recall that we defined a learning method as a function that takes a labeled training set as input and returns a classifier . For learning methods, we adopt as our goal to find a that, averaged over training sets, learns classifiers with minimal MSE. We can formalize this as minimizing learning error. where E {} is the expectation over labeled training sets. To keep things simple, we can assume that training sets have a fixed size - the distribution P() then defines a distribution P() over training sets. We can use learning error as a criterion for selecting a learning method in statistical text classification. A learning method is optimal for a distribution P() if it minimizes the learning error. Arithmetic transformations for the bias-variance decomposition. For the derivation of Equation 157. Writing for () for better readability, we can transform Equation 149. where the equivalence between and 162 is shown in Equation 157 in Figure 14.6 . Note that and are independent of each other. In general, for a random document and a random training set , does not contain a labeled instance of . Bias is the squared difference between P(c d), the true conditional probability of d being in c, and (), the prediction of the learned classifier, averaged over training sets. Bias is large if the learning method produces classifiers that are consistently wrong. Bias is small if (i) the classifiers are consistently right or (ii) different training sets cause errors on different documents or (iii) different training sets cause positive and negative errors on the same documents, but that average out to close to 0. If one of these three conditions holds, then E (), the expectation over all training sets, is close to P(c d). Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary, a linear hyperplane. If the generative model P() has a complex nonlinear class boundary, the bias term in Equation 162 will be high because a large number of points will be consistently misclassified. For example, the circular enclave in Figure 14.11 does not fit a linear model and will be misclassified consistently by linear classifiers. We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier. If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method. But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average. Nonlinear methods like kNN have low bias. We can see in Figure 14.6 that the decision boundaries of kNN are variable - depending on the distribution of documents in the training set, learned decision boundaries can vary greatly. As a result, each document has a chance of being classified correctly for some training sets. The average prediction E () is therefore closer to P(c) and bias is smaller than for a linear learning method. Variance is the variation of the prediction of learned classifiers: the average squared difference between (d) and its average E (d). Variance is large if different training sets give rise to very different classifiers . It is small if the training set has a minor effect on the classification decisions makes, be they correct or incorrect. Variance measures how inconsistent the decisions are, not whether they are correct or incorrect. Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes. The decision lines produced by linear learning methods in and 14.11 will deviate slightly from the main class boundaries, depending on the training set, but the class assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected. The circular enclave in Figure 14.11 will be consistently misclassified. Nonlinear methods like kNN have high variance. It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes. It is therefore sensitive to noise documents of the sort depicted in Figure 14.10 . As a result the variance term in Equation 162 is large for kNN: Test documents are sometimes misclassified - if they happen to be close to a noise document in the training set - and sometimes correctly classified - if there are no noise documents in the training set near them. This results in high variation from training set to training set. High-variance learning methods are prone to overfitting the training data. The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution P(). In overfitting, the learning method also learns from noise. Overfitting increases MSE and frequently is a problem for high-variance learning methods. We can also think of variance as the model complexity or, equivalently, memory capacity of the learning method - how detailed a characterization of the training set it can remember and then apply to new data. This capacity corresponds to the number of independent parameters available to fit the training set. Each kNN neighborhood S k makes an independent classification decision. The parameter in this case is the estimate (c S k) from Figure 14.7 . Thus, kNN's capacity is only limited by the size of the training set. It can memorize arbitrarily large training sets. In contrast, the number of parameters of Rocchio is fixed - J parameters per dimension, one for each centroid - and independent of the size of the training set. The Rocchio classifier (in form of the centroids defining it) cannot ``remember'' fine-grained details of the distribution of the documents in the training set. According to Equation 149, our goal in selecting a learning method is to minimize learning error. The fundamental insight captured by Equation 162, which we can succinctly state as: learning-error = bias + variance, is that the learning error has two components, bias and variance, which in general cannot be minimized simultaneously. When comparing two learning methods 1 and 2, in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance. The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias). Instead, we have to weigh the respective merits of bias and variance in our application and choose accordingly. This tradeoff is called the bias-variance tradeoff . Figure 14.10 provides an illustration, which is somewhat contrived, but will be useful as an example for the tradeoff. Some Chinese text contains English words written in the Roman alphabet like CPU, ONLINE, and GPS. Consider the task of distinguishing Chinese-only web pages from mixed Chinese-English web pages. A search engine might offer Chinese users without knowledge of English (but who understand loanwords like CPU) the option of filtering out mixed pages. We use two features for this classification task: number of Roman alphabet characters and number of Chinese characters on the web page. As stated earlier, the distribution P() of the generative model generates most mixed (respectively, Chinese) documents above (respectively, below) the short-dashed line, but there are a few noise documents. In Figure 14.10 , we see three classifiers: One-feature classifier. Shown as a dotted horizontal line. This classifier uses only one feature, the number of Roman alphabet characters. Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents). So a learning method producing this type of classifier has low variance. But its bias is high since it will consistently misclassify squares in the lower left corner and ``solid circle'' documents with more than 50 Roman characters. Linear classifier. Shown as a dashed line with long dashes. Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified. The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets. Thus, very few documents (documents close to the class boundary) will be inconsistently classified. ``Fit-training-set-perfectly'' classifier. Shown as a solid line. Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set. This method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test set right. But the variance of this learning method is high. Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified - something that a linear learning method is unlikely to do. It is perhaps surprising that so many of the best-known text classification algorithms are linear. Some of these methods, in particular linear SVMs, regularized logistic regression and regularized linear regression, are among the most effective known methods. The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.8 ). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases."
iir 16,Flat clustering,Document clustering,"CLUSTER, UNSUPERVISED learning, FLAT CLUSTERING, HARD CLUSTERING, SOFT CLUSTERING","Clustering algorithms group a set of documents into subsets or clusters . The algorithms' goal is to create clusters that are coherent internally, but clearly different from each other. In other words, documents within a cluster should be as similar as possible; and documents in one cluster should be as dissimilar as possible from documents in other clusters. Figure 16.1: An example of a data set with a clear cluster structure. Clustering is the most common form of unsupervised learning . No supervision means that there is no human expert who has assigned documents to classes. In clustering, it is the distribution and makeup of the data that will determine cluster membership. A simple example is Figure 16.1 . It is visually clear that there are three distinct clusters of points. This chapter and Chapter 17 introduce algorithms that find such clusters in an unsupervised fashion. The difference between clustering and classification may not seem great at first. After all, in both cases we have a partition of a set of documents into groups. But as we will see the two problems are fundamentally different. Classification is a form of supervised learning (Chapter 13 , page 13.1 ): our goal is to replicate a categorical distinction that a human supervisor imposes on the data. In unsupervised learning, of which clustering is the most important example, we have no such teacher to guide us. The key input to a clustering algorithm is the distance measure. In Figure 16.1 , the distance measure is distance in the 2D plane. This measure suggests three different clusters in the figure. In document clustering, the distance measure is often also Euclidean distance. Different distance measures give rise to different clusterings. Thus, the distance measure is an important means by which we can influence the outcome of clustering. Flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters and will be covered in Chapter 17 . Chapter 17 also addresses the difficult problem of labeling clusters automatically. A second important distinction can be made between hard and soft clustering algorithms. Hard clustering computes a hard assignment - each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft - a document's assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Latent semantic indexing, a form of dimensionality reduction, is a soft clustering algorithm (Chapter 18 , page 18.4 ). This chapter motivates the use of clustering in information retrieval by introducing a number of applications (Section 16.1 ), defines the problem we are trying to solve in clustering (Section 16.2 ) and discusses measures for evaluating cluster quality (Section 16.3 ). It then describes two flat clustering algorithms, K-means (Section 16.4 ), a hard clustering algorithm, and the Expectation-Maximization (or EM) algorithm (Section 16.5 ), a soft clustering algorithm. K-means is perhaps the most widely used flat clustering algorithm due to its simplicity and efficiency. The EM algorithm is a generalization of K-means and can be applied to a large variety of document representations and distributions. """
iir 16 1,Clustering in information retrieval,Document clustering,"CLUSTER HYPOTHESIS, SEARCH RESULT CLUSTERING, SCATTER-GATHER","The cluster hypothesis states the fundamental assumption we make when using clustering in information retrieval. Cluster hypothesis. Documents in the same cluster behave similarly with respect to relevance to information needs. The hypothesis states that if there is a document from a cluster that is relevant to a search request, then it is likely that other documents from the same cluster are also relevant. This is because clustering puts together documents that share many terms. The cluster hypothesis essentially is the contiguity hypothesis in Chapter 14 (page 14 ). In both cases, we posit that similar documents behave similarly with respect to relevance. Clustering of search results to improve recall. None of the top hits cover the animal sense of jaguar, but users can easily access it by clicking on the cat cluster in the Clustered Results panel on the left (third arrow from the top). The first application mentioned in Table 16.1 is search result clustering where by search results we mean the documents that were returned in response to a query. The default presentation of search results in information retrieval is a simple list. Users scan the list from top to bottom until they have found the information they are looking for. Instead, search result clustering clusters the search results, so that similar documents appear together. It is often easier to scan a few coherent groups than many individual documents. This is particularly useful if a search term has different word senses. The example in Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the animal and an Apple operating system. The Clustered Results panel returned by the VivÔäêë simo search engine (http://vivisimo.com) can be a more effective user interface for understanding what is in the search results than a simple list of documents. An example of a user session in Scatter-Gather. A collection of New York Times news stories is clustered (``scattered'') into eight clusters (top row). The user manually gathers three of these into a smaller collection International Stories and performs another scattering operation. This process repeats until a small cluster with relevant documents is found (e.g., Trinidad). A better user interface is also the goal of Scatter-Gather , the second application in Table 16.1 . Scatter-Gather clusters the whole collection to get groups of documents that the user can select or gather. The selected groups are merged and the resulting set is again clustered. This process is repeated until a cluster of interest is found. An example is shown in Figure 16.3 . Automatically generated clusters like those in Figure 16.3 are not as neatly organized as a manually constructed hierarchical tree like the Open Directory at http://dmoz.org. Also, finding descriptive labels for clusters automatically is a difficult problem (Section 17.7 , page 17.7 ). But cluster-based navigation is an interesting alternative to keyword searching, the standard information retrieval paradigm. This is especially true in scenarios where users prefer browsing over searching because they are unsure about which search terms to use. As an alternative to the user-mediated iterative clustering in Scatter-Gather, we can also compute a static hierarchical clustering of a collection that is not influenced by user interactions (``Collection clustering'' in Table 16.1 ). Google News and its precursor, the Columbia NewsBlaster system, are examples of this approach. In the case of news, we need to frequently recompute the clustering to make sure that users can access the latest breaking stories. Clustering is well suited for access to a collection of news stories since news reading is not really search, but rather a process of selecting a subset of stories about recent events. The fourth application of clustering exploits the cluster hypothesis directly for improving search results, based on a clustering of the entire collection. We use a standard inverted index to identify an initial set of documents that match the query, but we then add other documents from the same clusters even if they have low similarity to the query. For example, if the query is car and several car documents are taken from a cluster of automobile documents, then we can add documents from this cluster that use terms other than car (automobile, vehicle etc). This can increase recall since a group of documents with high mutual similarity is often relevant as a whole. More recently this idea has been used for language modeling. Equation 102 , page 102 , showed that to avoid sparse data problems in the language modeling approach to IR, the model of document d can be interpolated with a collection model. But the collection contains many documents with terms untypical of d. By replacing the collection model with a model derived from d's cluster, we get more accurate estimates of the occurrence probabilities of terms in d. Clustering can also speed up search. As we saw in Section 6.3.2 ( page 6.3.2 ) search in the vector space model amounts to finding the nearest neighbors to the query. The inverted index supports fast nearest-neighbor search for the standard IR setting. However, sometimes we may not be able to use an inverted index efficiently, e.g., in latent semantic indexing (Chapter 18 ). In such cases, we could compute the similarity of the query to every document, but this is slow. The cluster hypothesis offers an alternative: Find the clusters that are closest to the query and only consider documents from these clusters. Within this much smaller set, we can compute similarities exhaustively and rank documents in the usual way. Since there are many fewer clusters than documents, finding the closest cluster is fast; and since the documents matching a query are all similar to each other, they tend to be in the same clusters. While this algorithm is inexact, the expected decrease in search quality is small. This is essentially the application of clustering that was covered in Section 7.1.6 (page 7.1.6 ). Exercises. Define two documents as similar if they have at least two proper names like Clinton or Sarkozy in common. Give an example of an information need and two documents, for which the cluster hypothesis does not hold for this notion of similarity. Make up a simple one-dimensional example (i.e. points on a line) with two clusters where the inexactness of cluster-based retrieval shows up. In your example, retrieving clusters close to the query should do worse than direct nearest neighbor search. """
iir 16 2,Problem statement,Document clustering,"OBJECTIVE FUNCTION, PARTITIONAL CLUSTERING, EXHAUSTIVE, EXCLUSIVE, CARDINALITY","We can define the goal in hard flat clustering as follows. Given (i) a set of documents D = , (ii) a desired number of clusters K, and (iii) an objective function that evaluates the quality of a clustering, we want to compute an assignment : D that minimizes (or, in other cases, maximizes) the objective function. In most cases, we also demand that is surjective, i.e., that none of the K clusters is empty. The objective function is often defined in terms of similarity or distance between documents. Below, we will see that the objective in K-means clustering is to minimize the average distance between documents and their centroids or, equivalently, to maximize the similarity between documents and their centroids. The discussion of similarity measures and distance metrics in Chapter 14 (page 14.1 ) also applies to this chapter. As in Chapter 14 , we use both similarity and distance to talk about relatedness between documents. For documents, the type of similarity we want is usually topic similarity or high values on the same dimensions in the vector space model. For example, documents about China have high values on dimensions like Chinese, Beijing, and Mao whereas documents about the UK tend to have high values for London, Britain and Queen. We approximate topic similarity with cosine similarity or Euclidean distance in vector space (Chapter 6 ). If we intend to capture similarity of a type other than topic, for example, similarity of language, then a different representation may be appropriate. When computing topic similarity, stop words can be safely ignored, but they are important cues for separating clusters of English (in which the occurs frequently and la infrequently) and French documents (in which the occurs infrequently and la frequently). A note on terminology. An alternative definition of hard clustering is that a document can be a full member of more than one cluster. Partitional clustering always refers to a clustering where each document belongs to exactly one cluster. (But in a partitional hierarchical clustering (Chapter 17 ) all members of a cluster are of course also members of its parent.) On the definition of hard clustering that permits multiple membership, the difference between soft clustering and hard clustering is that membership values in hard clustering are either 0 or 1, whereas they can take on any non-negative value in soft clustering. Some researchers distinguish between exhaustive clusterings that assign each document to a cluster and non-exhaustive clusterings, in which some documents will be assigned to no cluster. Non-exhaustive clusterings in which each document is a member of either no cluster or one cluster are called exclusive . We define clustering to be exhaustive in this book. Cardinality - the number of clusters A difficult issue in clustering is determining the number of clusters or cardinality of a clustering, which we denote by K. Often K is nothing more than a good guess based on experience or domain knowledge. But for K-means, we will also introduce a heuristic method for choosing K and an attempt to incorporate the selection of K into the objective function. Sometimes the application puts constraints on the range of K. For example, the Scatter-Gather interface in Figure 16.3 could not display more than about K=10 clusters per layer because of the size and resolution of computer monitors in the early 1990s. Since our goal is to optimize an objective function, clustering is essentially a search problem. The brute force solution would be to enumerate all possible clusterings and pick the best. However, there are exponentially many partitions, so this approach is not feasible.[*] For this reason, most flat clustering algorithms refine an initial partitioning iteratively. If the search starts at an unfavorable initial point, we may miss the global optimum. Finding a good starting point is therefore another important problem we have to solve in flat clustering. """
iir 16 3,Evaluation of clustering,"Document clustering,K-means clustering","INTERNAL CRITERION of quality, external CRITERION of quality, purity, normalized mutual information, RAND INDEX, F MEASURE"," Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low inter-cluster similarity (documents from different clusters are dissimilar). This is an internal criterion for the quality of a clustering. But good scores on an internal criterion do not necessarily translate into good effectiveness in an application. An alternative to internal criteria is direct evaluation in the application of interest. For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms. This is the most direct evaluation, but it is expensive, especially if large user studies are necessary. As a surrogate for user judgments, we can use a set of classes in an evaluation benchmark or gold standard (see Section 8.5 , page 8.5 , and Section 13.6 , page 13.6 ). The gold standard is ideally produced by human judges with a good level of inter-judge agreement (see Chapter 8 , page 8.1 ). We can then compute an external criterion that evaluates how well the clustering matches the gold standard classes. For example, we may want to say that the optimal clustering of the search results for jaguar in Figure 16.2 consists of three classes corresponding to the three senses car, animal, and operating system. In this type of evaluation, we only use the partition provided by the gold standard, not the class labels. This section introduces four external criteria of clustering quality. Purity is a simple and transparent evaluation measure. Normalized mutual information can be information-theoretically interpreted. The Rand index penalizes both false positive and false negative decisions during clustering. The F measure in addition supports differential weighting of these two types of errors. To compute purity , each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by N. Formally: where Omega = is the set of clusters and {C} = is the set of classes. We interpret k as the set of documents in k and c j as the set of documents in c j in Equation 182. We present an example of how to compute purity in Figure 16.4 .[*] Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1 . Purity is compared with the other three measures discussed in this chapter in Table 16.2 . High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. A measure that allows us to make this tradeoff is normalized mutual information or NMI : (183) I is mutual information (cf. Chapter 13 , page 13.5.1 ): I( Omega ; {C} ) = k j P( k c j) {P( k c j)}{P( k)P(c j)} (184) = k j { k c j}{N} {N k c j}{ k c j} (185) where P( k), P(c j), and P( k c j) are the probabilities of a document being in cluster k, class c j, and in the intersection of k and c j, respectively. Equation 185 is equivalent to Equation 184 for maximum likelihood estimates of the probabilities (i.e., the estimate of each probability is the corresponding relative frequency). H is entropy as defined in Chapter 5 (page 5.3.2 ): H(Omega) = - k P( k) P( k) (186) = - k { k}{N} { k}{N} (187) where, again, the second equation is based on maximum likelihood estimates of the probabilities. I( Omega ; {C} ) in Equation 184 measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are. The minimum of I( Omega ; {C} ) is 0 if the clustering is random with respect to class membership. In that case, knowing that a document is in a particular cluster does not give us any new information about what its class might be. Maximum mutual information is reached for a clustering Omega {exact} that perfectly recreates the classes - but also if clusters in Omega {exact} are further subdivided into smaller clusters (Exercise 16.7 ). In particular, a clustering with K=N one-document clusters has maximum MI. So MI has the same problem as purity: it does not penalize large cardinalities and thus does not formalize our bias that, other things being equal, fewer clusters are better. The normalization by the denominator [H(Omega )+H({C} )]/2 in Equation 183 fixes this problem since entropy tends to increase with the number of clusters. For example, H(Omega) reaches its maximum N for K=N, which ensures that NMI is low for K=N. Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters. The particular form of the denominator is chosen because [H(Omega )+H({C} )]/2 is a tight upper bound on I( Omega ; {C} ) (Exercise 16.7 ). Thus, NMI is always a number between 0 and 1. An alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions, one for each of the N(N-1)/2 pairs of documents in the collection. We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters. The Rand index ( ) measures the percentage of decisions that are correct. That is, it is simply accuracy (Section 8.3 , page 8.3 ). As an example, we compute RI for Figure 16.4 . We first compute {TP}+{FP}. The three clusters contain 6, 6, and 5 points, respectively, so the total number of ``positives'' or pairs of documents that are in the same cluster is: (188) Of these, the x pairs in cluster 1, the o pairs in cluster 2, the pairs in cluster 3, and the x pair in cluster 3 are true positives: (189) Thus, {FP}=40-20=20. {FN} and {TN} are computed similarly, resulting in the following contingency table: Same cluster Different clusters Same class {TP} = 20 {FN} = 24 Different classes {FP} = 20 {TN} = 72 {RI} is then (20+72)/(20+20+24+72) 0.68. The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the F measure measuresperf to penalize false negatives more strongly than false positives by selecting a value > 1, thus giving more weight to recall. Based on the numbers in the contingency table, P= 20/40 = 0.5 and R= 20/44 0.455. This gives us F 1 0.48 for = 1 and F 5 0.456 for =5. In information retrieval, evaluating clustering with F has the advantage that the measure is already familiar to the research community. Exercises. Replace every point d in Figure 16.4 with two identical copies of d in the same class. (i) Is it less difficult, equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in Figure 16.4 ? (ii) Compute purity, NMI, RI, and F 5 for the clustering with 34 points. Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings? """
iir 16 4,K-means,"Document clustering,K-means clustering","CENTROID, RESIDUAL SUM OF squares, seed, OUTLIER, SINGLETON CLUSTER, K-MEDOIDS, MEDOID","K-means is the most important flat clustering algorithm. Its objective is to minimize the average squared Euclidean distance (Chapter 6 , page 6.4.4 ) of documents from their cluster centers where a cluster center is defined as the mean or centroid {} of the documents in a cluster : (190) The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way. We used centroids for Rocchio classification in Chapter 14 (page 14.2 ). They play a similar role here. The ideal cluster in K-means is a sphere with the centroid as its center of gravity. Ideally, the clusters should not overlap. Our desiderata for classes in Rocchio classification were the same. The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster. A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS , the squared distance of each vector from its centroid summed over all vectors: {RSS} = {k=1}^K {RSS} k (191) RSS is the objective function in K-means and our goal is to minimize it. Since N is fixed, minimizing RSS is equivalent to minimizing the average squared distance, a measure of how well centroids represent their documents. The first step of K-means is to select as initial cluster centers K randomly selected documents, the seeds . The algorithm then moves the cluster centers around in space in order to minimize RSS. As shown in Figure 16.5 , this is done iteratively by repeating two steps until a stopping criterion is met: reassigning documents to the cluster with the closest centroid; and recomputing each centroid based on the current members of its cluster. Figure 16.6 shows snapshots from nine iterations of the K-means algorithm for a set of points. The ``centroid'' column of Table 17.2 (page 17.2 ) shows examples of centroids. We can apply one of the following termination conditions. A fixed number of iterations I has been completed. This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations. Assignment of documents to clusters (the partitioning function ) does not change between iterations. Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long. Centroids k do not change between iterations. This is equivalent to not changing (Exercise 16.4.1 ). Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. In practice, we need to combine it with a bound on the number of iterations to guarantee termination. Terminate when the decrease in RSS falls below a threshold . For small , this indicates that we are close to convergence. Again, we need to combine it with a bound on the number of iterations to prevent very long runtimes. We now show that K-means converges by proving that {RSS} monotonically decreases in each iteration. We will use decrease in the meaning decrease or does not change in this section. First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to {RSS} decreases. Second, it decreases in the recomputation step because the new centroid is the vector {v} for which {RSS} k reaches its minimum. {RSS} k({v}) = {{x} k} {v}-{x}^2 = {{x} k} {m=1}^M (v {m}-x {m})^2 (192) { {RSS} k({v})} { v m} = {{x} k} 2 (v {m}-x {m}) (193) where x m and v m are the m^{th} components of their respective vectors. Setting the partial derivative to zero, we get: v m = {1}{ k} {{x} k} x {m} (194) which is the componentwise definition of the centroid. Thus, we minimize {RSS} k when the old centroid is replaced with the new centroid. {RSS}, the sum of the {RSS} k, must then also decrease during recomputation. Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum. Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids. Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost. While this proves the convergence of K-means, there is unfortunately no guarantee that a global minimum in the objective function will be reached. This is a particular problem if a document set contains many outliers , documents that are far from any other documents and therefore do not fit well into any cluster. Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations. Thus, we end up with a singleton cluster (a cluster with only one document) even though there is probably a clustering with lower RSS. Figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds. Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise 16.7 ). Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering. Since deterministic hierarchical clustering methods are more predictable than K-means, a hierarchical clustering of a small random sample of size i K (e.g., for i =5 or i =10) often provides good seeds (see the description of the Buckshot algorithm, Chapter 17 , page 17.8 ). Other initialization methods compute seeds that are not selected from the vectors to be clustered. A robust method that works well for a large variety of document distributions is to select i (e.g., i =10) random vectors for each cluster and use their centroid as the seed for this cluster. See Section 16.6 for more sophisticated initializations. What is the time complexity of K-means? Most of the time is spent on computing vector distances. One such operation costs Theta(M). The reassignment step computes KN distances, so its overall complexity is Theta(KNM). In the recomputation step, each vector gets added to a centroid once, so the complexity of this step is Theta(NM). For a fixed number of iterations I, the overall complexity is therefore Theta(IKNM). Thus, K-means is linear in all relevant factors: iterations, number of clusters, number of vectors and dimensionality of the space. This means that K-means is more efficient than the hierarchical algorithms in Chapter 17 . We had to fix the number of iterations I, which can be tricky in practice. But in most cases, K-means quickly reaches either complete convergence or a clustering that is close to convergence. In the latter case, a few documents would switch membership if further iterations were computed, but this has a small effect on the overall quality of the clustering. There is one subtlety in the preceding argument. Even a linear algorithm can be quite slow if one of the arguments of Theta() is large, and M usually is large. High dimensionality is not a problem for computing the distance between two documents. Their vectors are sparse, so that only a small fraction of the theoretically possible M componentwise differences need to be computed. Centroids, however, are dense since they pool all terms that occur in any of the documents of their clusters. As a result, distance computations are time consuming in a naive implementation of K-means. However, there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities. Truncating centroids to the most significant k terms (e.g., k=1000) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in Section 16.6 ). The same efficiency problem is addressed by K-medoids , a variant of K-means that computes medoids instead of centroids as cluster centers. We define the medoid of a cluster as the document vector that is closest to the centroid. Since medoids are sparse document vectors, distance computations are fast. Estimated minimal residual sum of squares as a function of the number of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there are two points where the {{RSS}} {min} curve flattens: at 4 clusters and at 9 clusters. The documents were selected from the categories China, Germany, Russia and Sports, so the K=4 clustering is closest to the Reuters classification. """
iir 16 4 1,Cluster cardinality in K-means,Document clustering,,"We stated in Section 16.2 that the number of clusters K is an input to most flat clustering algorithms. What do we do if we cannot come up with a plausible guess for K? A naive approach would be to select the optimal value of K according to the objective function, namely the value of K that minimizes RSS. Defining {RSS} {min}(K) as the minimal RSS of all clusterings with K clusters, we observe that {RSS} {min}(K) is a monotonically decreasing function in K (Exercise 16.7 ), which reaches its minimum 0 for K=N where N is the number of documents. We would end up with each document being in its own cluster. Clearly, this is not an optimal clustering. A heuristic method that gets around this problem is to estimate {RSS} {min}(K) as follows. We first perform i (e.g., i =10) clusterings with K clusters (each with a different initialization) and compute the RSS of each. Then we take the minimum of the i RSS values. We denote this minimum by {{RSS}} {min}(K). Now we can inspect the values {{RSS}} {min}(K) as K increases and find the ``knee'' in the curve - the point where successive decreases in {{RSS}} {min} become noticeably smaller. There are two such points in Figure 16.8 , one at K=4, where the gradient flattens slightly, and a clearer flattening at K=9. This is typical: there is seldom a single best number of clusters. We still need to employ an external constraint to choose from a number of possible values of K (4 and 9 in this case). A second type of criterion for cluster cardinality imposes a penalty for each new cluster - where conceptually we start with a single cluster containing all documents and then search for the optimal number of clusters K by successively incrementing K by one. To determine the cluster cardinality in this way, we create a generalized objective function that combines two elements: distortion , a measure of how much documents deviate from the prototype of their clusters (e.g., RSS for K-means); and a measure of model complexity . We interpret a clustering here as a model of the data. Model complexity in clustering is usually the number of clusters or a function thereof. For K-means, we then get this selection criterion for K: K = K [{RSS} {min}(K) + K] (195) where is a weighting factor. A large value of favors solutions with few clusters. For =0, there is no penalty for more clusters and K=N is the best solution. The obvious difficulty with Equation 195 is that we need to determine . Unless this is easier than determining K directly, then we are back to square one. In some cases, we can choose values of that have worked well for similar data sets in the past. For example, if we periodically cluster news stories from a newswire, there is likely to be a fixed value of that gives us the right K in each successive clustering. In this application, we would not be able to determine K based on past experience since K changes. A theoretical justification for Equation 195 is the Akaike Information Criterion or AIC, an information-theoretic measure that trades off distortion against model complexity. The general form of AIC is: (196) where -L(K), the negative maximum log-likelihood of the data for K clusters, is a measure of distortion and q(K), the number of parameters of a model with K clusters, is a measure of model complexity. We will not attempt to derive the AIC here, but it is easy to understand intuitively. The first property of a good model of the data is that each data point is modeled well by the model. This is the goal of low distortion. But models should also be small (i.e., have low model complexity) since a model that merely describes the data (and therefore has zero distortion) is worthless. AIC provides a theoretical justification for one particular way of weighting these two factors, distortion and model complexity, when selecting a model. For K-means, the AIC can be stated as follows: (197) Equation 197 is a special case of Equation 195 for = 2M. To derive Equation 197 from Equation 196 observe that q(K) = KM in K-means since each element of the K centroids is a parameter that can be varied independently; and that L(K) = -(1/2) {RSS} {min}(K) (modulo a constant) if we view the model underlying K-means as a Gaussian mixture with hard assignment, uniform cluster priors and identical spherical covariance matrices (see Exercise 16.7 ). The derivation of AIC is based on a number of assumptions, e.g., that the data are . These assumptions are only approximately true for data sets in information retrieval. As a consequence, the AIC can rarely be applied without modification in text clustering. In Figure 16.8 , the dimensionality of the vector space is M 50,000. Thus, 2MK > 50{,}000 dominates the smaller RSS-based term ( {{RSS}} {min}(1) <5000, not shown in the figure) and the minimum of the expression is reached for K=1. But as we know, K=4 (corresponding to the four classes China, Germany, Russia and Sports) is a better choice than K=1. In practice, Equation 195 is often more useful than Equation 197 - with the caveat that we need to come up with an estimate for . Exercises. Why are documents that do not use the same term for the concept car likely to end up in the same cluster in K-means clustering? Two of the possible termination conditions for K-means were (1) assignment does not change, (2) centroids do not change (page 16.4 ). Do these two conditions imply each other? """
iir 16 5,Model-based clustering,,"MODEL-BASED clustering, EXPECTATION-maximization algorithm, EXPECTATION STEP, MAXIMIZATION STEP","In this section, we describe a generalization of K-means, the EM algorithm. It can be applied to a larger variety of document representations and distributions than K-means. In K-means, we attempt to find centroids that are good representatives. We can view the set of K centroids as a model that generates the data. Generating a document in this model consists of first picking a centroid at random and then adding some noise. If the noise is normally distributed, this procedure will result in clusters of spherical shape. Model-based clustering assumes that the data were generated by a model and tries to recover the original model from the data. The model that we recover from the data then defines clusters and an assignment of documents to clusters. A commonly used criterion for estimating the model parameters is maximum likelihood. In K-means, the quantity (-{RSS}) is proportional to the likelihood that a particular model (i.e., a set of centroids) generated the data. For K-means, maximum likelihood and minimal RSS are equivalent criteria. We denote the model parameters by Theta. In K-means, Theta = . More generally, the maximum likelihood criterion is to select the parameters Theta that maximize the log-likelihood of generating the data D: (198) L(DTheta) is the objective function that measures the goodness of the clustering. Given two clusterings with the same number of clusters, we prefer the one with higher L(DTheta). This is the same approach we took in Chapter 12 (page 12.1.1 ) for language modeling and in Section 13.1 (page 13.4 ) for text classification. In text classification, we chose the class that maximizes the likelihood of generating a particular document. Here, we choose the clustering Theta that maximizes the likelihood of generating a given set of documents. Once we have Theta, we can compute an assignment probability P(d k;Theta) for each document-cluster pair. This set of assignment probabilities defines a soft clustering. An example of a soft assignment is that a document about Chinese cars may have a fractional membership of 0.5 in each of the two clusters China and automobiles, reflecting the fact that both topics are pertinent. A hard clustering like K-means cannot model this simultaneous relevance to two topics. Model-based clustering provides a framework for incorporating our knowledge about a domain. K-means and the hierarchical algorithms in Chapter 17 make fairly rigid assumptions about the data. For example, clusters in K-means are assumed to be spheres. Model-based clustering offers more flexibility. The clustering model can be adapted to what we know about the underlying distribution of the data, be it Bernoulli (as in the example in Table 16.3 ), Gaussian with non-spherical variance (another model that is important in document clustering) or a member of a different family. A commonly used algorithm for model-based clustering is the Expectation-Maximization algorithm or EM algorithm . EM clustering is an iterative algorithm that maximizes L(DTheta). EM can be applied to many different types of probabilistic modeling. We will work with a mixture of multivariate Bernoulli distributions here, the distribution we know from Section 11.3 (page 11.3 ) and Section 13.3 (page 13.3 ): P(d k ; Theta) = ( { m d} q {mk} ) ( { m d} (1-q {mk}) ) (199) where Theta = , Theta k = ( k , q {1k}, , q {Mk}), and q {mk} = P( m=1 k) are the parameters of the model.[*] P( m=1 k) is the probability that a document from cluster k contains term m. The probability k is the prior of cluster k: the probability that a document d is in k if we have no information about d. The mixture model then is: P(d Theta) = {k=1}^{K} k ( { m d} q {mk} ) ( { m d} (1-q {mk}) ) (200) In this model, we generate a document by first picking a cluster k with probability k and then generating the terms of the document according to the parameters q {mk}. Recall that the document representation of the multivariate Bernoulli is a vector of M Boolean values (and not a real-valued vector). How do we use EM to infer the parameters of the clustering from the data? That is, how do we choose parameters Theta that maximize L(DTheta)? EM is similar to K-means in that it alternates between an expectation step , corresponding to reassignment, and a maximization step , corresponding to recomputation of the parameters of the model. The parameters of K-means are the centroids, the parameters of the instance of EM in this section are the k and q {mk}. The maximization step recomputes the conditional parameters q {mk} and the priors k as follows: { Maximization step:} q {mk} = { {n=1}^N r... ... d n )} { {n=1}^N r {nk}} k = { {n=1}^N r {nk}}{N} (201) where I( m d n )=1 if m d n and 0 otherwise and r {nk} is the soft assignment of document d n to cluster k as computed in the preceding iteration. (We'll address the issue of initialization in a moment.) These are the maximum likelihood estimates for the parameters of the multivariate Bernoulli from Table 13.3 (page 13.3 ) except that documents are assigned fractionally to clusters here. These maximum likelihood estimates maximize the likelihood of the data given the model. The expectation step computes the soft assignment of documents to clusters given the current parameters q {mk} and k: { Expectation step:} r {nk} = { k ( {... ... ( { m d n} q {mk}) ( { m d n} (1-q {mk})) } (202) This expectation step applies and 200 to computing the likelihood that k generated document d n. It is the classification procedure for the multivariate Bernoulli in Table 13.3 . Thus, the expectation step is nothing else but Bernoulli Naive Bayes classification (including normalization, i.e. dividing by the denominator, to get a probability distribution over clusters). The EM clustering algorithm.The table shows a set of documents (a) and parameter values for selected iterations during EM clustering (b). Parameters shown are prior 1, soft assignment scores r {n,1} (both omitted for cluster 2), and lexical parameters q {m,k} for a few terms. The authors initially assigned document 6 to cluster 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For smoothing, the r {nk} in Equation 201 were replaced with r {nk}+ where = 0.0001. We clustered a set of 11 documents into two clusters using EM in Table 16.3 . After convergence in iteration 25, the first 5 documents are assigned to cluster 1 ( r {i,1} = 1.00) and the last 6 to cluster 2 (r {i,1}=0.00). Somewhat atypically, the final assignment is a hard assignment here. EM usually converges to a soft assignment. In iteration 25, the prior 1 for cluster 1 is 5/11 0.45 because 5 of the 11 documents are in cluster 1. Some terms are quickly associated with one cluster because the initial assignment can ``spread'' to them unambiguously. For example, membership in cluster 2 spreads from document 7 to document 8 in the first iteration because they share sugar (r {8,1}=0 in iteration 1). For parameters of terms occurring in ambiguous contexts, convergence takes longer. Seed documents 6 and 7 both contain sweet. As a result, it takes 25 iterations for the term to be unambiguously associated with cluster 2. (q {sweet,1}=0 in iteration 25.) Finding good seeds is even more critical for EM than for K-means. EM is prone to get stuck in local optima if the seeds are not chosen well. This is a general problem that also occurs in other applications of EM.[*]Therefore, as with K-means, the initial assignment of documents to clusters is often computed by a different algorithm. For example, a hard K-means clustering may provide the initial assignment, which EM can then ``soften up.'' Exercises. We saw above that the time complexity of K-means is Theta(IKNM). What is the time complexity of EM? """
Information retrieval,Information retrieval,,,"Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called information overload. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications. == Overview == An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy. An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata. Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. == History == The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a statistical machine - filed by Emanuel Goldberg in the 1920s and 30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further. == Model types == For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. === First dimension: mathematical basis === Set-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are: Standard Boolean model Extended Boolean model Fuzzy retrieval Algebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. Vector space model Generalized vector space model (Enhanced) Topic-based Vector Space Model Extended Boolean model Latent semantic indexing a.k.a. latent semantic analysis Probabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes theorem are often used in these models. Binary Independence Model Probabilistic relevance model on which is based the okapi (BM25) relevance function Uncertain inference Language models Divergence-from-randomness model Latent Dirichlet allocation Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature. === Second dimension: properties of the model === Models without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables. Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents. Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.) == Performance and correctness measures == The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy. Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks. The mathematical symbols used in the formulas below mean: X Š—ç’ Î© Y { X Y} - Intersection - in this case, specifying the documents in both sets X and Y | X | { |X|} - Cardinality - in this case, the number of documents in set X Š—ç’ ’ö { } - Integral Š—ç’ ’” { } - Summation ’â’– { } - Symmetric difference === Precision === Precision is the fraction of the documents retrieved that are relevant to the users information need. precision = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { retrieved documents } | {={} }}|}{|}}|}}} In binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n. Note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics. === Recall === Recall is the fraction of the documents that are relevant to the query that are successfully retrieved. recall = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { relevant documents } | {={} }}|}{|}}|}}} In binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query. It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision. === Fall-out === The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available: fall-out = | { non-relevant documents } Š—ç’ Î© { retrieved documents } | | { non-relevant documents } | {={} }}|}{|}}|}}} In binary classification, fall-out is closely related to specificity and is equal to ( 1 Š—ç’ ’« specificity ) { (1-{})} . It can be looked at as the probability that a non-relevant document is retrieved by the query. It is trivial to achieve fall-out of 0% by returning zero documents in response to any query. === F-score / F-measure === The weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is: F = 2 Š—ç’“’ p r e c i s i o n Š—ç’“’ r e c a l l ( p r e c i s i o n + r e c a l l ) { F={ }{( + )}}} This is also known as the F 1 { F {1}} measure, because recall and precision are evenly weighted. The general formula for non-negative real ’â { } is: F ’â = ( 1 + ’â 2 ) Š—ç’“’ ( p r e c i s i o n Š—ç’“’ r e c a l l ) ( ’â 2 Š—ç’“’ p r e c i s i o n + r e c a l l ) { F { }={) ( )}{( ^{2} + )}},} Two other commonly used F measures are the F 2 { F {2}} measure, which weights recall twice as much as precision, and the F 0.5 { F {0.5}} measure, which weights precision twice as much as recall. The F-measure was derived by van Rijsbergen (1979) so that F ’â { F { }} measures the effectiveness of retrieval with respect to a user who attaches ’â { } times as much importance to recall as precision. It is based on van Rijsbergens effectiveness measure E = 1 Š—ç’ ’« 1 ’âÎ± P + 1 Š—ç’ ’« ’âÎ± R { E=1-{{{{P}}+{{R}}}}} . Their relationship is: F ’â = 1 Š—ç’ ’« E { F { }=1-E} where ’âÎ± = 1 1 + ’â 2 { ={{1+ ^{2}}}} F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it. === Average precision === Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision p ( r ) { p(r)} as a function of recall r { r} . Average precision computes the average value of p ( r ) { p(r)} over the interval from r = 0 { r=0} to r = 1 { r=1} : AveP = Š—ç’ ’ö 0 1 p ( r ) d r { = {0}^{1}p(r)dr} That is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents: AveP = Š—ç’ ’” k = 1 n P ( k ) ’â’– r ( k ) { = {k=1}^{n}P(k) r(k)} where k { k} is the rank in the sequence of retrieved documents, n { n} is the number of retrieved documents, P ( k ) { P(k)} is the precision at cut-off k { k} in the list, and ’â’– r ( k ) { r(k)} is the change in recall from items k Š—ç’ ’« 1 { k-1} to k { k} . This finite sum is equivalent to: AveP = Š—ç’ ’” k = 1 n ( P ( k ) ’‘’• rel Š—çÎ’ ( k ) ) number of relevant documents { ={^{n}(P(k) (k))}{}}!} where rel Š—çÎ’ ( k ) { (k)} is an indicator function equaling 1 if the item at rank k { k} is a relevant document, zero otherwise. Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero. Some authors choose to interpolate the p ( r ) { p(r)} function to reduce the impact of wiggles in the curve. For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}: AveP = 1 11 Š—ç’ ’” r Š—ç’ ’ { 0 , 0.1 , Š—ç’ , 1.0 } p interp ( r ) { ={{11}} {r }p { }(r)} where p interp ( r ) { p { }(r)} is an interpolated precision that takes the maximum precision over all recalls greater than r { r} : p interp ( r ) = max r ~ : r ~ Š—ç’ Î‚ r Š—çÎ’ p ( r ~ ) { p { }(r)= {{}:{} r}p({})} . An alternative is to derive an analytical p ( r ) { p(r)} function by assuming a particular parametric distribution for the underlying decision values. For example, a binormal precision-recall curve can be obtained by assuming decision values in both classes to follow a Gaussian distribution. === Precision at K === For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or Precision at 10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k. Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1. It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not. === R-Precision === R-precision requires knowing all documents that are relevant to a query. The number of relevant documents, R { R} , is used as the cutoff for calculation, and this varies from query to query. For example, if there are 15 documents relevant to red in a corpus (R=15), R-precision for red looks at the top 15 documents returned, counts the number that are relevant r { r} turns that into a relevancy fraction: r / R = r / 15 { r/R=r/15} . Precision is equal to recall at the R-th position. Empirically, this measure is often highly correlated to mean average precision. === Mean average precision === Mean average precision for a set of queries is the mean of the average precision scores for each query. MAP = Š—ç’ ’” q = 1 Q A v e P ( q ) Q { ={^{Q} }{Q}}!} where Q is the number of queries. === Discounted cumulative gain === DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The DCG accumulated at a particular rank position p { p} is defined as: D C G p = r e l 1 + Š—ç’ ’” i = 2 p r e l i log 2 Š—çÎ’ i . { } =rel {1}+ {i=2}^{p}{}{ {2}i}}.} Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( I D C G p { IDCG {p}} ), which normalizes the score: n D C G p = D C G p I D C G p . { } ={}{IDCG{p}}}.} The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the D C G p { DCG {p}} will be the same as the I D C G p { IDCG {p}} producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable. === Other measures === Mean reciprocal rank Spearmans rank correlation coefficient bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents GMAP - geometric mean of (per-topic) average precision Measures based on marginal relevance and document diversity - see Relevance (information retrieval) ’ÇÎ Problems and alternatives === Visualization === Visualizations of information retrieval performance include: Graphs which chart precision on one axis and recall on the other Histograms of average precision over various topics Receiver operating characteristic (ROC curve) Confusion matrix == Timeline == Before the 1900s 1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations. 1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium. 1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data. 1920s-1930s Emanuel Goldberg submits patents for his Statistical MachineŠ—ç’ Î a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents. 1940s-1950s late 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans. 1945: Vannevar Bushs As We May Think appeared in Atlantic Monthly. 1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds. 1950s: Growing concern in the US for a science gap with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield). 1950: The term information retrieval was coined by Calvin Mooers. 1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT. 1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed framework for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved. 1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959) 1959: Hans Peter Luhn published Auto-encoding of documents for information retrieval. 1960s: early 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell. 1960: Melvin Earl Maron and John Lary Kuhns published On relevance, probabilistic indexing, and information retrieval in the Journal of the ACM 7(3):216-244, July 1960. 1962: Cyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems. Cranfield Collection of Aeronautics, Cranfield, England, 1962. Kent published Information Analysis and Retrieval. 1963: Weinberg report Science, Government and Information gave a full articulation of the idea of a crisis of scientific information. The report was named after Dr. Alvin Weinberg. Joseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963). 1964: Karen Sp’‘ rck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR. The National Bureau of Standards sponsored a symposium titled Statistical Association Methods for Mechanized Documentation. Several highly significant papers, including G. Saltons first published reference (we believe) to the SMART system. mid-1960s: National Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system. Project Intrex at MIT. 1965: J. C. R. Licklider published Libraries of the Future. 1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs. late 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval. 1968: Gerard Salton published Automatic Information Organization and Retrieval. John W. Sammon, Jr.s RADC Tech report Some Mathematics of Information Storage and Retrieval... outlined the vector model. 1969: Sammons A nonlinear mapping for data structure analysis (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system. 1970s early 1970s: First online systemsŠ—ç’ ’–NLMs AIM-TWX, MEDLINE; Lockheeds Dialog; SDCs ORBIT. Theodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines. 1971: Nicholas Jardine and Cornelis J. van Rijsbergen published The use of hierarchic clustering in information retrieval, which articulated the cluster hypothesis. 1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model: A Theory of Indexing (Society for Industrial and Applied Mathematics) A Theory of Term Importance in Automatic Text Analysis (JASIS v. 26) A Vector Space Model for Automatic Indexing (CACM 18:11) 1978: The First ACM SIGIR conference. 1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models. 1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback. 1980s 1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge. 1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing. 1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models. 1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System mid-1980s: Efforts to develop end-user versions of commercial IR systems. 1985-1993: Key papers on and experimental systems for visualization interfaces. Work by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others. 1989: First World Wide Web proposals by Tim Berners-Lee at CERN. 1990s 1992: First TREC conference. 1997: Publication of Korfhages Information Storage and Retrieval with emphasis on visualization and multi-reference point systems. late 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models. == Major Conferences == SIGIR: Conference on Research and Development in Information Retrieval ECIR: European Conference on Information Retrieval CIKM: Conference on Information and Knowledge Management WWW: International World Wide Web Conference WSDM: Conference on Web Search and Data Mining ICTIR: International Conference on Theory of Information Retrieval == Awards in the field == Tony Kent Strix award Gerard Salton Award == Leading IR Research Groups == Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts Amherst Information Retrieval Group at the University of Glasgow Information and Language Processing Systems (ILPS) at the University of Amsterdam == See also == == References == == Further reading == Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch’‘ tze. Introduction to Information Retrieval. Cambridge University Press, 2008. Stefan B’‘ ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010. == External links == ACM SIGIR: Information Retrieval Special Interest Group BCS IRSG: British Computer Society - Information Retrieval Specialist Group Text Retrieval Conference (TREC) Forum for Information Retrieval Evaluation (FIRE) Information Retrieval (online book) by C. J. van Rijsbergen Information Retrieval Wiki Information Retrieval Facility Information Retrieval @ DUTH TREC report on information retrieval evaluation techniques How eBay measures search relevance Information retrieval performance evaluation tool @ Athena Research Centre"
Accuracy and precision,Accuracy and precision,,,"Precision is a description of random errors, a measure of statistical variability. Accuracy has two definitions: More commonly, it is a description of systematic errors, a measure of statistical bias; as these cause a difference between a result and a true value, ISO calls this trueness. Alternatively, ISO defines accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness. In simplest terms, given a set of data points from a series of measurements, the set can be said to be precise if the values are close to the average value of the quantity being measured, while the set can be said to be accurate if the values are close to the true value of the quantity being measured. The two concepts are independent of each other, so a particular set of data can be said to be either accurate, or precise, or both, or neither. == Common definition == In the fields of science and engineering, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantitys true value. The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results. Although the two words precision and accuracy can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method. Interestingly, the field of statistics, where the interpretation of measurements plays a central role, prefers to use the terms bias and variability instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision. A measurement system can be accurate but not precise, precise but not accurate, neither, or both. For example, if an experiment contains a systematic error, then increasing the sample size generally increases precision but does not improve accuracy. The result would be a consistent yet inaccurate string of results from the flawed experiment. Eliminating the systematic error improves accuracy but does not change precision. A measurement system is considered valid if it is both accurate and precise. Related terms include bias (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and error (random variability). The terminology is also applied to indirect measurementsŠ—ç’ ’–that is, values obtained by a computational procedure from observed data. In addition to accuracy and precision, measurements may also have a measurement resolution, which is the smallest change in the underlying physical quantity that produces a response in the measurement. In numerical analysis, accuracy is also the nearness of a calculation to the true value; while precision is the resolution of the representation, typically defined by the number of decimal or binary digits. In military terms, accuracy refers primarily to the accuracy of fire (or justesse de tir), the precision of fire expressed by the closeness of a grouping of shots at and around the centre of the target. === Quantification === In industrial instrumentation, accuracy is the measurement tolerance, or transmission of the instrument and defines the limits of the errors made when the instrument is used in normal operating conditions. Ideally a measurement device is both accurate and precise, with measurements all close to and tightly clustered around the true value. The accuracy and precision of a measurement process is usually established by repeatedly measuring some traceable reference standard. Such standards are defined in the International System of Units (abbreviated SI from French: Syst’‘ÎŒme international dunit’‘Î©s) and maintained by national standards organizations such as the National Institute of Standards and Technology in the United States. This also applies when measurements are repeated and averaged. In that case, the term standard error is properly applied: the precision of the average is equal to the known standard deviation of the process divided by the square root of the number of measurements averaged. Further, the central limit theorem shows that the probability distribution of the averaged measurements will be closer to a normal distribution than that of individual measurements. With regard to accuracy we can distinguish: the difference between the mean of the measurements and the reference value, the bias. Establishing and correcting for bias is necessary for calibration. the combined effect of that and precision. A common convention in science and engineering is to express accuracy and/or precision implicitly by means of significant figures. Here, when not explicitly stated, the margin of error is understood to be one-half the value of the last significant place. For instance, a recording of 843.6 m, or 843.0 m, or 800.0 m would imply a margin of 0.05 m (the last significant place is the tenths place), while a recording of 8,436 m would imply a margin of error of 0.5 m (the last significant digits are the units). A reading of 8,000 m, with trailing zeroes and no decimal point, is ambiguous; the trailing zeroes may or may not be intended as significant figures. To avoid this ambiguity, the number could be represented in scientific notation: 8.0 ’‘’• 103 m indicates that the first zero is significant (hence a margin of 50 m) while 8.000 ’‘’• 103 m indicates that all three zeroes are significant, giving a margin of 0.5 m. Similarly, it is possible to use a multiple of the basic measurement unit: 8.0 km is equivalent to 8.0 ’‘’• 103 m. In fact, it indicates a margin of 0.05 km (50 m). However, reliance on this convention can lead to false precision errors when accepting data from sources that do not obey it. Precision includes: repeatability Š—ç’ ’– the variation arising when all efforts are made to keep conditions constant by using the same instrument and operator, and repeating during a short time period; and reproducibility Š—ç’ ’– the variation arising using the same measurement process among different instruments and operators, and over longer time periods. == ISO definition (ISO 5725) == A shift in the meaning of these terms appeared with the publication of the ISO 5725 series of standards in 1994, which is also reflected in the 2008 issue of the BIPM International Vocabulary of Metrology (VIM), items 2.13 and 2.14. According to ISO 5725-1, the general term accuracy is used to describe the closeness of a measurement to the true value. When the term is applied to sets of measurements of the same measurand, it involves a component of random error and a component of systematic error. In this case trueness is the closeness of the mean of a set of measurement results to the actual (true) value and precision is the closeness of agreement among a set of results. ISO 5725-1 and VIM also avoid the use of the term bias, previously specified in BS 5497-1, because it has different connotations outside the fields of science and engineering, as in medicine and law. == In binary classification == Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. To make the context clear by the semantics, it is often referred to as the Rand accuracy or Rand index. It is a parameter of the test. == In psychometrics and psychophysics == In psychometrics and psychophysics, the term accuracy is interchangeably used with validity and constant error. Precision is a synonym for reliability and variable error. The validity of a measurement instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal consistency test like Cronbachs alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population. == In logic simulation == In logic simulation, a common mistake in evaluation of accurate models is to compare a logic simulation model to a transistor circuit simulation model. This is a comparison of differences in precision, not accuracy. Precision is measured with respect to detail and accuracy is measured with respect to reality. == In information systems == The concepts of accuracy and precision have also been studied in the context of databases, information systems and their sociotechnical context. The necessary extension of these two concepts on the basis of theory of science suggests that they (as well as data quality and information quality) should be centered on accuracy defined as the closeness to the true value seen as the degree of agreement of readings or of calculated values of one same conceived entity, measured or calculated by different methods, in the context of maximum possible disagreement. == See also == == References == == External links == BIPM - Guides in metrology, Guide to the Expression of Uncertainty in Measurement (GUM) and International Vocabulary of Metrology (VIM) Beyond NIST Traceability: What really creates accuracy, Controlled Environments magazine Precision and Accuracy with Three Psychophysical Methods Appendix D.1: Terminology, Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results Accuracy and Precision Accuracy vs Precision Š—ç’ ’– a brief video by Matt Parker Whats the difference between accuracy and precision? by Matt Anticole at TED-Ed S, Alison (30 July 2016). Difference Between Accuracy And Precision - EnglishTipsDaily.com. Englishtipsdaily.com. Retrieved 5 August 2016."
Adversarial information retrieval,Adversarial information retrieval,,,"Adversarial information retrieval (adversarial IR) is a topic in information retrieval related to strategies for working with a data source where some portion of it has been manipulated maliciously. Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation. On the Web, the predominant form of such manipulation is search engine spamming (also known as spamdexing), which involves employing various techniques to disrupt the activity of web search engines, usually for financial gain. Examples of spamdexing are link-bombing, comment or referrer spam, spam blogs (splogs), malicious tagging. Reverse engineering of ranking algorithms, advertisement blocking, click fraud, and web content filtering may also be considered forms of adversarial data manipulation. Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic. == Topics == Topics related to Web spam (spamdexing): Link spam Keyword spamming Cloaking Malicious tagging Spam related to blogs, including comment spam, splogs, and ping spam Other topics: Click fraud detection Reverse engineering of search engines ranking algorithm Web content filtering Advertisement blocking Stealth crawling Troll (Internet) Malicious tagging or voting in social networks Astroturfing Sockpuppetry == History == The term adversarial information retrieval was first coined in 2000 by Andrei Broder (then Chief Scientist at Alta Vista) during the Web plenary session at the TREC-9 conference. == See also == Spamdexing Information retrieval == References == == External links == AIRWeb: series of workshops on Adversarial Information Retrieval on the Web Web Spam Challenge: competition for researchers on Web Spam Detection Web Spam Datasets: datasets for research on Web Spam Detection"
Association for Computing Machinery,Association for Computing Machinery,,,"The Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947 and is the worlds largest scientific and educational computing society. It is a not-for-profit professional membership group. Its membership is more than 100,000 as of 2011. Its headquarters are in New York City. The ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is Advancing Computing as a Science & Profession. == Activities == ACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette. Many of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters. ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer. == Services == === Publications === ACM publishes over 50 journals including the prestigious Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include: ACM XRDS, formerly Crossroads, was redesigned in 2010 and is the most popular student computing magazine in the US. ACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication. ACM Computing Surveys (CSUR) ACM Computers in Entertainment (CIE) A number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include: ACM Transactions on Computer Systems (TOCS) IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB) ACM Transactions on Computational Logic (TOCL) ACM Transactions on Computer-Human Interaction (TOCHI) ACM Transactions on Database Systems (TODS) ACM Transactions on Graphics (TOG) ACM Transactions on Mathematical Software (TOMS) ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) IEEE/ACM Transactions on Networking (TON) ACM Transactions on Programming Languages and Systems (TOPLAS) Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages. ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members. In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry. == Portal and Digital Library == The ACM Portal is an online service of the ACM. Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature. The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries. The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organizations journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature. ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement. ACM was a green publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Librarys permanently maintained Version of Record. All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription. There is also a mounting challenge to the ACMs publication practices coming from the open access movement. Some authors see a centralized peer-review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology. == Membership grades == In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and demonstrated performance that sets them apart from their peers. === Fellows === The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM. There are presently about 958 Fellows out of about 75,000 professional members. === Distinguished Members === In 2006 ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and have made a significant impact on the computing field. Note that in 2006 when the Distinguished Members first came out, one of the three levels was called Distinguished Member and was changed about two years later to Distinguished Educator. Those who already had the Distinguished Member title had their titles changed to one of the other three titles. === Senior Members === Also in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership. == Chapters == ACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters. As of 2011, ACM has professional & SIG Chapters in 56 countries. As of 2014, there exist ACM student chapters in 41 different countries. === Special Interest Groups === == Conferences == ACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005. MobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing The ACM is a co-presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology. There are some conferences hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM. . In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended. For additional non-ACM conferences, see this list of computer science conferences. == Awards == The ACM presents or co-presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology. Over 30 of ACMs Special Interest Groups also award individuals for their contributions with a few listed below. == Leadership == The President of ACM for 2016-2018 is Vicki L. Hanson, Distinguished Professor in the Department of Information Sciences and Technologies at the Rochester Institute of Technology and Professor and Chair of Inclusive Technologies at the University of Dundee, UK. She is successor of Alexander L. Wolf (2014-2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012-2014), an American computer scientist who is recognized as one of the fathers of the Internet; Alain Chesnais (2010-2012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008-2010). ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members-At-Large. This institution is often referred to simply as Council in Communications of the ACM. == Infrastructure == ACM has five Boards that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows: Publications Board SIG Governing Board Education Board Membership Services Board Practitioners Board == ACM Council on Women in Computing == ACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM-Ws main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women and IT, CRA-W. === Athena Lectures === The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers. 2006-2007: Deborah Estrin of UCLA 2007-2008: Karen Sp’‘ rck Jones of Cambridge University 2008-2009: Shafi Goldwasser of MIT and the Weitzmann Institute of Science 2009-2010: Susan Eggers of the University of Washington 2010-2011: Mary Jane Irwin of the Pennsylvania State University 2011-2012: Judith S. Olson of the University of California, Irvine 2012-2013: Nancy Lynch of MIT 2013-2014: Katherine Yelick of LBNL 2014-2015: Susan Dumais of Microsoft Research 2015-2016: Jennifer Widom of Stanford University 2016-2017: Jennifer Rexford of Princeton University == Cooperation == ACMs primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACMs agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula. ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM). == See also == == References == == External links == Official website ACM portal for publications ACM Digital Library Association for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota."
Bayes' theorem,Bayes' theorem,,,"In probability theory and statistics, BayesŠ—ç’ Îé theorem (alternatively BayesŠ—ç’ Îé law or Bayes rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using BayesŠ—ç’ Îé theorem, a personŠ—ç’ Îés age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the persons age. One of the many applications of BayesŠ—ç’ Îé theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in BayesŠ—ç’ Îé theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics. BayesŠ—ç’ Îé theorem is named after Rev. Thomas Bayes (; 1701-1761), who first provided an equation that allows new evidence to update beliefs. It was further developed by Pierre-Simon Laplace, who first published the modern formulation in his 1812 Š—ç’ ’Th’‘Î©orie analytique des probabilit’‘Î©s.Š—ç’ Î Sir Harold Jeffreys put BayesŠ—ç’ Îé algorithm and Laplaces formulation on an axiomatic basis. Jeffreys wrote that BayesŠ—ç’ Îé theorem Š—ç’ ’is to the theory of probability what the Pythagorean theorem is to geometry.Š—ç’ Î == Statement of theorem == Bayes theorem is stated mathematically as the following equation: P ( A Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A ) P ( A ) P ( B ) , { P(A B)={{P(B)}},} where A { A} and B { B} are events and P ( B ) Š—ç’ ’ 0 { P(B) 0} . P ( A ) { P(A)} and P ( B ) { P(B)} are the probabilities of observing A { A} and B { B} without regard to each other. P ( A Š—ç’ Î£ B ) { P(A B)} , a conditional probability, is the probability of observing event A { A} given that B { B} is true. P ( B Š—ç’ Î£ A ) { P(B A)} is the probability of observing event B { B} given that A { A} is true. == History == BayesŠ—ç’ Îé theorem was named after the Reverend Thomas Bayes (1701-1761), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). BayesŠ—ç’ Îé unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society. Price edited BayesŠ—ç’ Îé major work Š—ç’ ’An Essay towards solving a Problem in the Doctrine of ChancesŠ—ç’ Î (1763), which appeared in Š—ç’ ’Philosophical Transactions,Š—ç’ Î and contains BayesŠ—ç’ Îé Theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics. In 1765 he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes. The French mathematician Pierre-Simon Laplace reproduced and extended BayesŠ—ç’ Îé results in 1774, apparently quite unaware of BayesŠ—ç’ Îé work. The Bayesian interpretation of probability was developed mainly by Laplace. Stephen Stigler suggested in 1983 that BayesŠ—ç’ Îé theorem was discovered by Nicholas Saunderson, a blind English mathematician, some time before Bayes; that interpretation, however, has been disputed. Martyn Hooper and Sharon McGrayne have argued that Richard Prices contribution was substantial: By modern standards, we should refer to the Bayes-Price rule. Price discovered BayesŠ—ç’ Îé work, recognized its importance, corrected it, contributed to the article, and found a use for it. The modern convention of employing BayesŠ—ç’ Îé name alone is unfair but so entrenched that anything else makes little sense. == Examples == === Drug testing === Suppose a drug test is 99% sensitive and 99% specific. That is, the test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. What is the probability that a randomly selected individual with a positive test is a user? P ( User Š—ç’ Î£ + ) = P ( + Š—ç’ Î£ User ) P ( User ) P ( + ) = P ( + Š—ç’ Î£ User ) P ( User ) P ( + Š—ç’ Î£ User ) P ( User ) + P ( + Š—ç’ Î£ Non-user ) P ( Non-user ) = 0.99 ’‘’• 0.005 0.99 ’‘’• 0.005 + 0.01 ’‘’• 0.995 Š—ç’ ’ 33.2 % {)&={)P({})}{P(+)}}&={)P({})}{P({})P({})+P({})P({})}}[8pt]&={{0.99 0.005+0.01 0.995}}[8pt]& 33.2%}} Despite the apparent accuracy of the test, if an individual tests positive, it is more likely that they do not use the drug than that they do. This surprising result arises because the number of non-users is very large compared to the number of users; thus the number of false positives outweighs the number of true positives. To use concrete numbers, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.01 ’‘’• 995 Š—ç’ ’Ü 10 false positives are expected. From the 5 users, 0.99 ’‘’• 5 Š—ç’ ’ 5 true positives are expected. Out of 15 positive results, only 5, about 33%, are genuine. This illustrates the importance of base rates, and how the formation of policy can be egregiously misguided if base rates are neglected. The importance of specificity in this example can be seen by calculating that even if sensitivity is raised to 100% and specificity remains at 99% then the probability of the person being a drug user only rises from 33.2% to 33.4%, but if the sensitivity is held at 99% and the specificity is increased to 99.5% then probability of the person being a drug user rises to about 49.9%. === A more complicated example === The entire output of a factory is produced on three machines. The three machines account for different amounts of the factory output, namely 20%, 30%, and 50%. The fraction of defective items produced is this: for the first machine, 5%; for the second machine, 3%; for the third machine, 1%. If an item is chosen at random from the total output and is found to be defective, what is the probability that it was produced by the third machine? A solution is as follows. Let Ai denote the event that a randomly chosen item was made by the ith machine (for i = 1,2,3). Let B denote the event that a randomly chosen item is defective. Then, we are given the following information: P(A1) = 0.2, P(A2) = 0.3, P(A3) = 0.5. If the item was made by the first machine, then the probability that it is defective is 0.05; that is, P(B | A1) = 0.05. Overall, we have P(B | A1) = 0.05, P(B | A2) = 0.03, P(B | A3) = 0.01. To answer the original question, we first find P(B). That can be done in the following way: P(B) = ’âÎ£i P(B | Ai) P(Ai) = (0.05)(0.2) + (0.03)(0.3) + (0.01)(0.5) = 0.024. Hence 2.4% of the total output of the factory is defective. We are given that B has occurred, and we want to calculate the conditional probability of A3. By Bayes theorem, P(A3 | B) = P(B | A3) P(A3)/P(B) = (0.01)(0.50)/(0.024) = 5/24. Given that the item is defective, the probability that it was made by the third machine is only 5/24. Although machine 3 produces half of the total output, it produces a much smaller fraction of the defective items. Hence the knowledge that the item selected was defective enables us to replace the prior probability P(A3) = 1/2 by the smaller posterior probability P(A3 | B) = 5/24. Once again, the answer can be reached without recourse to the formula by applying the conditions to any hypothetical number of cases. For example, if 100,000 items are produced by the factory, 20,000 will be produced by Machine A, 30,000 by Machine B, and 50,000 by Machine C. Machine A will produce 1000 defective items, Machine B 900, and Machine C 500. Of the total 2400 defective items, only 500, or 5/24 were produced by Machine C. == Interpretations == The interpretation of BayesŠ—ç’ Îé theorem depends on the interpretation of probability ascribed to the terms. The two main interpretations are described below. === Bayesian interpretation === In the Bayesian (or epistemological) interpretation, probability measures a Š—ç’ ’degree of belief.Š—ç’ Î BayesŠ—ç’ Îé theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results. For proposition A and evidence B, P (A ), the prior, is the initial degree of belief in A. P (A | B ), the Š—ç’ ’posterior,Š—ç’ Î is the degree of belief having accounted for B. the quotient P(B |A )/P(B) represents the support B provides for A. For more on the application of BayesŠ—ç’ Îé theorem under the Bayesian interpretation of probability, see Bayesian inference. === Frequentist interpretation === In the frequentist interpretation, probability measures a Š—ç’ ’proportion of outcomes.Š—ç’ Î For example, suppose an experiment is performed many times. P(A) is the proportion of outcomes with property A, and P(B) that with property B. P(B | A ) is the proportion of outcomes with property B out of outcomes with property A, and P(A | B ) the proportion of those with A out of those with B. The role of BayesŠ—ç’ Îé theorem is best visualized with tree diagrams, as shown to the right. The two diagrams partition the same outcomes by A and B in opposite orders, to obtain the inverse probabilities. BayesŠ—ç’ Îé theorem serves as the link between these different partitionings. ==== Example ==== An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back. In the rare subspecies, 98% have the pattern, or P(Pattern | Rare) = 98%. In the common subspecies, 5% have the pattern. The rare subspecies accounts for only 0.1% of the population. How likely is the beetle having the pattern to be rare, or what is P(Rare | Pattern)? From the extended form of BayesŠ—ç’ Îé theorem (since any beetle can be only rare or common), P ( Rare Š—ç’ Î£ Pattern ) = P ( Pattern Š—ç’ Î£ Rare ) P ( Rare ) P ( Pattern Š—ç’ Î£ Rare ) P ( Rare ) + P ( Pattern Š—ç’ Î£ Common ) P ( Common ) = 0.98 ’‘’• 0.001 0.98 ’‘’• 0.001 + 0.05 ’‘’• 0.999 Š—ç’ ’ 1.9 % {)&={)P({})}{P({})P({})+P({})P({})}}[8pt]&={{0.98 0.001+0.05 0.999}}[8pt]& 1.9%}} == Forms == === Events === ==== Simple form ==== For events A and B, provided that P(B) Š—ç’ ’ 0, P ( A Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A ) P ( A ) P ( B ) Š—ç’“’ { P(A B)={{P(B)}} } In many applications, for instance in Bayesian inference, the event B is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events A. In such a situation the denominator of the last expression, the probability of the given evidence B, is fixed; what we want to vary is A. BayesŠ—ç’ Îé theorem then shows that the posterior probabilities are proportional to the numerator: P ( A Š—ç’ Î£ B ) Š—ç’ Î P ( A ) Š—ç’“’ P ( B Š—ç’ Î£ A ) { P(A B) P(A) P(B A)} (proportionality over A for given B). In words: posterior is proportional to prior times likelihood. If events A1, A2, ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together, and we know their probabilities up to proportionality, then we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event A, the event A itself and its complement ’Ç’äA are exclusive and exhaustive. Denoting the constant of proportionality by c we have P ( A Š—ç’ Î£ B ) = c Š—ç’“’ P ( A ) Š—ç’“’ P ( B Š—ç’ Î£ A ) and P ( ’Ç’ä A Š—ç’ Î£ B ) = c Š—ç’“’ P ( ’Ç’ä A ) Š—ç’“’ P ( B Š—ç’ Î£ ’Ç’ä A ) . { P(A B)=c P(A) P(B A){}P( A B)=c P( A) P(B A).} Adding these two formulas we deduce that 1 = c Š—ç’“’ ( P ( B Š—ç’ Î£ A ) Š—ç’“’ P ( A ) + P ( B Š—ç’ Î£ ’Ç’ä A ) Š—ç’“’ P ( ’Ç’ä A ) ) , { 1=c (P(B A) P(A)+P(B A) P( A)),} or c = 1 P ( B Š—ç’ Î£ A ) Š—ç’“’ P ( A ) + P ( B Š—ç’ Î£ ’Ç’ä A ) Š—ç’“’ P ( ’Ç’ä A ) = 1 P ( B ) . { c={{P(B A) P(A)+P(B A) P( A)}}={{P(B)}}.} ==== Alternative form ==== Another form of BayesŠ—ç’ Îé Theorem that is generally encountered when looking at two competing statements or hypotheses is: P ( A Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A ) P ( A ) P ( B Š—ç’ Î£ A ) P ( A ) + P ( B Š—ç’ Î£ ’Ç’ä A ) P ( ’Ç’ä A ) Š—ç’“’ { P(A B)={{P(B A)P(A)+P(B A)P( A)}} } For an epistemological interpretation: For proposition A and evidence or background B, P(A),the prior probability, is the initial degree of belief in A. P(Š—ç’ ’«A), is the corresponding probability of the initial degree of belief against A: 1 Š—ç’ ’« P(A) = P(Š—ç’ ’«A) P(B | A), the conditional probability or likelihood, is the degree of belief in B, given that the proposition A is true. P(B | Š—ç’ ’«A), the conditional probability or likelihood, is the degree of belief in B, given that the proposition A is false. P(A | B), the posterior probability, is the probability for A after taking into account B for and against A. ==== Extended form ==== Often, for some partition {Aj} of the sample space, the event space is given or conceptualized in terms of P(Aj) and P(B | Aj). It is then useful to compute P(B) using the law of total probability: P ( B ) = Š—ç’ ’” j P ( B Š—ç’ Î£ A j ) P ( A j ) , { P(B)={ {j}P(B A {j})P(A {j})},} Š—ç’¾’« P ( A i Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A i ) P ( A i ) Š—ç’ ’” j P ( B Š—ç’ Î£ A j ) P ( A j ) Š—ç’“’ { P(A {i} B)={),P(A {i})}{ {j}P(B A {j}),P(A {j})}} } In the special case where A is a binary variable: P ( A Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A ) P ( A ) P ( B Š—ç’ Î£ A ) P ( A ) + P ( B Š—ç’ Î£ ’Ç’ä A ) P ( ’Ç’ä A ) Š—ç’“’ { P(A B)={{P(B A)P(A)+P(B A)P( A)}} } === Random variables === Consider a sample space ’âÎ© generated by two random variables X and Y. In principle, BayesŠ—ç’ Îé theorem applies to the events A = {X = x} and B = {Y = y}. However, terms become 0 at points where either variable has finite probability density. To remain useful, BayesŠ—ç’ Îé theorem may be formulated in terms of the relevant densities (see Derivation). ==== Simple form ==== If X is continuous and Y is discrete, f X ( x Š—ç’ Î£ Y = y ) = P ( Y = y Š—ç’ Î£ X = x ) f X ( x ) P ( Y = y ) . { f {X}(x Y=y)={(x)}{P(Y=y)}}.} If X is discrete and Y is continuous, P ( X = x Š—ç’ Î£ Y = y ) = f Y ( y Š—ç’ Î£ X = x ) P ( X = x ) f Y ( y ) . { P(X=x Y=y)={(y X=x),P(X=x)}{f {Y}(y)}}.} If both X and Y are continuous, f X ( x Š—ç’ Î£ Y = y ) = f Y ( y Š—ç’ Î£ X = x ) f X ( x ) f Y ( y ) . { f {X}(x Y=y)={(y X=x),f {X}(x)}{f {Y}(y)}}.} ==== Extended form ==== A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For fY(y), this becomes an integral: f Y ( y ) = Š—ç’ ’ö Š—ç’ ’« Š—ç’ Š—ç’ f Y ( y Š—ç’ Î£ X = ’â ) f X ( ’â ) d ’â . { f {Y}(y)= {- }^{ }f {Y}(y X= ),f {X}( ),d .} === BayesŠ—ç’ Îé rule === Bayes rule is BayesŠ—ç’ Îé theorem in odds form. O ( A 1 : A 2 Š—ç’ Î£ B ) = O ( A 1 : A 2 ) Š—ç’“’ ’â’ ( A 1 : A 2 Š—ç’ Î£ B ) { O(A {1}:A {2} B)=O(A {1}:A {2}) (A {1}:A {2} B)} where ’â’ ( A 1 : A 2 Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A 1 ) P ( B Š—ç’ Î£ A 2 ) { (A {1}:A {2} B)={)}{P(B A {2})}}} is called the Bayes factor or likelihood ratio and the odds between two events is simply the ratio of the probabilities of the two events. Thus O ( A 1 : A 2 ) = P ( A 1 ) P ( A 2 ) , { O(A {1}:A {2})={)}{P(A {2})}},} O ( A 1 : A 2 Š—ç’ Î£ B ) = P ( A 1 Š—ç’ Î£ B ) P ( A 2 Š—ç’ Î£ B ) , { O(A {1}:A {2} B)={ B)}{P(A {2} B)}},} So the rule says that the posterior odds are the prior odds times the Bayes factor, or in other words, posterior is proportional to prior times likelihood. == Derivation == === For events === BayesŠ—ç’ Îé theorem may be derived from the definition of conditional probability: P ( A Š—ç’ Î£ B ) = P ( A Š—ç’ Î© B ) P ( B ) , if P ( B ) Š—ç’ ’ 0 , { P(A B)={{P(B)}},{}P(B) 0,} P ( B Š—ç’ Î£ A ) = P ( B Š—ç’ Î© A ) P ( A ) , if P ( A ) Š—ç’ ’ 0 , { P(B A)={{P(A)}},{}P(A) 0,} because P ( B Š—ç’ Î© A ) = P ( A Š—ç’ Î© B ) { P(B A)=P(A B)} Š—ç’¾’« P ( A Š—ç’ Î© B ) = P ( A Š—ç’ Î£ B ) P ( B ) = P ( B Š—ç’ Î£ A ) P ( A ) { P(A B)=P(A B),P(B)=P(B A),P(A)} Š—ç’¾’« P ( A Š—ç’ Î£ B ) = P ( B Š—ç’ Î£ A ) P ( A ) P ( B ) , if P ( B ) Š—ç’ ’ 0. { P(A B)={{P(B)}},{}P(B) 0.} === For random variables === For two continuous random variables X and Y, BayesŠ—ç’ Îé theorem may be analogously derived from the definition of conditional density: f X ( x Š—ç’ Î£ Y = y ) = f X , Y ( x , y ) f Y ( y ) { f {X}(x Y=y)={(x,y)}{f {Y}(y)}}} f Y ( y Š—ç’ Î£ X = x ) = f X , Y ( x , y ) f X ( x ) { f {Y}(y X=x)={(x,y)}{f {X}(x)}}} Š—ç’¾’« f X ( x Š—ç’ Î£ Y = y ) = f Y ( y Š—ç’ Î£ X = x ) f X ( x ) f Y ( y ) . { f {X}(x Y=y)={(y X=x),f {X}(x)}{f {Y}(y)}}.} == See also == Bayesian inference Bayesian probability Inductive probability == Notes == == Further reading == Bruss, F. Thomas (2013), Š—ç’ ’250 years of Š—ç’ ’àAn Essay towards solving a Problem in the Doctrine of Chance. By the late Rev. Mr. Bayes, communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S.Š—ç’ Îé,Š—ç’ Î doi:10.1365/s13291-013-0077-z, Jahresbericht der Deutschen Mathematiker-Vereinigung, Springer Verlag, Vol. 115, Issue 3-4 (2013), 129-133. Gelman, A, Carlin, JB, Stern, HS, and Rubin, DB (2003), Š—ç’ ’Bayesian Data Analysis,Š—ç’ Î Second Edition, CRC Press. Grinstead, CM and Snell, JL (1997), Š—ç’ ’Introduction to Probability (2nd edition),Š—ç’ Î American Mathematical Society (free pdf available) [1]. Hazewinkel, Michiel, ed. (2001), Bayes formula, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 McGrayne, SB (2011). The Theory That Would Not Die: How BayesŠ—ç’ Îé Rule Cracked the Enigma Code, Hunted Down Russian Submarines & Emerged Triumphant from Two Centuries of Controversy. Yale University Press. ISBN 978-0-300-18822-6. Laplace, P (1774/1986), Š—ç’ ’Memoir on the Probability of the Causes of Events,Š—ç’ Î Š—ç’ ’Statistical ScienceŠ—ç’ Î 1(3):364-378. Lee, Peter M (2012), Š—ç’ ’Bayesian Statistics: An Introduction,Š—ç’ Î 4th edition. Wiley. ISBN 978-1-118-33257-3. Puga JL, Krzywinski M, Altman N (31 March 2015). BayesŠ—ç’ Îé theorem. Nature Methods. 12 (4): 277-8. Rosenthal, Jeffrey S (2005), Š—ç’ ’Struck by Lightning: The Curious World of Probabilities.Š—ç’ Î HarperCollins. (Granta, 2008. ISBN 9781862079960). Stigler, SM (1986). LaplaceŠ—ç’ Îés 1774 Memoir on Inverse Probability. Statistical Science. 1 (3): 359-363. doi:10.1214/ss/1177013620. Stone, JV (2013), download chapter 1 of Š—ç’ ’BayesŠ—ç’ Îé Rule: A Tutorial Introduction to Bayesian AnalysisŠ—ç’ Î, Sebtel Press, England. Bayesian Reasoning for Intelligent People An introduction and tutorial to the use of Bayes theorem in statistics and cognitive science. == External links == BayesŠ—ç’ Îé theorem at Encyclop’‘ dia Britannica The Theory That Would Not Die by Sharon Bertsch McGrayne New York Times Book Review by John Allen Paulos on 5 August 2011 Visual explanation of Bayes using trees (video) BayesŠ—ç’ Îé frequentist interpretation explained visually (video) Earliest Known Uses of Some of the Words of Mathematics (B). Contains origins of Š—ç’ ’Bayesian,Š—ç’ Î Š—ç’ ’BayesŠ—ç’ Îé Theorem,Š—ç’ Î Š—ç’ ’Bayes Estimate/Risk/Solution,Š—ç’ Î Š—ç’ ’Empirical Bayes,Š—ç’ Î and Š—ç’ ’Bayes Factor.Š—ç’ Î Weisstein, Eric W. BayesŠ—ç’ Îé Theorem. MathWorld. BayesŠ—ç’ Îé theorem at PlanetMath.org. Bayes Theorem and the Folly of Prediction A tutorial on probability and BayesŠ—ç’ Îé theorem devised for Oxford University psychology students An Intuitive Explanation of Bayes Theorem by Eliezer S. Yudkowsky"
Binary Independence Model,Binary Independence Model,,,"The Binary Independence Model (BIM) is a probabilistic information retrieval technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible. == Definitions == The Binary Independence Assumption is that documents are binary vectors. That is, only the presence or absence of terms in documents are recorded. Terms are independently distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents. The representation is an ordered set of Boolean variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector d = (x1, ..., xm) where xt=1 if term t is present in the document d and xt=0 if its not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way. Independence signifies that terms in the document are considered independently from each other and no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the naive assumption of a Naive Bayes classifier, where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a Vector space model by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms. The probability P ( R | d , q ) { P(R|d,q)} that a document is relevant derives from the probability of relevance of the terms vector of that document P ( R | x , q ) { P(R|x,q)} . By using the Bayes rule we get: P ( R | x , q ) = P ( x | R , q ) Š—ç’ ’• P ( R | q ) P ( x | q ) { P(R|x,q)={{P(x|q)}}} where P ( x | R = 1 , q ) { P(x|R=1,q)} and P ( x | R = 0 , q ) { P(x|R=0,q)} are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that documents representation is x. The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used. P ( R = 1 | q ) { P(R=1|q)} and P ( R = 0 | q ) { P(R=0|q)} indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query q. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities. Since a document is either relevant or nonrelevant to a query we have that: P ( R = 1 | x , q ) + P ( R = 0 | x , q ) = 1 { P(R=1|x,q)+P(R=0|x,q)=1} === Query Terms Weighting === Given a binary query and the dot product as the similarity function between a document and a query, the problem is to assign weights to the terms in the query such that the retrieval effectiveness will be high. Let p i { p {i}} and q i { q {i}} be the probability that a relevant document and an irrelevant document has the ith term respectively. Yu and Salton, who first introduce BIM, propose that the weight of the ith term is an increasing function of Y i = p i Š—ç’ ’• ( 1 Š—ç’ ’« q i ) ( 1 Š—ç’ ’« p i ) Š—ç’ ’• q i { Y {i}={*(1-q {i})}{(1-p {i})*q {i}}}} . Thus, if Y i { Y {i}} is higher than Y j { Y {j}} , the weight of term i will be higher than that of term j. Yu and Salton showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. Robertson and Sp’‘ rck Jones later showed that if the ith term is assigned the weight of log Š—çÎ’ Y i { Y {i}} , then optimal retrieval effectiveness is obtained under the Binary Independence Assumption. The Binary Independence Model was introduced by Yu and Salton. The name Binary Independence Model was coined by Robertson and Sp’‘ rck Jones. == See also == Bag of words model == Further reading == Christopher D. Manning; Prabhakar Raghavan; Hinrich Sch’‘ tze (2008), Introduction to Information Retrieval, Cambridge University Press Stefan B’‘ ttcher; Charles L. A. Clarke; Gordon V. Cormack (2010), Information Retrieval: Implementing and Evaluating Search Engines, MIT Press == References =="
Binary classification,Binary classification,,,"Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a classification rule. Instancing a decision whether an item has or not some qualitative property, some specified characteristic, some typical binary classification tasks are: Medical testing to determine if a patient has certain disease or not - the classification property is the presence of the disease. A pass or fail test method or quality control in factories, i.e. deciding if a specification has or has not been met - a Go/no go classification. Information retrieval, namely deciding whether a page or an article should be in the result set of a search or not - the classification property is the relevance of the article, or the usefulness to the user. Binary classification is dichotomization applied to practical purposes, and therefore an important point is that in many practical binary classification problems, the two groups are not symmetric - rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present). Porting human discriminative abilities to scientific soundness and technical practice is far from trivial. == Statistical binary classification == Statistical classification is a problem studied in machine learning. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification. Some of the methods commonly used for binary classification are: Decision trees Random forests Bayesian networks Support vector machines Neural networks Logistic regression Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example random forests perform better than SVM classifiers for 3D point clouds. == Evaluation of binary classifiers == There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence - both types are useful, but they have very different properties. Given a classification of a specific data set, there are four basic data: the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These can be arranged into a 2’‘’•2 contingency table, with columns corresponding to actual value - condition positive (CP) or condition negative (CN) - and rows corresponding to classification value - test outcome positive or test outcome negative. There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form true positive row ratio or false negative column ratio, though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair - the other four numbers are the complements. The column ratios are True Positive Rate (TPR, aka Sensitivity or recall), with complement the False Negative Rate (FNR); and True Negative Rate (TNR, aka Specificity, SPC), with complement False Positive Rate (FPR). These are the proportion of the population with the condition (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence. The row ratios are Positive Predictive Value (PPV, aka precision), with complement the False Discovery Rate (FDR); and Negative Predictive Value (NPV), with complement the False Omission Rate (FOR). These are the proportion of the population with a given test result for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence. In diagnostic testing, the main ratios used are the true column ratios - True Positive Rate and True Negative Rate - where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) - Positive Predictive Value and True Positive Rate - where they are known as precision and recall. One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP’‘’•TN)/(FP’‘’•FN) = (TP/FN)/(FP/TN); this has a useful interpretation - as an odds ratio - and is prevalence-independent. There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youdens J statistic, the uncertainty coefficient, the Phi coefficient, and Cohens kappa. == Converting continuous values to binary == Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff. However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as positive with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as positive as the one of 52 mIU/ml. == See also == Examples of Bayesian inference Classification rule Detection theory Kernel methods Matthews correlation coefficient Multiclass classification Multi-label classification One-class classification Prosecutors fallacy Receiver operating characteristic Thresholding (image processing) Type I and type II errors Uncertainty coefficient, aka Proficiency Qualitative property == References == == Bibliography == Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book) John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 ([2] Kernel Methods Book) Bernhard Sch’‘Î lkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, MA, 2002. (Partly available on line: [3].) ISBN 0-262-19475-9"
Categorization,Categorization,,,"Categorization is the process in which ideas and objects are recognized, differentiated, and understood. Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a relationship between the subjects and objects of knowledge. Categorization is fundamental in language, prediction, inference, decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in computer programming. There are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified: Classical categorization Conceptual clustering Prototype theory == The classical view == Classical categorization first appears in the context of Western Philosophy in the work of Plato, who, in his Statesman dialogue, introduces the approach of grouping objects based on their similar properties. This approach was further explored and systematized by Aristotle in his Categories treatise, where he analyzes the differences between classes and objects. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as Is it an animal or vegetable?, How many feet does it have?, Does it have fur or feathers?, Can it fly?...), establishing this way the basis for natural taxonomy. The classical Aristotelian view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In analytic philosophy, these properties are assumed to establish the conditions which are both necessary and sufficient conditions to capture meaning. According to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories. == Conceptual clustering == Conceptual clustering is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, classes (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions. Conceptual clustering developed mainly during the 1980s, as a machine paradigm for unsupervised learning. It is distinguished from ordinary data clustering by generating a concept description for each generated category. Categorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, supervised learning, or concept learning. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, unsupervised learning, or data clustering. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the abstraction of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., exemplar models). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of generating a classification structure. Conceptual clustering is closely related to fuzzy set theory, in which objects may belong to one or more groups, in varying degrees of fitness. == Prototype theory == Since the research by Eleanor Rosch and George Lakoff in the 1970s, categorization can also be viewed as the process of grouping things based on prototypesŠ—ç’ ’–the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via embodiment. A cognitive approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members. Systems of categories are not objectively out there in the world but are rooted in peoples experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture. Categories form part of a hierarchical structure when applied to such subjects as taxonomy in biological classification: higher level: life-form level, middle level: generic or genus level, and lower level: the species level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision. Categories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the embodied level, with similarities leading to formulation of higher categories, and differences leading to differentiation within categories. == Miscategorization == Miscategorization can be a logical fallacy in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon characters that virtually all things have in common. == See also == == References == == External links == To Cognize is to Categorize: Cognition is Categorization Wikipedia Categories Visualizer Interdisciplinary Introduction to Categorization: Interview with Dvora Yanov (political sciences), Amie Thomasson (philosophy) and Thomas Serre (artificial intelligence) Category. Encyclop’‘ dia Britannica. 5 (11th ed.). 1911. pp. 508-510."
Center for Intelligent Information Retrieval,Center for Intelligent Information Retrieval,,,"Center for Intelligent Information Retrieval (CIIR) is a research center at the Department of Computer Science, University of Massachusetts Amherst. It is a leading research center in the area of Information Retrieval and Information Extraction. CIIR is led by Distinguished Professor W. Bruce Croft and Professor James Allan. == References =="
Classification of the sciences (Peirce),Classification of the sciences (Peirce),,,"The philosopher Charles Sanders Peirce (1839-1914) did considerable work over a period of years on the classification of sciences (including mathematics). His classifications are of interest both as a map for navigating his philosophy and as an accomplished polymaths survey of research in his time. Peirce himself was well grounded and produced work in many research fields, including logic, mathematics, statistics, philosophy, spectroscopy, gravimetry, geodesy, chemistry, and experimental psychology. == Classifications == Philosophers have done little work on classification of the sciences and mathematics since Peirces time. Noting Peirces important contribution, Denmarks Birger Hj’‘’ rland commented: There is not today (2005), to my knowledge, any organized research program about the classification of the sciences in any discipline or in any country. As Miksa (1998) writes, the interest for this question largely died in the beginning of the 20th century. It is not clear whether Hj’‘’ rland includes the classification of mathematics in that characterization. === Taxa === In 1902 and 1903 Peirce elaborates classifications of the sciences in: A Detailed Classification of the Sciences in Minute Logic (Feb.-Apr. 1902), Collected Papers of Charles Sanders Peirce (CP) v. 1, paragraphs 203-283 July 1902 application to the Carnegie institution (MS L75) An Outline Classification of the Sciences (CP 1.180-202) in his A Syllabus of Certain Topics in Logic (1903), wherein his classifications of the sciences take more or less their final form However, only in the Detailed Classification and the Carnegie application does he discuss the taxa which he used, which were inspired by the biological taxa of Louis Agassiz. === Sciences === In 1902, he divided science into Theoretical and Practical. Theoretical Science consisted of Science of Discovery and Science of Review, the latter of which he also called Synthetic Philosophy, a name taken from the title of the vast work, written over many years, by Herbert Spencer. Then, in 1903, he made it a three-way division: Science of Discovery, Science of Review, and Practical Science. In 1903 he characterized Science of Review as: ...arranging the results of discovery, beginning with digests, and going on to endeavor to form a philosophy of science. Such is the nature of Humboldts Cosmos, of Comtes Philosophie positive, and of Spencers Synthetic Philosophy. The classification of the sciences belongs to this department. Peirce had already for a while divided the Sciences of Discovery into: (1) Mathematics - draws necessary conclusions about hypothetical objects (2) Cenoscopy - philosophy about positive phenomena in general, such as confront a person at every waking moment, rather than special classes, and not settling theoretical issues by special experiences or experiments (3) Idioscopy - the special sciences, about special classes of positive phenomena, and settling theoretical issues by special experiences or experiments Thus Peirce ends up framing two fields each of which is philosophy in a sense: cenoscopic philosophy which precedes the special sciences, and synthetic philosophy (that is to say, science of review), which does take advantage of the results of all the sciences of discovery and develops, for instance, classifications of the sciences. Peirce opens his 1903 classification (the Syllabus classification) with a concise statement of method and purpose: This classification, which aims to base itself on the principal affinities of the objects classified, is concerned not with all possible sciences, nor with so many branches of knowledge, but with sciences in their present condition, as so many businesses of groups of living men. It borrows its idea from Comtes classification; namely, the idea that one science depends upon another for fundamental principles, but does not furnish such principles to that other. It turns out that in most cases the divisions are trichotomic; the First of the three members relating to universal elements or laws, the Second arranging classes of forms and seeking to bring them under universal laws, the Third going into the utmost detail, describing individual phenomena and endeavoring to explain them. But not all the divisions are of this character.... The following table is based mostly on Peirces 1903 classification, which was more or less the final form. But see after the table for discussion of his later remarks on the divisions of logic. ==== Logics divisions later ==== In a piece which the Collected Papers editors called Phaneroscopy and dated as 1906, Peirce wrote (CP 4.9): ...I extend logic to embrace all the necessary principles of semeiotic, and I recognize a logic of icons, and a logic of indices, as well as a logic of symbols; and in this last I recognize three divisions: Stecheotic (or stoicheiology), which I formerly called Speculative Grammar; Critic, which I formerly called Logic; and Methodeutic, which I formerly called Speculative Rhetoric Thus the three main 1903 departments of logic were now sub-departments of the study of the logic of symbols. In a letter to J. H. Kehler, printed in The New Elements of Mathematics v.3, p. 207 and dated 1911, Peirce wrote: I have now sketched my doctrine of Logical Critic, skipping a good deal. I recognize two other parts of Logic. One which may be called Analytic examines the nature of thought, not psychologically but simply to define what it is to doubt, to believe, to learn, etc., and then to base critic on these definitions is my real method, though in this letter I have taken the third branch of logic, Methodeutic, which shows how to conduct an inquiry. This is what the greater part of my life has been devoted to, though I base it upon Critic. There in 1911 Peirce does not mention the 1906 division into logics of icons, indices and symbols. Critic and Methodeutic appear, as in 1902 and 1903, as the second and third main departments of logic. Analytic is now the first department and the word Stechiology goes unused. He includes in Analytic the consideration of issues which, back in his 1902 Carnegie Institute application, he had discussed in sections on logic with headings such as Presuppositions of Logic and On the Logical Conception of Mind that he had placed before the sections on logics departments (stechiology, critic, and methodeutic). On the question of the relationship between Stechiology and the Analytic that seems to have replaced it, note that, in Draft D of Memoir 15 in his 1902 Carnegie Institute application, Peirce said that stechiology, also called grammatica speculativa, amounts to an Erkenntnisslehre, a theory of cognition, provided that that theory is stripped of matter irrelevant and inadmissible in philosophical logic, irrelevant matter such as all truths (for example, the association of ideas) established by psychologists, insofar as the special science of psychology depends on logic, not vice versa. In that same Carnegie Institute application as in many other places, Peirce treated belief and doubt as issues of philosophical logic apart from psychology. == Notes == == References == Peirce, C.S., 1902, An Outline Classification of the Sciences, The Collected Papers, vol. 1, pp. 203-283 (1902) Eprint, from projected book Minute Logic. Peirce, C.S., 1902, On the Classification of the Theoretic Sciences of Research, Manuscript L75.350-357, Arisbe Eprint, from Logic, Considered As Semeiotic, Manuscript L75, with draft sections labeled and interpolated into the final (submitted July 1902) version of the 1902 Carnegie Institute application, Joseph Ransdell, ed., Arisbe Eprint. Peirce, C.S., 1903, A Detailed Classification of the Sciences, The Collected Papers, vol. 1, pp. 180-202 (1903) Eprint and Eprint, from A Syllabus Of Certain Topics In Logic, the Essential Peirce, vol. 2, pp. 258-330. Vehkavaara, Tommi, 2001, The outline of Peirces classification of sciences (1902-1911), Eprint (PDF). (11.4 KiB). Vehkavaara, Tommi, 2003, Development of Peirces classification of sciences - three stages: 1889, 1898, 1903, Eprint (PDF). (19.4 KiB). == External links == Arisbe: The Peirce Gateway, Joseph Ransdell, ed. The Commens Dictionary of Peirces Terms, Mats Bergman & Sami Paavola, eds. C.S. PeirceŠ—ç’ Îés: Architectonic Philosophy, Albert Atkin, 2004, 2005, the Internet Encyclopedia of Philosophy. Speziali, Pierre (1973). Classification of the Sciences. In Wiener, Philip P. Dictionary of the History of Ideas. ISBN 0-684-13293-1. Retrieved 2009-12-02. Classification (of the sciences) (once there, scroll down) by Professor A. C. Armstrong, Jr. (Wesleyan University) in the Dictionary of Philosophy and Psychology, James Mark Baldwin, ed., 1901-1905. Peirces first classification of sciences (1889); Peirces classification of theoretical sciences and arts (1898); Peirces outline classification of sciences (1903). Compiled by Tommi Vehkavaara, 2003."
Co-occurrence,Co-occurrence,,,"Co-occurrence or cooccurrence is a linguistics term that can either mean concurrence / coincidence or, in a more specific sense, the above-chance frequent occurrence of two terms from a text corpus alongside each other in a certain order. Co-occurrence in this linguistic sense can be interpreted as an indicator of semantic proximity or an idiomatic expression. Corpus linguistics and its statistic analyses reveal patterns of co-occurrences within a language and enable to work out typical collocations for its lexical items. A co-occurrence restriction is identified when linguistic elements never occur together. Analysis of these restrictions can lead to discoveries about the structure and development of a language. == See also == Correlation Distributional hypothesis Statistical semantics Co-occurrence matrix Co-occurrence networks == References =="
Collaborative information seeking,Collaborative information seeking,,,"Collaborative information seeking (CIS) is a field of research that involves studying situations, motivations, and methods for people working in collaborative groups for information seeking projects, as well as building systems for supporting such activities. Such projects often involve information searching or information retrieval (IR), information gathering, and information sharing. Beyond that, CIS can extend to collaborative information synthesis and collaborative sense-making. == Background == Seeking for information is often considered a solo activity, but there are many situations that call for people working together for information seeking. Such situations are typically complex in nature, and involve working through several sessions exploring, evaluating, and gathering relevant information. Take for example, a couple going on a trip. They have the same goal, and in order to accomplish their goal, they need to seek out several kinds of information, including flights, hotels, and sightseeing. This may involve them working together over multiple sessions, exploring and collecting useful information, and collectively making decisions that help them move toward their common goal. It is a common knowledge that collaboration is either necessary or highly desired in many activities that are complex or difficult to deal with for an individual. Despite its natural appeal and situational necessity, collaboration in information seeking is an understudied domain. The nature of the available information and its role in our lives have changed significantly, but the methods and tools that are used to access and share that information in collaboration have remained largely unaltered. People still use general-purpose systems such as email and IM for doing CIS projects, and there is a lack of specialized tools and techniques to support CIS explicitly. There are also several models to explain information seeking and information behavior, but the areas of collaborative information seeking and collaborative information behavior remain understudied. A few specialized systems for supporting CIS have emerged in the recent past, but their usage and evaluations have underwhelmed. Despite such limitations, the field of CIS has been getting a lot of attention lately, and several promising theories and tools have come forth. A recent review of CIS related literature is written by Shah. Shah provides a comprehensive review of this field, including theories, models, systems, evaluation, and future research directions. Other books in this area include one by Morris and Teevan, as well as Fosters book on collaborative information behavior. == Theories == Depending upon what one includes or excludes while talking about CIS, we have many or hardly any theories. If we consider the past work on the groupware systems, many interesting insights can be obtained about people working on collaborative projects, the issues they face, and the guidelines for system designers. One of the notable works is by Grudin, who laid out eight design principles for developers of groupware systems. The discussion below is primarily based on some of the recent works in the field of computer supported cooperative work CSCW, collaborative IR, and CIS. === Definitions and terminology === The literature is filled with works that use terms such as collaborative information retrieval, social searching, concurrent search, collaborative exploratory search, co-browsing, collaborative information behavior, collaborative information synthesis, and collaborative information seeking, which are often used interchangeably. There are several definitions of such related or similar terms in the literature. For instance, Foster defined collaborative IR as the study of the systems and practices that enable individuals to collaborate during the seeking, searching, and retrieval of information. Shah defined CIS as a process of collaboratively seeking information that is defined explicitly among the participants, interactive, and mutually beneficial. While there is still a lack of a definition or a terminology that is universally accepted, but most agree that CIS is an active process, as opposed to collaborative filtering, where a system connects the users based on their passive involvement (e.g., buying similar products on Amazon). === Models of collaboration === Foley and Smeaton defined two key aspects of collaborative information seeking as division of labor and the sharing of knowledge. Division of labor allows collaborating searchers to tackle larger problems by reducing the duplication of effort (e.g., finding documents that ones collaborator has already discovered). The sharing of knowledge allows searchers to influence each others activities as they interact with the retrieval system in pursuit of their (often evolving) information need. This influence can occur in real time if the collaborative search system supports it, or it can occur in a turn-taking, asynchronous manner if that is how interaction is structured. Teevan et al. characterized two classes of collaboration, task-based vs. trait-based. Task-based collaboration corresponds to intentional collaboration; trait-based collaboration facilitates the sharing of knowledge through inferred similarity of information need. === Situations, motivations, and methods === One of the important issues to study in CIS is the instance, reason, and the methods behind a collaboration. For instance, Morris, using a survey with 204 knowledge workers at a large technology company found that people often like and want to collaborate, but they do not find specialized tools to help them in such endeavors. Some of the situations for doing collaborative information seeking in this survey were travel planning, shopping, and literature search. Shah, similarly, using personal interviews, identified three main reasons why people collaborate. Requirement/setup. Sometimes a group of people are forced to collaborate. Example includes a merger between two companies. Division of labor. Working together may help the participants to distribute the workload. Example includes a group of students working on a class project. Diversity of skills. Often people get together because they could not individually possess the required set of skills. Example includes co-authorship, where different authors bring different set of skills to the table. As far as the tools and/or methods for CIS are concerned, both Morris and Shah found that email is still the most used tool. Other popular methods are face-to-face meetings, IM, and phone or conference calls. In general, the choice of the method or tool for our respondents depended on their situation (co-located or remote), and objective (brainstorming or working on independent parts). === Space-time organization of CIS systems and methods === The classical way of organizing collaborative activities is based on two factors: location and time. Recently Hansen & Jarvelin and Golovchinsky, Pickens, & Back also classified approaches to collaborative IR using these two dimensions of space and time. See Browsing is a Collaborative Process, where the authors depict various library activities on these two dimensions. As we can see from this figure, the majority of collaborative activities in conventional libraries are co-located and synchronous, whereas collaborative activities relating to digital libraries are more remote and synchronous. Social information filtering, or collaborative filtering, as we saw earlier, is a process benefitting from other users actions in the past; thus, it falls under asynchronous and mostly remote domain. These days email also serves as a tool for doing asynchronous collaboration among users who are not co-located. Chat or IM (represented as internet in the figure) helps to carry out synchronous and remote collaboration. Rodden, similarly, presented a classification of CSCW systems using the form of interaction and the geographical nature of cooperative systems. Further, Rodden & Blair presented an important characteristic to all CSCW systems - control. According to the authors, two predominant control mechanisms have emerged within CSCW systems: speech act theory systems, and procedure based systems. These mechanisms are tightly coupled with the kind of control the system can support in a collaborative environment (discussed later). Often researchers also talk about other dimensions, such as intentionality and depth of mediation (system mediated or user mediated), while classifying various CIS systems. === Control, communication, and awareness === Three components specific to group-work or collaboration that are highly predominant in the CIS or CSCW literature are control, communication, and awareness. In this section key definitions and related works for these components will be highlighted. Understanding their roles can also help us address various design issues with CIS systems. ==== Control ==== Rodden identified the value of control in CSCW systems and listed a number of projects with their corresponding schemes for implementing for control. For instance, the COSMOS project had a formal structure to represent control in the system. They used roles to represent people or automatons, and rules to represent the flow and processes. Roles of the people could be supervisor, processor, or analyst. Rules could be a condition that a process needs to satisfy in order to start or finish. Due to such a structure seen in projects like COSMOS, Rodden classified these control systems as procedural based systems. ==== Communication ==== This is one of the most critical components of any collaboration. In fact, Rodden (1991) identified message or communication systems as the class of systems in CSCW that is most mature and most widely used. Since the focus here is on CIS systems that allow its participants to engage in an intentional and interactive collaboration, there must be a way for the participants to communicate with each other. What is interesting to note is that often, collaboration could begin by letting a group of users communicate with each other. For instance, Donath & Robertson presented a system that allows a user to know that others were currently viewing the same webpage and communicate with those people to initiate a possible collaboration or at least a co-browsing experience. Providing communication capabilities even in an environment that was not originally designed for carrying out collaboration is an interesting way of encouraging collaboration. ==== Awareness ==== Awareness, in the context of CSCW, has been defined as an understanding of the activities of others, which provides a context for your own activity. The following four kinds of awareness are often discussed and addressed in the CSCW literature: Group awareness. This kind of awareness includes providing information to each group member about the status and activities of the other collaborators at a given time. Workspace awareness. This refers to a common workspace that the group has where they can bring and discuss their findings, and create a common product. Contextual awareness. This type of awareness relates to the application domain, rather than the users. Here, we want to identify what content is useful for the group, and what the goals are for the current project. Peripheral awareness. This relates to the kind of information that has resulted from personal and the groups collective history, and should be kept separate from what a participant is currently viewing or doing. Shah and Marchionini studied awareness as provided by interface in collaborative information seeking. They found that one needs to provide right (not too little, not too much, and appropriate for the task at hand) kind of awareness to reduce the cost of coordination and maximize the benefits of collaboration. == Systems == A number of specialized systems have been developed back from the days of the groupware systems to todays Web 2.0 interfaces. A few such examples, in chronological order, are given below. === Ariadne === Twidale et al. developed Ariadne to support the collaborative learning of database browsing skills. In addition to enhancing the opportunities and effectiveness of the collaborative learning that already occurred, Ariadne was designed to provide the facilities that would allow collaborations to persist as people increasingly searched information remotely and had less opportunity for spontaneous face-to-face collaboration. Ariadne was developed in the days when Telnet-based access to library catalogs was a common practice. Building on top of this command-line interface, Ariadne could capture the usersŠ—ç’ Îé input and the databaseŠ—ç’ Îés output, and form them into a search history that consisted of a series of command-output pairs. Such a separation of capture and display allowed Ariadne to work with various forms of data capture methods. To support complex browsing processes in collaboration, Ariadne presented a visualization of the search process. This visualization consisted of thumbnails of screens, looking like playing cards, which represented command-output pairs. Any such card can be expanded to reveal its details. The horizontal axis on AriadneŠ—ç’ Îés display represented time, and the vertical axis showed information on the semantics of the action it represented: the top row for the top level menus, the middle row for specifying a search, and the bottom row for looking at particular book details. This visualization of the search process in Ariadne makes it possible to annotate, discuss with colleagues around the screen, and distribute to remote collaborators for asynchronous commenting easily and effectively. As we saw in the previous section, having access to oneŠ—ç’ Îés history as well as the history of oneŠ—ç’ Îés collaborators are very crucial to effective collaboration. Ariadne implements these requirements with the features that let one visualize, save, and share a search process. In fact, the authors found one of the advantages of search visualization was the ability to recap previous searching sessions easily in a multi-session exploratory searching. === SearchTogether === More recently, one of the collaborative information seeking tools that have caught a lot of attention is SearchTogether, developed by Morris and Horvitz. The design of this tool was motivated by a survey that the researchers did with 204 knowledge workers, in which they discovered the following. A majority of respondents wanted to collaborate while searching on the Web. The most common ways of collaborating in information seeking tasks are sending emails back and forth, using IM to exchange links and query terms, and using phone calls while looking at a Web browser. Some of the most popular Web searching tasks on which people like to collaborate are planning travels or social events, making expensive purchases, researching medical conditions, and looking for information related to a common project. Based on the survey responses, and the current and desired practices for collaborative search, the authors of SearchTogether identified three key features for supporting peopleŠ—ç’ Îés collaborative information behavior while searching on the Web: awareness, division of labor, and persistence. Let us look at how these three features are implemented. SearchTogether instantiates awareness in several ways, one of which is per-user query histories. This is done by showing each group memberŠ—ç’ Îés screen name, his/her photo and queries in the Š—ç’ ’Query AwarenessŠ—ç’ Î region. The access to the query histories is immediate and interactive, as clicking on a query brings back the results of that query from when it was executed. The authors identified query awareness as a very important feature in collaborative searching, which allows group members to not only share their query terms, but also learn better query formulation techniques from one another. Another component of SearchTogether that facilitates awareness is the display of page-specific metadata. This region includes several pieces of information about the displayed page, including group members who viewed the given page, and their comments and ratings. The authors claim that such visitation information can help one either choose to avoid a page already visited by someone in the group to reduce the duplication of efforts, or perhaps choose to visit such pages, as they provide a sign of promising leads as indicated by the presence of comments and/or ratings. Division of labor in SearchTogether is implemented in three ways: (1) Š—ç’ ’Split SearchŠ—ç’ Î allows one to split the search results among all online group members in a round-robin fashion, (2) Š—ç’ ’Multi-Engine SearchŠ—ç’ Î takes a query and runs it on n different search engines, where n is the number of online group members, (3) manual division of labor can be facilitated using integrated IM. Finally, the persistence feature in SearchTogether is instantiated by storing all the objects and actions, including IM conversations, query histories, recommendation queues, and page-specific metadata. Such data about all the group members are available to each member when he/she logs in. This allows one to easily carry a multi-session collaborative project. === Cerchiamo === Cerchiamo is a collaborative information seeking tool that explores issues related to algorithmic mediation of information seeking activities and how collaborators roles can be used to structure the user interface. Cerchiamo introduced the notion of algorithmic mediation, that is, the ability of the system to collect input asynchronously from multiple collaborating searchers, and to use these multiple streams of input to affect the information that is being retrieved and displayed to the searchers. Cerchiamo collected judgments of relevance from multiple collaborating searchers and used those judgments to create a ranked list of items that were potentially relevant to the information need. This algorithm prioritized items that were retrieved by multiple queries and that were retrieved by queries that also retrieved many other relevant documents. This rank fusion is just one way in which a search system that manages activities of multiple collaborating searchers can combine their inputs to generate results that are better than those produced by individuals working independently. Cerchiamo implemented two rolesŠ—ç’ ’–Prospector and MinerŠ—ç’ ’–that searchers could assume. Each role had an associated interface. The Prospector role/interface focused on running many queries and making a few judgments of relevance for each query to explore the information space. The Miner role/interface focused on making relevance judgments on a ranked list of items selected from items retrieved by all queries in the current session. This combination of roles allowed searchers to explore and exploit the information space, and led teams to discover more unique relevant documents than pairs of individuals working separately. === Coagmento === Coagmento (Latin for working together) is a new and unique system that allows a group of people work together for their information seeking tasks without leaving their browsers. Coagmento has been developed with a client-server architecture, where the client is implemented as a Firefox plug-in that helps multiple people working in collaboration to communicate, and search, share and organize information. The server component stores and provides all the objects and actions collected from the client. Due to this decoupling, Coagmento provides a flexible architecture that allows its users to be co-located or remote, working synchronously or asynchronously, and use different platforms. Coagmento includes a toolbar and a sidebar. The toolbar has several buttons that helps one collect information and be aware of the progress in a given collaboration. The toolbar has three major parts: Buttons for collecting information and making annotations. These buttons help one save or remove a webpage, make annotations on a webpage, and highlight and collect text snippets. Page-specific statistics. The middle portion of the toolbar shows various statistics, such as the number of views, annotations, and snippets, for the displayed page. A user can click on a given statistic and obtain more information. For instance, clicking on the number of snippets will bring up a window that shows all the snippets collected by the collaborators from the displayed page. Project-specific statistics. The last portion of the toolbar displays task/project name and various statistics, including number of pages visited and saved, about the current project. Clicking on that portion brings up the workspace where one can view all the collected objects (pages and snippets) brought in by the collaborators for that project. The sidebar features a chat window, under which there are three tabs with the history of search engine queries, saved pages and snippets. With each of these objects, the user who created or collected that object is shown. Anyone in the group can access an object by clicking on it. For instance, one can click on a query issued by anyone in the group to re-run that query and bring up the results in the main browser window. An Android (operating system) app for Coagmento can be found in the Android Market. === Cosme === Fernandez-Luna et al. introduce Cosme (COde Search MEeting) as a NetBeans IDE plug-in that enables remote team of software developers to collaborate in real time during source-code search sessions. The COSME design was motivated by early stadies of C. Foley, M. R. Morris, C. Shah, among others researchers, and by habits of software developers identified in a survey of 117 universities students and professors related with projects of software development, as well as to computer programmers of some companies. The five more commons collaborative search habits (or related to it) of the interviewees was: Revision of problems by the team in the workstation of one of them. Suggest addresses of Web pages that they have already visited previously, digital books stored in some FTP, or source files of a version control system. Send emails with algorithms or explanatory text. Division of search tasks among each member of the team for sharing the final result. Store relevant information in individual workstation. COSME is designed to enable either synchronous or asynchronous, but explicit remote collaboration among team developers with shared technical information needs. Its client user interface include a search panel that lets developers to specify queries, division of labor principle (possible combination include the use of different search engines, ranking fusion, and split algorithms), searching field (comments, source-code, class or methods declaration), and the collection type (source-code files or digital documentation). The sessions panel wraps the principal options to management the collaborative search sessions, which consists in a team of developers working together to satisfy their shared technical information needs. For example, a developer can use the embedded chat room to negotiate the creation of a collaborative search session, and show comments of the current and historical search results. The implementation of Cosme was based on CIRLab (Collaborative Information Retrieval Laboratory) instantiation, a groupware framework for CIS research and experimentation, Java as programming language, NetBeans IDE Platform as plug-in base, and Amenities (A MEthodology for aNalysis and desIgn of cooperaTIve systEmS) as software engineering methodology. == Open-source application frameworks and toolkits == CIS systems development is a complex task, which involves software technologies and Know-how in different areas such as distributed programming, information search and retrieval, collaboration among people, task coordination and many others according to the context. This situation is not ideal because it requires great programming efforts. Fortunately, some CIS application frameworks and toolkits are increasing their popularity since they have a high reusability impact for both developers and researchers, like Coagmento Collaboratory and DrakkarKeel. == Future research directions == Many interesting and important questions remain to be addressed in the field of CIS, including Why do people collaborate? Identifying their motivations can help us design better support for their specific needs. What additional tools are required to enhance existing methods of collaboration, given a specific domain? How to evaluate various aspects of collaborative information seeking, including system and user performance? How to measure the costs and benefits of collaboration? What are the information seeking situations in which collaboration is beneficial? When does it not pay off? How can we measure the performance of a collaborative group? How can we measure the contribution of an individual in a collaborative group? What sorts of retrieval algorithms can be used to combine input from multiple searchers? What kinds of algorithmic mediation can improve team performance? == See also == Collaboration Collaborative innovation network Collaborative learning Collaborative software Collaborative working environment Collaborative working system Computer-supported collaboration Computer-supported collaborative learning Human-computer information retrieval Information seeking Integrated collaboration environment == References == == External links == Workshop on the Evaluation on Collaborative Information Seeking and Retrieval (ECol) (October 2015) Workshop on Social and Collaborative Information Seeking (SCIS) (May 2015) IEEE Computer Special Issue on Collaborative Information Seeking (March 2014) YouTube video produced by IEEE Computer on Collaborative Information Seeking CIS group at Rutgers University Workshop on Collaboration, Recommendation and Social Search 2013 The third workshop on Collaborative Information Retrieval at CIKM 2011 The second workshop on Collaborative Information Retrieval at CSCW 2010 The first workshop on Collaborative Information Retrieval at JCDL 2008 Workshop on Collaborative Information Behavior CIS 2010 - First workshop on Collaborative Information Seeking at ACM Group 2010 CIS 2011 - Second workshop on Collaborative Information Seeking at ASIST 2011 CIS 2013 - Third workshop on Collaborative Information Seeking at ACM CSCW 2013 CSCW 2010 conference homepage CSCW 2011 conference homepage CSCW 2012 conference homepage CSCW 2014 conference homepage Coagmento Cultivating a Culture of Collaboration blog FXPALs blog on collaborative search CIR group at University of Washington CFP for IEEE Computer Special Issue on CIS"
Computational linguistics,Computational linguistics,,,"Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective. Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others. Computational linguistics has theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science, and applied computational linguistics focuses on the practical outcome of modeling human language use. The Association for Computational Linguistics defines computational linguistics as: ...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. == Origins == Computational linguistics is often grouped within the field of artificial intelligence, but actually was present before the development of artificial intelligence. Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English. Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language. Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families. Earlier methods such as lexicostatistics and glottochronology have been proven to be premature and inaccurate. However, recent interdisciplinary studies which borrow concepts from biological studies, especially gene mapping, have proved to produce more sophisticated analytical tools and more trustful results. When machine translation (also known as mechanical translation) failed to yield accurate translations right away, automated processing of human languages was recognized as far more complex than had originally been assumed. Computational linguistics was born as the name of the new field of study devoted to developing algorithms and software for intelligently processing language data. When artificial intelligence came into existence in the 1960s, the field of computational linguistics became that sub-division of artificial intelligence dealing with human-level comprehension and production of natural languages. In order to translate one language into another, it was observed that one had to understand the grammar of both languages, including both morphology (the grammar of word forms) and syntax (the grammar of sentence structure). In order to understand syntax, one had to also understand the semantics and the lexicon (or vocabulary), and even something of the pragmatics of language use. Thus, what started as an effort to translate between languages evolved into an entire discipline devoted to understanding how to represent and process natural languages using computers. Nowadays research within the scope of computational linguistics is done at computational linguistics departments, computational linguistics laboratories, computer science departments, and linguistics departments. Some research in the field of computational linguistics aims to create working speech or text processing systems while others aim to create a system allowing human-machine interaction. Programs meant for human-machine communication are called conversational agents. == Approaches == Just as computational linguistics can be performed by experts in a variety of fields and through a wide assortment of departments, so too can the research fields broach a diverse range of topics. The following sections discuss some of the literature available across the entire field broken into four main area of discourse: developmental linguistics, structural linguistics, linguistic production, and linguistic comprehension. === Developmental approaches === Language is a cognitive skill which develops throughout the life of an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence. This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a simple hypothesis testing procedure for information as complex as language, and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual. Attempts have been made to model the developmental process of language acquisition in children from a computational angle, leading to both statistical grammars and connectionist models. Work in this realm has also been proposed as a method to explain the evolution of language through history. Using models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span. This was simultaneously posed as a reason for the long developmental period of human children. Both conclusions were drawn because of the strength of the neural network which the project created. The ability of infants to develop language has also been modeled using robots in order to test linguistic theories. Enabled to learn as children might, a model was created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development. It is important to note that this information could only have been empirically tested using a computational approach. As our understanding of the linguistic development of an individual within a lifetime is continually improved using neural networks and learning robotic systems, it is also important to keep in mind that languages themselves change and develop through time. Computational approaches to understanding this phenomenon have unearthed very interesting information. Using the Price Equation and P’‘ lya urn dynamics, researchers have created a system which not only predicts future linguistic evolution, but also gives insight into the evolutionary history of modern-day languages. This modeling effort achieved, through computational linguistics, what would otherwise have been impossible. It is clear that the understanding of linguistic development in humans as well as throughout evolutionary time has been fantastically improved because of advances in computational linguistics. The ability to model and modify systems at will affords science an ethical method of testing hypotheses that would otherwise be intractable. === Structural approaches === In order to create better computational models of language, an understanding of languageŠ—ç’ Îés structure is crucial. To this end, the English language has been meticulously studied using computational approaches to better understand how the language works on a structural level. One of the most important pieces of being able to study linguistic structure is the availability of large linguistic corpora, or samples. This grants computational linguists the raw data necessary to run their models and gain a better understanding of the underlying structures present in the vast amount of data which is contained in any single language. One of the most cited English linguistic corpora is the Penn Treebank. Derived from widely-different sources, such as IBM computer manuals and transcribed telephone conversations, this corpus contains over 4.5 million words of American English. This corpus has been primarily annotated using part-of-speech tagging and syntactic bracketing and has yielded substantial empirical observations related to language structure. Theoretical approaches to the structure of languages have also been developed. These works allow computational linguistics to have a framework within which to work out hypotheses that will further the understanding of the language in a myriad of ways. One of the original theoretical theses on internalization of grammar and structure of language proposed two types of models. In these models, rules or patterns learned increase in strength with the frequency of their encounter. The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck? Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field. Structural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances. For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances. This technique is a strong tool for further probing the structure of human discourse. Without the computational approach to this question, the vastly complex information present in discourse data would have remained inaccessible to scientists. Information regarding the structural data of a language is available for English as well as other languages, such as Japanese. Using computational methods, Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length. Though the exact cause of this lognormality remains unknown, it is precisely this sort of intriguing information which computational linguistics is designed to uncover. This information could lead to further important discoveries regarding the underlying structure of Japanese, and could have any number of effects on the understanding of Japanese as a language. Computational linguistics allows for very exciting additions to the scientific knowledge base to happen quickly and with very little room for doubt. Without a computational approach to the structure of linguistic data, much of the information that is available now would still be hidden under the vastness of data within any single language. Computational linguistics allows scientists to parse huge amounts of data reliably and efficiently, creating the possibility for discoveries unlike any seen in most other approaches. === Production approaches === The production of language is equally as complex in the information it provides and the necessary skills which a fluent producer must have. That is to say, comprehension is only half the problem of communication. The other half is how a system produces language, and computational linguistics has made some very interesting discoveries in this area. In a now famous paper published in 1950 Alan Turing proposed the possibility that machines might one day have the ability to think. As a thought experiment for what might define the concept of thought in machines, he proposed an imitation test in which a human subject has two text-only conversations, one with a fellow human and another with a machine attempting to respond like a human. Turing proposes that if the subject cannot tell the difference between the human and the machine, it may be concluded that the machine is capable of thought. Today this test is known as the Turing test and it remains an influential idea in the area of artificial intelligence. One of the earliest and best known examples of a computer program designed to converse naturally with humans is the ELIZA program developed by Joseph Weizenbaum at MIT in 1966. The program emulated a Rogerian psychotherapist when responding to written statements and questions posed by a user. It appeared capable of understanding what was said to it and responding intelligently, but in truth it simply followed a pattern matching routine that relied on only understanding a few keywords in each sentence. Its responses were generated by recombining the unknown parts of the sentence around properly translated versions of the known words. For example, in the phrase It seems that you hate me ELIZA understands you and me which matches the general pattern you [some words] me, allowing ELIZA to update the words you and me to I and you and replying What makes you think I hate you?. In this example ELIZA has no understanding of the word hate, but it is not required for a logical response in the context of this type of psychotherapy. Some projects are still trying to solve the problem which first started computational linguistics off as its own field in the first place. However, the methods have become more refined and clever, and consequently the results generated by computational linguists have become more enlightening. In an effort to improve computer translation, several models have been compared, including hidden Markov models, smoothing techniques, and the specific refinements of those to apply them to verb translation. The model which was found to produce the most natural translations of German and French words was a refined alignment model with a first-order dependence and a fertility model[16]. They also provide efficient training algorithms for the models presented, which can give other scientists the ability to improve further on their results. This type of work is specific to computational linguistics, and has applications which could vastly improve understanding of how language is produced and comprehended by computers. Work has also been done in making computers produce language in a more naturalistic manner. Using linguistic input from humans, algorithms have been constructed which are able to modify a systems style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality. This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural. ==== Text-based interactive approach ==== Many of the earliest and simplest models of human-computer interaction, such as ELIZA for example, involve a text-based input from the user to generate a response from the computer. By this method, words typed by a user trigger the computer to recognize specific patterns and reply accordingly, through a process known as keyword spotting. ==== Speech-based interactive approach ==== Recent technologies have placed more of an emphasis on speech-based interactive systems. These systems, such as Siri of the iOS operating system, operate on a similar pattern-recognizing technique as that of text-based systems, but with the former, the user input is conducted through speech recognition. This branch of linguistics involves the processing of the users speech as sound waves and the interpreting of the acoustics and language patterns in order for the computer to recognize the input. === Comprehension approaches === Much of the focus of modern computational linguistics is on comprehension. With the proliferation of the internet and the abundance of easily accessible written human language, the ability to create a program capable of understanding human language would have many broad and exciting possibilities, including improved search engines, automated customer service, and online education. Early work in comprehension included applying Bayesian statistics to the task of optical character recognition, as illustrated by Bledsoe and Browing in 1959 in which a large dictionary of possible letters were generated by learning from example letters and then the probability that any one of those learned examples matched the new input was combined to make a final decision. Other attempts at applying Bayesian statistics to language analysis included the work of Mosteller and Wallace (1963) in which an analysis of the words used in The Federalist Papers was used to attempt to determine their authorship (concluding that Madison most likely authored the majority of the papers). In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule governed environment. The primary language parsing program in this project was called SHRDLU, which was capable of carrying out a somewhat natural conversation with the user giving it commands, but only within the scope of the toy environment designed for the task. This environment consisted of different shaped and colored blocks, and SHRDLU was capable of interpreting commands such as Find a block which is taller than the one you are holding and put it into the box. and asking questions such as I dont understand which pyramid you mean. in response to the users input. While impressive, this kind of natural language processing has proven much more difficult outside the limited scope of the toy environment. Similarly a project developed by NASA called LUNAR was designed to provide answers to naturally written questions about the geological analysis of lunar rocks returned by the Apollo missions. These kinds of problems are referred to as question answering. Initial attempts at understanding spoken language were based on work done in the 1960s and 1970s in signal modeling where an unknown signal is analyzed to look for patterns and to make predictions based on its history. An initial and somewhat successful approach to applying this kind of signal modeling to language was achieved with the use of hidden Markov models as detailed by Rabiner in 1989. This approach attempts to determine probabilities for the arbitrary number of models that could be being used in generating speech as well as modeling the probabilities for various words generated from each of these possible models. Similar approaches were employed in early speech recognition attempts starting in the late 70s at IBM using word/part-of-speech pair probabilities. More recently these kinds of statistical approaches have been applied to more difficult tasks such as topic identification using Bayesian parameter estimation to infer topic probabilities in text documents. == Applications == Modern computational linguistics is often a combination of studies in computer science and programming, math, particularly statistics, language structures, and natural language processing. Combined, these fields most often lead to the development of systems that can recognize speech and perform some task based on that speech. Examples include speech recognition software, such as Apples Siri feature, spellcheck tools, speech synthesis programs, which are often used to demonstrate pronunciation or help the disabled, and machine translation programs and websites, such as Google Translate and Word Reference. Computational linguistics can be especially helpful in situations involving social media and the Internet. For example, filters in chatrooms or on website searches require computational linguistics. Chat operators often use filters to identify certain words or phrases and deem them inappropriate so that users cannot submit them. Another example of using filters is on websites. Schools use filters so that websites with certain keywords are blocked from children to view. There are also many programs in which parents use Parental controls to put content filters in place. Computational linguists can also develop programs that group and organize content through Social media mining. An example of this is Twitter, in which programs can group tweets by subject or keywords. Computational linguistics is also used for document retrieval and clustering. When you do an online search, documents and websites are retrieved based on the frequency of unique labels related to what you typed into a search engine. For instance, if you search red, large, four-wheeled vehicle, with the intention of finding pictures of a red truck, the search engine will still find the information desired by matching words such as four-wheeled with car. == Subfields == Computational linguistics can be divided into major areas depending upon the medium of the language being processed, whether spoken or textual; and upon the task being performed, whether analyzing language (recognition) or synthesizing language (generation). Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers. Parsing and generation are sub-divisions of computational linguistics dealing respectively with taking language apart and putting it together. Machine translation remains the sub-division of computational linguistics dealing with having computers translate between languages. The possibility of automatic language translation, however, has yet to be realized and remains a notorious branch of computational linguistics. Some of the areas of research that are studied by computational linguistics include: Computational complexity of natural language, largely modeled on automata theory, with the application of context-sensitive grammar and linearly bounded Turing machines. Computational semantics comprises defining suitable logics for linguistic meaning representation, automatically constructing them and reasoning with them Computer-aided corpus linguistics, which has been used since the 1970s as a way to make detailed advances in the field of discourse analysis Design of parsers or chunkers for natural languages Design of taggers like POS-taggers (part-of-speech taggers) Machine translation as one of the earliest and most difficult applications of computational linguistics draws on many subfields. Simulation and study of language evolution in historical linguistics/glottochronology. == Legacy == The subject of computational linguistics has had a recurring impact on popular culture: The 1983 film WarGames features a young computer hacker who interacts with an artificially intelligent supercomputer. A 1997 film, Conceiving Ada, focuses on Ada Lovelace, considered one of the first computer scientists, as well as themes of computational linguistics. Her, a 2013 film, depicts a mans interactions with the worlds first artificially intelligent operating system. The 2014 film The Imitation Game follows the life of computer scientist Alan Turing, developer of the Turing Test. The 2015 film Ex Machina centers around human interaction with artificial intelligence. == See also == == References == == External links == Association for Computational Linguistics (ACL) ACL Anthology of research papers ACL Wiki for Computational Linguistics CICLing annual conferences on Computational Linguistics Computational Linguistics - Applications workshop Free online introductory book on Computational Linguistics at the Wayback Machine (archived January 25, 2008) Language Technology World Resources for Text, Speech and Language Processing The Research Group in Computational Linguistics"
Computer data storage,Computer data storage,,,"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers. The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) are referred to as memory, while slower persistent technologies are referred to as storage; however, memory is sometimes also used when referring to persistent storage. In the Von Neumann architecture, the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data. == Functionality == Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines. == Data organization and representation == A modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 1 or 0. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate the binary representation of the piece of information, or simply data. For example, the complete works of Shakespeare, about 1250 pages in print, can be stored in about five megabytes (40 million bits) with one byte per character. Data is encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g., character encodings like ASCII, image encodings like JPEG, video encodings like MPEG-4). By adding bits to each encoded unit, redundancy allows the computer to both detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or physical bit fatigue, loss of the physical bit in storage its ability to maintain distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g., due to random radiation) is typically corrected upon detection. A bit, or a group of malfunctioning physical bits (not always the specific defective bit is known; group definition depends on specific storage device) is typically automatically fenced-out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried. Data compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (compress) and reconstruct the original string (decompress) when needed. This utilizes substantially less storage (tens of percents) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not. For security reasons certain types of data (e.g., credit-card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots. == Hierarchy of storage == Generally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary and off-line storage is also guided by cost per bit. In contemporary usage, memory is usually semiconductor storage read-write random-access memory, typically DRAM (dynamic RAM) or other forms of fast but temporary storage. Storage consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down). Historically, memory has been called core memory, main memory, real storage or internal memory. Meanwhile, non-volatile storage devices have been referred to as secondary storage, external memory or auxiliary/peripheral storage. === Primary storage === Primary storage (also known as main memory or internal memory), often referred to simply as memory, is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner. Historically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive. This led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. (The particular types of RAM used for primary storage are also volatile, i.e. they lose the information when not powered). As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM: Processor registers are located inside the processor. Each register typically holds a word of data (often 32 or 64 bits). CPU instructions instruct the arithmetic logic unit to perform various calculations or other operations on this data (or with the help of it). Registers are the fastest of all forms of computer data storage. Processor cache is an intermediate stage between ultra-fast registers and much slower main memory. It was introduced solely to improve the performance of computers. Most actively used information in the main memory is just duplicated in the cache memory, which is faster, but of much lesser capacity. On the other hand, main memory is much slower, but has a much greater storage capacity than processor registers. Multi-level hierarchical cache setup is also commonly usedŠ—ç’ ’–primary cache being smallest, fastest and located inside the processor; secondary cache being somewhat larger and slower. Main memory is directly or indirectly connected to the central processing unit via a memory bus. It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks. As the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile secondary storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of random access). Many types of ROM are not literally read only, as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly. Recently, primary storage and secondary storage in some uses refer to what was historically called, respectively, secondary storage and tertiary storage. === Secondary storage === Secondary storage (also known as external memory or auxiliary storage), differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfers the desired data using intermediate area in primary storage. Secondary storage does not lose the data when the device is powered downŠ—ç’ ’–it is non-volatile. Per unit, it is typically also two orders of magnitude less expensive than primary storage. Modern computer systems typically have two orders of magnitude more secondary storage than primary storage and data are kept for a longer time there. In modern computers, hard disk drives are usually used as secondary storage. The time taken to access a given byte of information stored on a hard disk is typically a few thousandths of a second, or milliseconds. By contrast, the time taken to access a given byte of information stored in random-access memory is measured in billionths of a second, or nanoseconds. This illustrates the significant access-time difference which distinguishes solid-state memory from rotating magnetic storage devices: hard disks are typically about a million times slower than memory. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. With disk drives, once the disk read/write head reaches the proper placement and the data of interest rotates under it, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. When data reside on disk, blocking access to hide latency offers an opportunity to design efficient external memory algorithms. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based upon sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel in order to increase the bandwidth between primary and secondary memory. Some other examples of secondary storage technologies are flash memory (e.g. USB flash drives or keys), floppy disks, magnetic tape, paper tape, punched cards, standalone RAM disks, and Iomega Zip drives. The secondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, providing also additional information (called metadata) describing the owner of a certain file, the access time, the access permissions, and other information. Most computer operating systems use the concept of virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to secondary storage devices (to a swap file or page file), retrieving them later when they are needed. As more of these retrievals from slower secondary storage are necessary, the more the overall system performance is degraded. === Tertiary storage === Tertiary storage or tertiary memory provides a third level of storage. Typically, it involves a robotic mechanism which will mount (insert) and dismount removable mass storage media into a storage device according to the systems demands; this data is often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5-60 seconds vs. 1-10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes. When a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library. Tertiary storage is also known as nearline storage because it is near to online. The formal distinction between online, nearline, and offline storage is: Online storage is immediately available for I/O. Nearline storage is not immediately available, but can be made online quickly without human intervention. Offline storage is not immediately available, and requires some human intervention to become online. For example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage. === Off-line storage === Off-line storage is a computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction. Off-line storage is used to transfer information, since the detached medium can be easily physically transported. Additionally, in case a disaster, for example a fire, destroys the original data, a medium in a remote location will probably be unaffected, enabling disaster recovery. Off-line storage increases general information security, since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage. In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are most popular, and to much lesser extent removable hard disk drives. In enterprise uses, magnetic tape is predominant. Older examples are floppy disks, Zip disks, or punched cards. == Characteristics of storage == Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance. === Volatility === Non-volatile memory retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information. Volatile memory requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory. Dynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed, otherwise it would vanish. Static random-access memory is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost. An uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes. === Mutability === Read/write storage or mutable storage Allows information to be overwritten at any time. A computer without some amount of read/write storage for primary storage purposes would be useless for many tasks. Modern computers typically use read/write storage also for secondary storage. Read only storage Retains the information stored at the time of manufacture, and write once storage (Write Once Read Many) allows the information to be written only once at some point after manufacture. These are called immutable storage. Immutable storage is used for tertiary and off-line storage. Examples include CD-ROM and CD-R. Slow write, fast read storage Read/write storage which allows information to be overwritten multiple times, but with the write operation being much slower than the read operation. Examples include CD-RW and swayne memory === Accessibility === Random access Any location in storage can be accessed at any moment in approximately the same amount of time. Such characteristic is well suited for primary and secondary storage. Most semiconductor memories and disk drives provide random access. Sequential access The accessing of pieces of information will be in a serial order, one after the other; therefore the time to access a particular piece of information depends upon which piece of information was last accessed. Such characteristic is typical of off-line storage. === Addressability === Location-addressable Each individually accessible unit of information in storage is selected with its numerical memory address. In modern computers, location-addressable storage usually limits to primary storage, accessed internally by computer programs, since location-addressability is very efficient, but burdensome for humans. File addressable Information is divided into files of variable length, and a particular file is selected with human-readable directory and file names. The underlying device is still location-addressable, but the operating system of a computer provides the file system abstraction to make the operation more understandable. In modern computers, secondary, tertiary and off-line storage use file systems. Content-addressable Each individually accessible unit of information is selected based on the basis of (part of) the contents stored there. Content-addressable storage can be implemented using software (computer program) or hardware (computer device), with hardware being faster but more expensive option. Hardware content addressable memory is often used in a computers CPU cache. === Capacity === Raw capacity The total amount of stored information that a storage device or medium can hold. It is expressed as a quantity of bits or bytes (e.g. 10.4 megabytes). Memory storage density The compactness of stored information. It is the storage capacity of a medium divided with a unit of length, area or volume (e.g. 1.2 megabytes per square inch). === Performance === Latency The time it takes to access a particular location in storage. The relevant unit of measurement is typically nanosecond for primary storage, millisecond for secondary storage, and second for tertiary storage. It may make sense to separate read latency and write latency (especially for non-volatile memory) and in case of sequential access storage, minimum, maximum and average latency. Throughput The rate at which information can be read from or written to the storage. In computer data storage, throughput is usually expressed in terms of megabytes per second (MB/s), though bit rate may also be used. As with latency, read rate and write rate may need to be differentiated. Also accessing media sequentially, as opposed to randomly, typically yields maximum throughput. Granularity The size of the largest chunk of data that can be efficiently accessed as a single unit, e.g. without introducing additional latency. Reliability The probability of spontaneous bit value change under various conditions, or overall failure rate. === Energy use === Storage devices that reduce fan usage, automatically shut-down during inactivity, and low power hard drives can reduce energy consumption by 90 percent. 2.5 inch hard disk drives often consume less power than larger ones. Low capacity solid-state drives have no moving parts and consume less power than hard disks. Also, memory may use more power than hard disks. Large caches, which are used to avoid hitting memory wall, may also consume a large amount of power. == Storage media == As of 2011, the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies have been used in the past or are proposed for development. === Semiconductor === Semiconductor memory uses semiconductor-based integrated circuits to store information. A semiconductor memory chip may contain millions of tiny transistors or capacitors. Both volatile and non-volatile forms of semiconductor memory exist. In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor memory or dynamic random-access memory. Since the turn of the century, a type of non-volatile semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them. As early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD. === Magnetic === Magnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is non-volatile. The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms: Magnetic disk Floppy disk, used for off-line storage Hard disk drive, used for secondary storage Magnetic tape, used for tertiary and off-line storage Carousel memory (magnetic rolls) In early computers, magnetic storage was also used as: Primary storage in a form of magnetic memory, or core memory, core rope memory, thin-film memory and/or twistor memory. Tertiary (e.g. NCR CRAM) or off line storage in the form of magnetic cards. Magnetic tape was then often used for secondary storage. === Optical === Optical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is non-volatile. The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are currently in common use: CD, CD-ROM, DVD, BD-ROM: Read only storage, used for mass distribution of digital information (music, video, computer programs) CD-R, DVD-R, DVD+R, BD-R: Write once storage, used for tertiary and off-line storage CD-RW, DVD-RW, DVD+RW, DVD-RAM, BD-RE: Slow write, fast read storage, used for tertiary and off-line storage Ultra Density Optical or UDO is similar in capacity to BD-R or BD-RE and is slow write, fast read storage used for tertiary and off-line storage. Magneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is non-volatile, sequential access, slow write, fast read storage used for tertiary and off-line storage. 3D optical data storage has also been proposed. Light induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage. === Paper === Paper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. A few technologies allow people to make marks on paper that are easily read by machineŠ—ç’ ’–these are widely used for tabulating votes and grading standardized tests. Barcodes made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it. === Other storage media or substrates === Vacuum tube memory A Williams tube used a cathode ray tube, and a Selectron tube used a large vacuum tube to store information. These primary storage devices were short-lived in the market, since Williams tube was unreliable and the Selectron tube was expensive. Electro-acoustic memory Delay line memory used sound waves in a substance such as mercury to store information. Delay line memory was dynamic volatile, cycle sequential read/write storage, and was used for primary storage. Optical tape is a medium for optical storage generally consisting of a long and narrow strip of plastic onto which patterns can be written and from which the patterns can be read back. It shares some technologies with cinema film stock and optical discs, but is compatible with neither. The motivation behind developing this technology was the possibility of far greater storage capacities than either magnetic tape or optical discs. Phase-change memory uses different mechanical phases of phase-change material to store information in an X-Y addressable matrix, and reads the information by observing the varying electrical resistance of the material. Phase-change memory would be non-volatile, random-access read/write storage, and might be used for primary, secondary and off-line storage. Most rewritable and many write once optical disks already use phase change material to store information. Holographic data storage stores information optically inside crystals or photopolymers. Holographic storage can utilize the whole volume of the storage medium, unlike optical disc storage which is limited to a small number of surface layers. Holographic storage would be non-volatile, sequential access, and either write once or read/write storage. It might be used for secondary and off-line storage. See Holographic Versatile Disc (HVD). Molecular memory stores information in polymer that can store electric charge. Molecular memory might be especially suited for primary storage. The theoretical storage capacity of molecular memory is 10 terabits per square inch. Magnetic photoconductors store magnetic information which can be modified by low-light illumination. DNA stores information in DNA nucleotides. It was first done in 2012 when researchers achieved a rate of 1.28 petabytes per gram of DNA. In March 2017 scientists reported that a new algorithm called a DNA fountain achieved 85% of the theoretical limit, at 215 petabytes per gram of DNA. == Related technologies == === Redundancy === While a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices: Device mirroring (replication) - A common solution to the problem is constantly maintaining an identical copy of device content on another device (typically of a same type). The downside is that this doubles the storage, and both devices (copies) need to be updated simultaneously with some overhead and possibly some delays. The upside is possible concurrent read of a same data group by two independent processes, which increases performance. When one of the replicated devices is detected to be defective, the other copy is still operational, and is being utilized to generate a new copy on another device (usually available operational in a pool of stand-by devices for this purpose). Redundant array of independent disks (RAID) - This method generalizes the device mirroring above by allowing one device in a group of N devices to fail and be replaced with the content restored (Device mirroring is RAID with N=2). RAID groups of N=5 or N=6 are common. N>2 saves storage, when comparing with N=2, at the cost of more processing during both regular operation (with often reduced performance) and defective device replacement. Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in a same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle also recovery from disasters (see disaster recovery above). === Network connectivity === A secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree. Direct-attached storage (DAS) is a traditional mass storage, that does not use any network. This is still a most popular approach. This retronym was coined recently, together with NAS and SAN. Network-attached storage (NAS) is mass storage attached to a computer which another computer can access at file level over a local area network, a private wide area network, or in the case of online file storage, over the Internet. NAS is commonly associated with the NFS and CIFS/SMB protocols. Storage area network (SAN) is a specialized network, that provides other computers with storage capacity. The crucial difference between NAS and SAN is the former presents and manages file systems to client computers, whilst the latter provides access at block-addressing (raw) level, leaving it to attaching systems to manage data or file systems within the provided capacity. SAN is commonly associated with Fibre Channel networks. === Robotic storage === Large quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. Smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers. Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots. R"
Conference on Information and Knowledge Management,Conference on Information and Knowledge Management,,,"The ACM Conference on Information and Knowledge Management (CIKM, pronounced /’å’ sik¬êÎém/) is an annual computer science research conference dedicated to information management (IM) and knowledge management (KM). Since the first event in 1992, the conference has evolved into one of the major forums for research on database management, information retrieval, and knowledge management. The conference is noted for its interdisciplinarity, as it brings together communities that otherwise often publish at separate venues. Recent editions have attracted well beyond 500 participants. In addition to the main research program, the conference also features a number of workshops, tutorials, and industry presentations. For many years, the conference was held in the USA. Since 2005, venues in other countries have been selected as well. Locations include: 1992: Baltimore, Maryland, USA 1993: Washington, D.C., USA 1994: Gaithersburg, Maryland, USA 1995: Baltimore, Maryland, USA 1996: Rockville, Maryland, USA 1997: Las Vegas, Nevada, USA 1998: Bethesda, Maryland, USA 1999: Kansas City, Missouri, USA 2000: Washington, D.C., USA 2001: Atlanta, Georgia, USA 2002: McLean, Virginia, USA 2003: New Orleans, Louisiana, USA 2004: Washington, D.C., USA 2005: Bremen, Germany 2006: Arlington, Virginia, USA 2007: Lisbon, Portugal 2008: Napa Valley, California, USA 2009: Hong Kong, China 2010: Toronto, Ontario, Canada 2011: Glasgow, Scotland, UK == See also == SIGIR Conference == References == == External links == Official website"
Confusion matrix,Confusion matrix,,,"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another). It is a special kind of contingency table, with two dimensions (actual and predicted), and identical sets of classes in both dimensions (each combination of dimension and class is a variable in the contingency table). == Example == If a classification system has been trained to distinguish between cats, dogs and rabbits, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 27 animals Š—ç’ ’– 8 cats, 6 dogs, and 13 rabbits, the resulting confusion matrix could look like the table below: == Table of confusion == In predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct guesses (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the number of samples in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, the classifier could easily be biased into classifying all the samples as cats. The overall accuracy would be 95%, but in practice the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class. Assuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be: The final table of confusion would contain the average values for all classes combined. Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2’‘’•2 confusion matrix, as follows: == References == == External links == Theory about the confusion matrix GM-RKB Confusion Matrix concept page"
Controlled vocabulary,Controlled vocabulary,,,"Controlled vocabularies provide a way to organize knowledge for subsequent retrieval. They are used in subject indexing schemes, subject headings, thesauri, taxonomies and other forms of knowledge organization systems. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction. == In library and information science == In library and information science controlled vocabulary is a carefully selected list of words and phrases, which are used to tag units of information (document or work) so that they may be more easily retrieved by a search. Controlled vocabularies solve the problems of homographs, synonyms and polysemes by a bijection between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency. For example, in the Library of Congress Subject Headings (a subject heading system that uses a controlled vocabulary), authorized termsŠ—ç’ ’–subject headings in this caseŠ—ç’ ’–have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (cockroach versus Periplaneta americana), and choices between synonyms (automobile versus car), among other difficult issues. Choices of authorized terms are based on the principles of user warrant (what terms users are likely to use), literary warrant (what terms are generally used in the literature and documents), and structural warrant (terms chosen by considering the structure, scope of the controlled vocabulary). Controlled vocabularies also typically handle the problem of homographs, with qualifiers. For example, the term pool has to be qualified to refer to either swimming pool or the game pool to ensure that each authorized term or heading refers to only one concept. There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences. Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not. For example, the Library of Congress Subject Heading itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term Broader term and Narrow term. The terms are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the documents text. Well known subject heading systems include the Library of Congress system, MeSH, and Sears. Well known thesauri include the Art and Architecture Thesaurus and the ERIC Thesaurus. Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue. Controlled vocabulary elements (terms/phrases) employed as tags, to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as metadata. == Indexing languages == There are three main types of indexing languages. Controlled indexing language - only approved terms can be used by the indexer to describe the document Natural language indexing language - any term from the document in question can be used to describe the document Free indexing language - any term (not only from the document) can be used to describe the document When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document. In recent years free text search as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is indexed). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors. Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce irrelevant items in the retrieval list. These irrelevant items (false positives) are often caused by the inherent ambiguity of natural language. Take the English word football for example. Football is the name given to a number of different team sports. Worldwide the most popular of these team sports is association football, which also happens to be called soccer in several countries. The word football is also applied to rugby football (rugby union and rugby league), American football, Australian rules football, Gaelic football, and Canadian football. A search for football therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by tagging the documents in such a way that the ambiguities are eliminated. Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually relevant to the search topic). In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you dont need to worry about searching for other terms that might be synonyms of that term. However, a controlled vocabulary search may also lead to unsatisfactory recall, in that it will fail to retrieve some documents that are actually relevant to the search question. This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer. Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with football because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless. On the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision. Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the authors own words. The use of controlled vocabularies can be costly compared to free text searches because human experts or expensive automated systems are necessary to index each entry. Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision. Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including faceted classification, which enables a given data record or document to be described in multiple ways. == Applications == Controlled vocabularies, such as the Library of Congress Subject Headings, are an essential component of bibliography, the study and classification of books. They were initially developed in library and information science. In the 1950s, government agencies began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the Medical Subject Headings (MeSH) developed by the U.S. National Library of Medicine. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup X.25 networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first full text databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library. In large organizations, controlled vocabularies may be introduced to improve technical communication. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing. This consistency of terms is one of the most important concepts in technical writing and knowledge management, where effort is expended to use the same word throughout a document or organization instead of slightly different ones to refer to the same thing. Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a Semantic Web, in which the content of Web pages is described using a machine-readable metadata scheme. One of the first proposals for such a scheme is the Dublin Core Initiative. An example of a controlled vocabulary which is usable for indexing web pages is PSH. It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web. To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web pages contents. The eXchangeable Faceted Metadata Language (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on faceted classification principles. Controlled vocabularies of the Semantic Web define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of Š—ç’ ’PersonŠ—ç’ Î, such as the Friend of a Friend (FOAF) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of Schema.org. Similarly, a book can be described using the Book vocabulary of Schema.org and general publication terms from the Dublin Core vocabulary, an event with the Event vocabulary of Schema.org, and so on. To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, HTML5 Microdata, or JSON-LD in the markup, or RDF serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files. == See also == Authority control Controlled natural language IMS Vocabulary Definition Exchange Named-entity recognition Nomenclature Ontology (computer science) Terminology Thesaurus Universal Data Element Framework Vocabulary-based transformation == References == == External links == controlledvocabulary.com Š—ç’ ’– explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases. photo-keywords.com/ Š—ç’ ’– useful guides to creating and editing your own controlled vocabulary suitable for image cataloging. ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies"
Cross-language information retrieval,Cross-language information retrieval,,,"Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the users query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most CLIR systems use translation techniques. CLIR techniques can be classified into different categories based on different translation resources: Dictionary-based CLIR techniques Parallel corpora based CLIR techniques Comparable corpora based CLIR techniques Machine translator based CLIR techniques CLIR systems have improved so much that the most accurate CLIR systems today are nearly as effective as monolingual systems. The first workshop on CLIR was held in Z’‘ rich during the SIGIR-96 conference. Workshops have been held yearly since 2000 at the meetings of the Cross Language Evaluation Forum (CLEF). Researchers also convene at the annual Text Retrieval Conference (TREC) to discuss their findings regarding different systems and methods of information retrieval, and the conference has served as a point of reference for the CLIR subfield. The term cross-language information retrieval has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term multilingual information retrieval refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual. Google Search had a cross-language search feature that was removed in 2013. == See also == EXCLAIM (EXtensible Cross-Linguistic Automatic Information Machine) == References == == External links == A resource page for CLIR A search engine for CLIR"
Data mining,Data mining,,,"Data mining is the computing process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. It is an interdisciplinary subfield of computer science. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the knowledge discovery in databases process, or KDD. The term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics - or, when referring to actual methods, artificial intelligence and machine learning - are more appropriate. The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps. The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. == Etymology == In the 1960s, statisticians used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term data mining appeared around 1990 in the database community. For a short time in 1980s, a phrase database miningŠ—ç’£Î¢, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term knowledge discovery in databases for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities. Currently, the terms data mining and knowledge discovery are used interchangeably. In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations. The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field. == Background == The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct hands-on data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets. == Process == The knowledge discovery in databases (KDD) process is commonly defined with the stages: (1) Selection (2) Pre-processing (3) Transformation (4) Data mining (5) Interpretation/evaluation. It exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases: (1) Business Understanding (2) Data Understanding (3) Data Preparation (4) Modeling (5) Evaluation (6) Deployment or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation. Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3-4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008. === Pre-processing === Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data. === Data Mining === Data mining involves six common classes of tasks: Anomaly detection (outlier/change/deviation detection) - The identification of unusual data records, that might be interesting or data errors that require further investigation. Association rule learning (dependency modelling) - Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis. Clustering - is the task of discovering groups and structures in the data that are in some way or another similar, without using known structures in the data. Classification - is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as legitimate or as spam. Regression - attempts to find a function which models the data with the least error that is, for estimating the relationships among data or datasets. Summarization - providing a more compact representation of the data set, including visualization and report generation. === Results validation === Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening. The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish spam from legitimate emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves. If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge. == Research == The premier professional body in the field is the Association for Computing Machinerys (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled SIGKDD Explorations. Computer science conferences on data mining include: CIKM Conference - ACM Conference on Information and Knowledge Management DMIN Conference - International Conference on Data Mining DMKD Conference - Research Issues on Data Mining and Knowledge Discovery DSAA Conference - IEEE International Conference on Data Science and Advanced Analytics ECDM Conference - European Conference on Data Mining ECML-PKDD Conference - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases EDM Conference - International Conference on Educational Data Mining INFOCOM Conference - IEEE INFOCOM ICDM Conference - IEEE International Conference on Data Mining KDD Conference - ACM SIGKDD Conference on Knowledge Discovery and Data Mining MLDM Conference - Machine Learning and Data Mining in Pattern Recognition PAKDD Conference - The annual Pacific-Asia Conference on Knowledge Discovery and Data Mining PAW Conference - Predictive Analytics World SDM Conference - SIAM International Conference on Data Mining (SIAM) SSTD Symposium - Symposium on Spatial and Temporal Databases WSDM Conference - ACM Conference on Web Search and Data Mining Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases == Standards == There have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft. For exchanging the extracted models - in particular for use in predictive analytics - the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG. == Notable uses == Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance. == Privacy concerns and ethics == While the term data mining itself may have no ethical implications, it is often associated with the mining of information in relation to peoples behavior (ethical and otherwise). The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns. Data mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). This is not data mining per se, but a result of the preparation of data before - and for the purposes of - the analysis. The threat to an individuals privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous. It is recommended that an individual is made aware of the following before data are collected: the purpose of the data collection and any (known) data mining projects; how the data will be used; who will be able to mine the data and use the data and their derivatives; the status of security surrounding access to the data; how collected data can be updated. Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even de-identified/anonymized data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL. The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies. === Situation in Europe === Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowdens global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed. === Situation in the United States === In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their informed consent regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, [i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena, says the AAHC. More importantly, the rules goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants, which approach a level of incomprehensibility to average individuals. This underscores the necessity for data anonymity in data aggregation and mining practices. U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation. == Copyright law == === Situation in Europe === Due to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013. === Situation in the United States === By contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Googles digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining. == Software == === Free open-source data mining software and applications === The following applications are available under free/open source licenses. Public access to application source code is also available. Carrot2: Text and search results clustering framework. Chemicalize.org: A chemical structure miner and web search engine. ELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language. GATE: a natural language processing and language engineering tool. KNIME: The Konstanz Information Miner, a user friendly and comprehensive data analytics framework. Massive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language. ML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results. MLPACK library: a collection of ready-to-use machine learning algorithms written in the C++ language. MEPX - cross platform tool for regression and classification problems based on a Genetic Programming variant. NLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language. OpenNN: Open neural networks library. Orange: A component-based data mining and machine learning software suite written in the Python language. R: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project. scikit-learn is an open source machine learning library for the Python programming language Torch: An open source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms. UIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video - originally developed by IBM. Weka: A suite of machine learning software applications written in the Java programming language. === Proprietary data-mining software and applications === The following applications are available under proprietary licenses. Angoss KnowledgeSTUDIO: data mining tool. Clarabridge: text analytics product. Vertica: data mining software provided by Hewlett-Packard. SPSS Modeler: data mining software provided by IBM. KXEN Modeler: data mining tool provided by KXEN Inc.. LIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach. Megaputer Intelligence: data and text mining software is called PolyAnalyst. Microsoft Analysis Services: data mining software provided by Microsoft. NetOwl: suite of multilingual text and entity analytics products that enable data mining. OpenText Big Data Analytics: Visual Data Mining & Predictive Analysis by Open Text Corporation Oracle Data Mining: data mining software by Oracle Corporation. PSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE. Qlucore Omics Explorer: data mining software. RapidMiner: An environment for machine learning and data mining experiments. SAS Enterprise Miner: data mining software provided by the SAS Institute. STATISTICA Data Miner: data mining software provided by StatSoft. Tanagra: Visualisation-oriented data mining software, also for teaching. === Marketplace surveys === Several researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include: Hurwitz Victory Index: Report for Advanced Analytics as a market research assessment tool, it highlights both the diverse uses for advanced analytics technology and the vendors who make those applications possible.Recent-research 2011 Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery Rexer Analytics Data Miner Surveys (2007-2015) Forrester Research 2010 Predictive Analytics and Data Mining Solutions report Gartner 2008 Magic Quadrant report Robert A. Nisbets 2006 Three Part Series of articles Data Mining Tools: Which One is Best For CRM? Haughton et al.s 2003 Review of Data Mining Software Packages in The American Statistician Goebel & Gruenwald 1999 A Survey of Data Mining a Knowledge Discovery Software Tools in SIGKDD Explorations == See also == Methods Application domains Application examples Related topics Data mining is about analyzing data; for information about extracting information out of data, see: Other resources International Journal of Data Warehousing and Mining == References == == Further reading == Cabena, Peter; Hadjnian, Pablo; Stadler, Rolf; Verhees, Jaap; Zanasi, Alessandro (1997); Discovering Data Mining: From Concept to Implementation, Prentice Hall, ISBN 0-13-743980-6 M.S. Chen, J. Han, P.S. Yu (1996) Data mining: an overview from a database perspective. Knowledge and data Engineering, IEEE Transactions on 8 (6), 866-883 Feldman, Ronen; Sanger, James (2007); The Text Mining Handbook, Cambridge University Press, ISBN 978-0-521-83657-9 Guo, Yike; and Grossman, Robert (editors) (1999); High Performance Data Mining: Scaling Algorithms, Applications and Systems, Kluwer Academic Publishers Han, Jiawei, Micheline Kamber, and Jian Pei. Data mining: concepts and techniques. Morgan kaufmann, 2006. Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome (2001); The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, ISBN 0-387-95284-5 Liu, Bing (2007); Web Data Mining: Exploring Hyperlinks, Contents and Usage Data, Springer, ISBN 3-540-37881-2 Murphy, Chris (16 May 2011). Is Data Mining Free Speech?. InformationWeek. UMB: 12. Nisbet, Robert; Elder, John; Miner, Gary (2009); Handbook of Statistical Analysis & Data Mining Applications, Academic Press/Elsevier, ISBN 978-0-12-374765-5 Poncelet, Pascal; Masseglia, Florent; and Teisseire, Maguelonne (editors) (October 2007); Data Mining Patterns: New Methods and Applications, Information Science Reference, ISBN 978-1-59904-162-9 Tan, Pang-Ning; Steinbach, Michael; and Kumar, Vipin (2005); Introduction to Data Mining, ISBN 0-321-32136-7 Theodoridis, Sergios; and Koutroumbas, Konstantinos (2009); Pattern Recognition, 4th Edition, Academic Press, ISBN 978-1-59749-272-0 Weiss, Sholom M.; and Indurkhya, Nitin (1998); Predictive Data Mining, Morgan Kaufmann Witten, Ian H.; Frank, Eibe; Hall, Mark A. (30 January 2011). Data Mining: Practical Machine Learning Tools and Techniques (3 ed.). Elsevier. ISBN 978-0-12-374856-0. (See also Free Weka software) Ye, Nong (2003); The Handbook of Data Mining, Mahwah, NJ: Lawrence Erlbaum == External links == Knowledge Discovery Software at DMOZ Data Mining Tool Vendors at DMOZ"
Data modeling,Data modeling,,,"Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. == Overview == Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. There are three different types of data models produced while progressing from requirements to the actual database to be used for the information system. The data requirements are initially recorded as a conceptual data model which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into a logical data model, which documents structures of the data that can be implemented in databases. Implementation of one conceptual data model may require multiple logical data models. The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details. Data modeling defines not just data elements, but also their structures and the relationships between them. Data modeling techniques and methodologies are used to model data in a standard, consistent, predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using data modeling: to assist business analysts, programmers, testers, manual writers, IT package selectors, engineers, managers, related organizations and clients to understand and use an agreed semi-formal model the concepts of the organization and how they relate to one another to manage data as a resource for the integration of information systems for designing databases/data warehouses (aka data repositories) Data modeling may be performed during various types of projects and in multiple phases of projects. Data models are progressive; there is no such thing as the final data model for a business or application. Instead a data model should be considered a living document that will change in response to a changing business. The data models should ideally be stored in a repository so that they can be retrieved, expanded, and edited over time. Whitten et al. (2004) determined two types of data modeling: Strategic data modeling: This is part of the creation of an information systems strategy, which defines an overall vision and architecture for information systems is defined. Information engineering is a methodology that embraces this approach. Data modeling during systems analysis: In systems analysis logical data models are created as part of the development of new databases. Data modeling is also used as a technique for detailing business requirements for specific databases. It is sometimes called database modeling because a data model is eventually implemented in a database. == Data modeling topics == === Data models === Data models provide a structure for data used within information systems by providing specific definition and format. If a data model is used consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data seamlessly. The results of this are indicated in the diagram. However, systems and interfaces are often expensive to build, operate, and maintain. They may also constrain the business rather than support it. This may occur when the quality of the data models implemented in systems and interfaces is poor. Business rules, specific to how things are done in a particular place, are often fixed in the structure of a data model. This means that small changes in the way business is conducted lead to large changes in computer systems and interfaces. So, business rules need to be implemented in a flexible way that does not result in complicated dependencies, rather the data model should be flexible enough so that changes in the business can be implemented within the data model in a relatively quick and efficient way. Entity types are often not identified, or are identified incorrectly. This can lead to replication of data, data structure and functionality, together with the attendant costs of that duplication in development and maintenance.Therefore, data definitions should be made as explicit and easy to understand as possible to minimize misinterpretation and duplication. Data models for different systems are arbitrarily different. The result of this is that complex interfaces are required between systems that share data. These interfaces can account for between 25-70% of the cost of current systems. Required interfaces should be considered inherently while designing a data model, as a data model on its own would not be usable without interfaces within different systems. Data cannot be shared electronically with customers and suppliers, because the structure and meaning of data has not been standardised. To obtain optimal value from an implemented data model, it is very important to define standards that will ensure that data models will both meet business needs and be consistent. === Conceptual, logical and physical schemas === In 1975 ANSI described three kinds of data-model instance: Conceptual schema: describes the semantics of a domain (the scope of the model). For example, it may be a model of the interest area of an organization or of an industry. This consists of entity classes, representing kinds of things of significance in the domain, and relationships assertions about associations between pairs of entity classes. A conceptual schema specifies the kinds of facts or propositions that can be expressed using the model. In that sense, it defines the allowed expressions in an artificial language with a scope that is limited by the scope of the model. Simply described, a conceptual schema is the first step in organizing the data requirements. Logical schema: describes the structure of some domain of information. This consists of descriptions of (for example) tables, columns, object-oriented classes, and XML tags. The logical schema and conceptual schema are sometimes implemented as one and the same. Physical schema: describes the physical means used to store data. This is concerned with partitions, CPUs, tablespaces, and the like. According to ANSI, this approach allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual schema. The table/column structure can change without (necessarily) affecting the conceptual schema. In each case, of course, the structures must remain consistent across all schemas of the same data model. === Data modeling process === In the context of business process integration (see figure), data modeling complements business process modeling, and ultimately results in database generation. The process of designing a database involves producing the previously described three types of schemas - conceptual, logical, and physical. The database design documented in these schemas are converted through a Data Definition Language, which can then be used to generate a database. A fully attributed data model contains detailed attributes (descriptions) for every entity within it. The term database design can describe many different parts of the design of an overall database system. Principally, and most correctly, it can be thought of as the logical design of the base data structures used to store the data. In the relational model these are the tables and views. In an object database the entities and relationships map directly to object classes and named relationships. However, the term database design could also be used to apply to the overall process of designing, not just the base data structures, but also the forms and queries used as part of the overall database application within the Database Management System or DBMS. In the process, system interfaces account for 25% to 70% of the development and support costs of current systems. The primary reason for this cost is that these systems do not share a common data model. If data models are developed on a system by system basis, then not only is the same analysis repeated in overlapping areas, but further analysis must be performed to create the interfaces between them. Most systems within an organization contain the same basic data, redeveloped for a specific purpose. Therefore, an efficiently designed basic data model can minimize rework with minimal modifications for the purposes of different systems within the organization === Modeling methodologies === Data models represent information areas of interest. While there are many ways to create data models, according to Len Silverston (1997) only two modeling methodologies stand out, top-down and bottom-up: Bottom-up models or View Integration models are often the result of a reengineering effort. They usually start with existing data structures forms, fields on application screens, or reports. These models are usually physical, application-specific, and incomplete from an enterprise perspective. They may not promote data sharing, especially if they are built without reference to other parts of the organization. Top-down logical data models, on the other hand, are created in an abstract way by getting information from people who know the subject area. A system may not implement all the entities in a logical model, but the model serves as a reference point or template. Sometimes models are created in a mixture of the two methods: by considering the data needs and structure of an application and by consistently referencing a subject-area model. Unfortunately, in many environments the distinction between a logical data model and a physical data model is blurred. In addition, some CASE tools donŠ—ç’ Îét make a distinction between logical and physical data models. === Entity relationship diagrams === There are several notations for data modeling. The actual model is frequently called Entity relationship model, because it depicts data in terms of the entities and relationships described in the data. An entity-relationship model (ERM) is an abstract conceptual representation of structured data. Entity-relationship modeling is a relational schema database modeling method, used in software engineering to produce a type of conceptual data model (or semantic data model) of a system, often a relational database, and its requirements in a top-down fashion. These models are being used in the first stage of information system design during the requirements analysis to describe information needs or the type of information that is to be stored in a database. The data modeling technique can be used to describe any ontology (i.e. an overview and classifications of used terms and their relationships) for a certain universe of discourse i.e. area of interest. Several techniques have been developed for the design of data models. While these methodologies guide data modelers in their work, two different people using the same methodology will often come up with very different results. Most notable are: Bachman diagrams Barkers notation Chens Notation Data Vault Modeling Extended Backus-Naur form IDEF1X Object-relational mapping Object-Role Modeling Relational Model Relational Model/Tasmania === Generic data modeling === Generic data models are generalizations of conventional data models. They define standardized general relation types, together with the kinds of things that may be related by such a relation type. The definition of generic data model is similar to the definition of a natural language. For example, a generic data model may define relation types such as a classification relation, being a binary relation between an individual thing and a kind of thing (a class) and a part-whole relation, being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related. Given an extensible list of classes, this allows the classification of any individual thing and to specify part-whole relations for any individual object. By standardization of an extensible list of relation types, a generic data model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages. Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model. === Semantic data modeling === The logical data structure of a DBMS, whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. That is unless the semantic data model is implemented in the database on purpose, a choice which may slightly impact performance but generally vastly improves productivity. Therefore, the need to define data from a conceptual view has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure the real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world. A semantic data model can be used to serve many purposes, such as: planning of data resources building of shareable databases evaluation of vendor software integration of existing databases The overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the Artificial Intelligence field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations. == See also == Architectural pattern (computer science) Comparison of data modeling tools Data (computing) Data dictionary Document modeling Information Management Informative modeling Metadata modeling Three schema approach Zachman Framework == References == This article incorporates public domain material from the National Institute of Standards and Technology website http://www.nist.gov. == Further reading == J.H. ter Bekke (1991). Semantic Data Modeling in Relational Environments John Vincent Carlis, Joseph D. Maguire (2001). Mastering Data Modeling: A User-driven Approach. Alan Chmura, J. Mark Heumann (2005). Logical Data Modeling: What it is and how to Do it. Martin E. Modell (1992). Data Analysis, Data Modeling, and Classification. M. Papazoglou, Stefano Spaccapietra, Zahir Tari (2000). Advances in Object-oriented Data Modeling. G. Lawrence Sanders (1995). Data Modeling Graeme C. Simsion, Graham C. Witt (2005). Data Modeling Essentials Matthew West (2011) Developing High Quality Data Models == External links == Agile/Evolutionary Data Modeling Data modeling articles Database Modelling in UML Data Modeling 101 Semantic data modeling System Development, Methodologies and Modeling Notes on by Tony Drewry Request For Proposal - Information Management Metamodel (IMM) of the Object Management Group Data Modeling is NOT just for DBMSs Part 1 Chris Bradley Data Modeling is NOT just for DBMSs Part 2 Chris Bradley"
Dimensionality reduction,Dimensionality reduction,,,"In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables. It can be divided into feature selection and feature extraction. == Feature selection == Feature selection approaches try to find a subset of the original variables (also called features or attributes). There are three strategies; filter (e.g. information gain) and wrapper (e.g. search guided by accuracy) approaches, and embedded (features are selected to add or be removed while building the model based on the prediction errors). See also combinatorial optimization problems. In some cases, data analysis such as regression or classification can be done in the reduced space more accurately than in the original space. == Feature extraction == Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning. === Principal component analysis (PCA) === The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigen vectors on this matrix are computed. The eigen vectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigen vectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. === Kernel PCA === Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled kernel PCA. === Graph-based kernel PCA === Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and local tangent space alignment (LTSA). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors. An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis. A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation. === Linear discriminant analysis (LDA) === Linear discriminant analysis (LDA) is a generalization of fishers linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. === Generalized discriminant analysis (GDA) === GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter. == Dimension reduction == For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality. Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding. For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality sensitive hashing, random projection, sketches or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option. == Advantages of dimensionality reduction == It reduces the time and storage space required. Removal of multi-collinearity improves the performance of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. == Applications == A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved. == See also == == Notes == == References == Fodor,I. (2002) A survey of dimension reduction techniques. Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494 Cunningham, P. (2007) Dimension Reduction University College Dublin, Technical Report UCD-CSI-2007-7 Zahorian, Stephen A.; Hu, Hongbing (2011). Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition. Speech Technologies. ISBN 978-953-307-996-7. doi:10.5772/16863. Lakshmi Padmaja, Dhyaram; Vishnuvardhan, B (18 August 2016). Comparative Study of Feature Subset Selection Methods for Dimensionality Reduction on Scientific Data: 31-34. doi:10.1109/IACC.2016.16. Retrieved 7 October 2016. == External links == JMLR Special Issue on Variable and Feature Selection ELastic MAPs Locally Linear Embedding A Global Geometric Framework for Nonlinear Dimensionality Reduction"
Discounted cumulative gain,Discounted cumulative gain,,,"Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks. == Overview == Two assumptions are made in using DCG and its related measures. Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks) Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents. DCG originates from an earlier, more primitive, measure called Cumulative Gain. === Cumulative Gain === Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position p { p} is defined as: C G p = Š—ç’ ’” i = 1 p r e l i { } = {i=1}^{p}rel {i}} Where r e l i { rel {i}} is the graded relevance of the result at position i { i} . The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document d i { d {i}} above a higher ranked, less relevant, document d j { d {j}} does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure. === Discounted Cumulative Gain === The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position p { p} is defined as: D C G p = Š—ç’ ’” i = 1 p r e l i log 2 Š—çÎ’ ( i + 1 ) = r e l 1 + Š—ç’ ’” i = 2 p r e l i log 2 Š—çÎ’ ( i + 1 ) { } = {i=1}^{p}{}{ {2}(i+1)}}=rel {1}+ {i=2}^{p}{}{ {2}(i+1)}}} Previously there has not been any theoretically sound justification for using a logarithmic reduction factor other than the fact that it produces a smooth reduction. But Wang et al. (2013) give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner. An alternative formulation of DCG places stronger emphasis on retrieving relevant documents: D C G p = Š—ç’ ’” i = 1 p 2 r e l i Š—ç’ ’« 1 log 2 Š—çÎ’ ( i + 1 ) { } = {i=1}^{p}{}-1}{ {2}(i+1)}}} The latter formula is commonly used in industry including major web search companies and data science competition platform such as Kaggle. These two formulations of DCG are the same when the relevance values of documents are binary; r e l i Š—ç’ ’ { 0 , 1 } { rel {i} } . Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2. When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation. Clearly, the base of the log affects the value of DCG in both formulations. === Normalized DCG === Search result lists vary in length depending on the query. Comparing a search engines performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of p { p} should be normalized across queries. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position p { p} , also called Ideal DCG (IDCG) through that position. For a query, the normalized discounted cumulative gain, or nDCG, is computed as: n D C G p = D C G p I D C G p { } ={}{IDCG {p}}}} , where: I D C G p = Š—ç’ ’” i = 1 | R E L | 2 r e l i Š—ç’ ’« 1 log 2 Š—çÎ’ ( i + 1 ) { } = {i=1}^{|REL|}{}-1}{ {2}(i+1)}}} and |REL| represents the list of relevant documents (ordered by their relevance) in the corpus up to position p. The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engines ranking algorithm. Note that in a perfect ranking algorithm, the D C G p { DCG {p}} will be the same as the I D C G p { IDCG {p}} producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable. The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial relevance feedback is available. == Example == Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning not relevant, 3 meaning highly relevant, and 1 and 2 meaning somewhere in between. For the documents ordered by the ranking algorithm as D 1 , D 2 , D 3 , D 4 , D 5 , D 6 { D {1},D {2},D {3},D {4},D {5},D {6}} the user provides the following relevance scores: 3 , 2 , 3 , 0 , 1 , 2 { 3,2,3,0,1,2} That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is: C G 6 = Š—ç’ ’” i = 1 6 r e l i = 3 + 2 + 3 + 0 + 1 + 2 = 11 { } = {i=1}^{6}rel {i}=3+2+3+0+1+2=11} Changing the order of any two documents does not affect the CG measure. If D 3 { D {3}} and D 4 { D {4}} are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is: So the D C G 6 { DCG {6}} of this ranking is: D C G 6 = Š—ç’ ’” i = 1 6 r e l i log 2 Š—çÎ’ ( i + 1 ) = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861 { } = {i=1}^{6}{}{ {2}(i+1)}}=3+1.262+1.5+0+0.387+0.712=6.861} Now a switch of D 3 { D {3}} and D 4 { D {4}} results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank. The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized. To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the monotonically decreasing sort of the relevance judgments provided by the experiment participant, which is: 3 , 3 , 2 , 2 , 1 , 0 { 3,3,2,2,1,0} The DCG of this ideal ordering, or IDCG (Ideal DCG) , is then: I D C G 6 = 7.141 { } =7.141} And so the nDCG for this query is given as: n D C G 6 = D C G 6 I D C G 6 = 6.861 7.141 = 0.961 { } ={}{IDCG {6}}}={{7.141}}=0.961} == Limitations == Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores 1 , 1 , 1 { 1,1,1} and 1 , 1 , 1 , 0 { 1,1,1,0} respectively, both would be considered equally good even if the latter contains a bad result. One way to take into account this limitation is to use 1 Š—ç’ ’« 2 r e l i { 1-2^{rel {i}}} in the numerator for scores for which we want to penalize and 2 r e l i Š—ç’ ’« 1 { 2^{rel {i}}-1} for all others. For example, for the ranking judgments E x c e l l e n t , F a i r , B a d { Excellent,Fair,Bad} one might use numerical scores 1 , 0 , Š—ç’ ’« 1 { 1,0,-1} instead of 2 , 1 , 0 { 2,1,0} . Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores 1 , 1 , 1 { 1,1,1} and 1 , 1 , 1 , 1 , 1 { 1,1,1,1,1} respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores 1 , 1 , 1 , 0 , 0 { 1,1,1,0,0} and 1 , 1 , 1 , 1 , 1 { 1,1,1,1,1} and quote nDCG as nDCG@5. Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as restaurants nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive. == References =="
Human-computer information retrieval,Human-computer information retrieval,,,"Human-computer information retrieval (HCIR) is the study and engineering of information retrieval techniques that bring human intelligence into the search process. It combines the fields of human-computer interaction (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback. == History == This term human-computer information retrieval was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006. MarchioniniŠ—ç’ Îés main thesis is that HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy. In 1996 and 1998, a pair of workshops at the University of Glasgow on information retrieval and human-computer interaction sought to address the overlap between these two fields. Marchionini notes the impact of the World Wide Web and the sudden increase in information literacy - changes that were only embryonic in the late 1990s. A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the University of Maryland Human-Computer Interaction Lab in 2005, alternates between the Association for Computing Machinery Special Interest Group on Information Retrieval (SIGIR) and Special Interest Group on Computer-Human Interaction (CHI) conferences. Also in 2005, the European Science Foundation held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the Massachusetts Institute of Technology. == Description == HCIR includes various aspects of IR and HCI. These include exploratory search, in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system. A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users. Most modern IR systems employ a ranked retrieval model, in which the documents are scored based on the probability of the documents relevance to the query. In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their mean average precision over a set of benchmark queries from organizations like the Text Retrieval Conference (TREC). Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models - one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkins 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of precision and recall but apply them to the results of multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlunds IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc. == Goals == HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results. Systems should no longer only deliver the relevant documents, but must also provide semantic information along with those documents increase user responsibility as well as control; that is, information systems require human intellectual effort have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases aim to be part of information ecology of personal and shared memories and tools rather than discrete standalone services support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase support tuning by end users and especially by information professionals who add value to information resources be engaging and fun to use In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process Š—ç’ ’– the information professional Š—ç’ ’– to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs). == Techniques == The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift. Many search engines have features that incorporate HCIR techniques. Spelling suggestions and automatic query reformulation provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the userŠ—ç’ Îés hands. Faceted search enables users to navigate information hierarchically, going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging. Faceted navigation, like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking. Lookahead provides a general approach to penalty-free exploration. For example, various web applications employ AJAX to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., metadata about the objects) and snippets of document text that are most pertinent to the words in the search query. Relevance feedback allows users to guide an IR system by indicating whether particular results are more or less relevant. Summarization and analytics help users digest the results that come back from the query. Summarization here is intended to encompass any means of aggregating or compressing the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is clustering, which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for java might return clusters for Java (programming language), Java (island), or Java (coffee). Visual representation of data is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of information visualization that allow users access to summary views of search results include tag clouds and treemapping. == Related Areas == Exploratory Video Search == References == == External links == Workshops on Human Computer Information Retrieval. ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)."
Conference on Human Factors in Computing Systems,Conference on Human Factors in Computing Systems,,,"The ACM Conference on Human Factors in Computing Systems (CHI) series of academic conferences is generally considered the most prestigious in the field of human-computer interaction and is one of the top ranked conferences in computer science. It is hosted by ACM SIGCHI, the Special Interest Group on computer-human interaction. CHI has been held annually since 1982 and attracts thousands of international attendees. CHI 2015 was held in Seoul, South Korea, and CHI 2016 was held in San Jose, United States from May 7 to May 12. CHI 2017 was held in Denver, Colorado from May 6-11, 2017. == History == The CHI conference series started with the Human Factors in Computer Systems conference in Gaithersburg, Maryland, US in 1982, organized by Bill Curtis and Ben Shneiderman. During this meeting the formation of the ACM Special Interest Group on Computer-Human Interaction (SIGCHI) was first publicly announced. ACM SIGCHI became the sponsor of the Conference on Human Factors in Computing Systems. The first CHI conference was held in Boston, Massachusetts, US, in 1983. The second conference took place in San Francisco, in 1985. Since then, CHI conferences have been held annually in spring each year. Until 1992 the conference was held in Canada or the US. In 1993 CHI moved to Europe for the first time and was held in Amsterdam, the Netherlands. Over the years, CHI has grown in popularity. The 1982 meeting drew 907 attendees. CHI 90 attracted 2,314. Attendance has been fairly stable since then. After the early years CHI became highly selective. Since 1993 the acceptance rate for full papers was consistently below 30 percent. After 1992 the average acceptance rate was around 20 percent. The number of accepted full papers is slowly increasing and reached 157 accepted papers with an acceptance rate of 22 percent in 2008. CHI continues to grow, reaching over 3,300 attendees in 2013 and 3,800 in 2016. == Tracks == The CHI conference consists of multiple tracks, including: Academic papers and notes (short papers) on a variety of topics, such as (ubiquitous computing, visualization, usability and user experience design) Posters and demonstrations Workshops and courses hosted by domain experts Invited panels on relevant topics Case studies from industry practitioners == Past and upcoming CHI conferences == Past and future CHI conferences include: == References == == External links == ACM SIGCHI website"
Cluster analysis,Cluster analysis,,,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek ’â ’Œ’ ’Œ’£’ŒÎ’Œ’ ’Œ’¢ grape) and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology. == Definition == The notion of a cluster cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these cluster models is key to understanding the differences between the various algorithms. Typical cluster models include: Connectivity models: for example, hierarchical clustering builds models based on distance connectivity. Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector. Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the expectation-maximization algorithm. Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space. Subspace models: in biclustering (also known as co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes. Group models: some algorithms do not provide a refined model for their results and just provide the grouping information. Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm. A clustering is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as: Hard clustering: each object belongs to a cluster or not Soft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster) There are also finer distinctions possible, for example: Strict partitioning clustering: each object belongs to exactly one cluster Strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers Overlapping clustering (also: alternative clustering, multi-view clustering): objects may belong to more than one cluster; usually involving hard clusters Hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster Subspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap == Algorithms == Clustering algorithms can be categorized based on their cluster model, as listed above. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms. There is no objectively correct clustering algorithm, but as it was noted, clustering is in the eye of the beholder. The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. It should be noted that an algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model. For example, k-means cannot find non-convex clusters. === Connectivity-based clustering (hierarchical clustering) === Connectivity based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect objects to form clusters based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name hierarchical clustering comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters dont mix. Connectivity based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance to) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances) or UPGMA (Unweighted Pair Group Method with Arithmetic Mean, also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions). These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as chaining phenomenon, in particular with single-linkage clustering). In the general case, the complexity is O ( n 3 ) {}(n^{3})} for agglomerative clustering and O ( 2 n Š—ç’ ’« 1 ) {}(2^{n-1})} for divisive clustering, which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity O ( n 2 ) {}(n^{2})} ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. They did however provide inspiration for many later methods such as density based clustering. Linkage clustering examples === Centroid-based clustering === In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k { k} cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized. The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximative method is Lloyds algorithm, often actually referred to as k-means algorithm. It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means). Most k-means-type algorithms require the number of clusters - k { k} - to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders in between of clusters (which is not surprising, as the algorithm optimized cluster centers, not cluster borders). K-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based clustering, and Lloyds algorithm as a variation of the Expectation-maximization algorithm for this model discussed below. k-means clustering examples === Distribution-based clustering === The clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution. While the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult. One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modelled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary. Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data). Expectation-maximization (EM) clustering examples === Density-based clustering === In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in these sparse areas - that are required to separate clusters - are usually considered to be noise and border points. The most popular density based clustering method is DBSCAN. In contrast to many newer methods, it features a well-defined cluster model called density-reachability. Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects range. Another interesting property of DBSCAN is that its complexity is fairly low - it requires a linear number of range queries on the database - and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter ’âÎµ { } , and produces a hierarchical result related to that of linkage clustering. DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the ’âÎµ { } parameter entirely and offering performance improvements over OPTICS by using an R-tree index. The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions - a common use case in artificial data - the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data. Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these density attractors can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails. Density-based clustering examples === Recent developments === In recent years considerable effort has been put into improving the performance of existing algorithms. Among them are CLARANS (Ng and Han, 1994), and BIRCH (Zhang et al., 1996). With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting clusters are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering. Various other approaches to clustering have been tried such as seed based clustering. For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (correlated) subspace clusters that can be modeled by giving a correlation of their attributes. Examples for such clustering algorithms are CLIQUE and SUBCLU. Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adopted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using correlation connectivity and ERiC exploring hierarchical density-based correlation clusters). Several different clustering systems based on mutual information have been proposed. One is Marina MeilŠäŒ’Üs variation of information metric; another provides hierarchical clustering. Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information. Also message passing algorithms, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms. == Evaluation and assessment == Evaluation (or validation) of clustering results is as difficult as the clustering itself. Popular approaches involve internal evaluation, where the clustering is summarized to a single quality score, external evaluation, where the clustering is compared to an existing ground truth classification, manual evaluation by a human expert, and indirect evaluation by evaluating the utility of the clustering in its intended application. Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective. For example, one could cluster the data set by the optimum Silhouette coefficient; except that there is no known efficient algorithm for this. By using such an internal measure for evaluation, we rather compare the similarity of the optimization problems, and not necessarily how useful the clustering is. External evaluation has similar problems: if we have such ground truth labels, then we would not need to cluster; and in practical applications we usually do not have such labels. On the other hand, the labels only reflect one possible partitioning of the data set, which does not imply that there does not exist a different, and maybe even better, clustering. Neither of these approaches can therefore ultimately judge the actual quality of a clustering, but this needs human evaluation, which is highly subjective. Nevertheless, such statistics can be quite informative in identifying bad clusterings, but one should not dismiss subjective human evaluation. === Internal evaluation === When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications. Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering. Therefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another. Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion. For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound. The following methods can be used to assess the quality of clustering algorithms based on internal criterion: Davies-Bouldin index The Davies-Bouldin index can be calculated by the following formula: D B = 1 n Š—ç’ ’” i = 1 n max j Š—ç’ ’ i ( ’Œ’Ü i + ’Œ’Ü j d ( c i , c j ) ) { DB={{n}} {i=1}^{n} {j i}({+ {j}}{d(c {i},c {j})}})} where n is the number of clusters, c x { c {x}} is the centroid of cluster x { x} , ’Œ’Ü x { {x}} is the average distance of all elements in cluster x { x} to centroid c x { c {x}} , and d ( c i , c j ) { d(c {i},c {j})} is the distance between centroids c i { c {i}} and c j { c {j}} . Since algorithms that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity) will have a low Davies-Bouldin index, the clustering algorithm that produces a collection of clusters with the smallest Davies-Bouldin index is considered the best algorithm based on this criterion. Dunn index The Dunn index aims to identify dense and well-separated clusters. It is defined as the ratio between the minimal inter-cluster distance to maximal intra-cluster distance. For each cluster partition, the Dunn index can be calculated by the following formula: D = min 1 Š—ç’ i < j Š—ç’ n d ( i , j ) max 1 Š—ç’ k Š—ç’ n d Š—ç’ ( k ) , { D={d(i,j)}{ {1 k n}d^{ }(k)}},,} where d(i,j) represents the distance between clusters i and j, and d (k) measures the intra-cluster distance of cluster k. The inter-cluster distance d(i,j) between two clusters may be any number of distance measures, such as the distance between the centroids of the clusters. Similarly, the intra-cluster distance d (k) may be measured in a variety ways, such as the maximal distance between any pair of elements in cluster k. Since internal criterion seek clusters with high intra-cluster similarity and low inter-cluster similarity, algorithms that produce clusters with high Dunn index are more desirable. Silhouette coefficient The silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters. === External evaluation === In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation. These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies. Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result. In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial. A number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster. Some of the measures of quality of a cluster algorithm using external criterion include: Purity: Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster. Now take the sum over all clusters and divide by the total number of data points. Formally, given some set of clusters M { M} and some set of classes D { D} , both partitioning N { N} data points, purity can be defined as: 1 N Š—ç’ ’” m Š—ç’ ’ M max d Š—ç’ ’ D | m Š—ç’ Î© d | {{N}} {m M} {d D}{|m d|}} Note that this measure doesnt penalise having many clusters. So for example, a purity score of 1 is possible by putting each data point in its own cluster. Rand measure (William M. Rand 1971) The Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. One can also view the Rand index as a measure of the percentage of correct decisions made by the algorithm. It can be computed using the following formula: R I = T P + T N T P + F P + F N + T N { RI={{TP+FP+FN+TN}}} where T P { TP} is the number of true positives, T N { TN} is the number of true negatives, F P { FP} is the number of false positives, and F N { FN} is the number of false negatives. One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern, as does the chance-corrected adjusted Rand index. F-measure The F-measure can be used to balance the contribution of false negatives by weighting recall through a parameter ’â Š—ç’ Î‚ 0 { 0} . Let precision and recall (both external evaluation measures in themselves) be defined as follows: P = T P T P + F P { P={{TP+FP}}} R = T P T P + F N { R={{TP+FN}}} where P { P} is the precision rate and R { R} is the recall rate. We can calculate the F-measure by using the following formula: F ’â = ( ’â 2 + 1 ) Š—ç’“’ P Š—ç’“’ R ’â 2 Š—ç’“’ P + R { F { }={+1) P R}{ ^{2} P+R}}} Notice that when ’â = 0 { =0} , F 0 = P { F {0}=P} . In other words, recall has no impact on the F-measure when ’â = 0 { =0} , and increasing ’â { } allocates an increasing amount of weight to recall in the final F-measure. Jaccard index The Jaccard index is used to quantify the similarity between two datasets. The Jaccard index takes on a value between 0 and 1. An index of 1 means that the two dataset are identical, and an index of 0 indicates that the datasets have no common elements. The Jaccard index is defined by the following formula: J ( A , B ) = | A Š—ç’ Î© B | | A Š—ç’ ÎŽ B | = T P T P + F P + F N { J(A,B)={{|A B|}}={{TP+FP+FN}}} This is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets. Fowlkes-Mallows index (E. B. Fowlkes & C. L. Mallows 1983) The Fowlkes-Mallows index computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications. The higher the value of the Fowlkes-Mallows index the more similar the clusters and the benchmark classifications are. It can be computed using the following formula: F M = T P T P + F P Š—ç’“’ T P T P + F N { FM={{TP+FP}}{TP+FN}}}}} where T P { TP} is the number of true positives, F P { FP} is the number of false positives, and F N { FN} is the number of false negatives. The F M { FM} index is the geometric mean of the precision and recall P { P} and R { R} , while the F-measure is their harmonic mean. Moreover, precision and recall are also known as Wallaces indices B I { B^{I}} and B I I { B^{II}} . The mutual information is an information theoretic measure of how much information is shared between a clustering and a ground-truth classification that can detect a non-linear similarity between two clusterings. Adjusted mutual information is the corrected-for-chance variant of this that has a reduced bias for varying cluster numbers. Confusion matrix A confusion matrix can be used to quickly visualize the results of a classification (or clustering) algorithm. It shows how different a cluster is from the gold standard cluster. === Cluster tendency === To measure cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering. One way to do this is to compare the data against random data. On average, random data should not have clusters. Hopkins statistic There are multiple formulations of the Hopkins Statistic. A typical one is as follows. Let X { X} be the set of n { n} data points in d { d} dimensional space. Consider a random sample (without replacement) of m Š—ç’ ÎŽ n { m n} data points with members x i { x {i}} . Also generate a set Y { Y} of m { m} uniformly randomly distributed data points. Now define two distance measures, u i { u {i}} to be the distance of y i Š—ç’ ’ Y { y {i} Y} from its nearest neighbor in X and w i { w {i}} to be the distance of x i Š—ç’ ’ X { x {i} X} from its nearest neighbor in X. We then define the Hopkins statistic as: H = Š—ç’ ’” i = 1 m u i d Š—ç’ ’” i = 1 m u i d + Š—ç’ ’” i = 1 m w i d , { H={^{m}{u {i}^{d}}}{ {i=1}^{m}{u {i}^{d}}+ {i=1}^{m}{w {i}^{d}}}},,} With this definition, uniform random data should tend to have values near to 0.5, and clustered data should tend to have values nearer to 1. However, data containing just a single Gaussian will also score close to 1, as this statistic measures deviation from a uniform distribution, not multimodality, making this statistic largely useless in application (as real data never is remotely uniform). == Applications == Biology, computational biology and bioinformatics Plant and animal ecology cluster analysis is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments; it is also used in plant systematics to generate artificial phylogenies or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes Transcriptomics clustering is used to build groups of genes with related expression patterns (also known as coexpressed genes) as in HCS clustering algorithm . Often such groups contain functionally related proteins, such as enzymes for a specific pathway, or genes that are co-regulated. High throughput experiments using expressed sequence tags (ESTs) or DNA microarrays can be a powerful tool for genome annotation, a general aspect of genomics. Sequence analysis clustering is used to group homologous sequences into gene families. This is a very important concept in bioinformatics, and evolutionary biology in general. See evolution by gene duplication. High-throughput genotyping platforms clustering algorithms are used to automatically assign genotypes. Human genetic clustering The similarity of genetic data is used in clustering to infer population structures. Medicine Medical imaging On PET scans, cluster analysis can be used to differentiate between diff"
Data compression,Data compression,,,"In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding. Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space-time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data. == Lossless == Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding red pixel, red pixel, ... the data may be encoded as 279 red pixels. This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy. The Lempel-Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip, and PNG. LZW (Lempel-Ziv-Welch) is used in GIF images. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX). Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsofts CAB format. The best modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows-Wheeler transform can also be viewed as an indirect form of statistical modelling. The class of grammar-based codes are gaining popularity because they can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which software is publicly available. In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.264/MPEG-4 AVC and HEVC for video coding. == Lossy == Lossy data compression is the converse of lossless data compression. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video. Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression. In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from audio compression. Different audio and speech compression standards are listed under audio coding formats. Voice compression is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players. == Theory == The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate-distortion theory for lossy compression. These areas of study were essentially forged by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference. === Machine learning === There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for general intelligence. === Data differencing === Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a difference given a source and a target, with patching producing a target given a source and a difference, while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a difference from nothing. This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data. When one wishes to emphasize the connection, one may use the term differential compression to refer to data differencing. == Uses == === Audio === Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them. In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data. The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640MB. Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50-60% of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten, and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression. When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies. A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apples Apple Lossless (ALAC), MPEG-4 ALS, Microsofts Windows Media Audio 9 Lossless (WMA Lossless), Monkeys Audio, TTA, and WavPack. See list of lossless codecs for a complete listing. Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream. Other formats are associated with a distinct system, such as: Direct Stream Transfer, used in Super Audio CD Meridian Lossless Packing, used in DVD-Audio, Dolby TrueHD, Blu-ray and HD DVD ==== Lossy audio compression ==== Lossy audio compression is used in a wide range of applications. In addition to the direct applications (MP3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data. The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all. Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minutes worth of music at adequate quality. ===== Coding methods ===== To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous maskingŠ—ç’ ’–the phenomenon wherein a signal is masked by another signal separated by frequencyŠ—ç’ ’–and, in some cases, temporal maskingŠ—ç’ ’–where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models. Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sounds generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coders quantization noise into the spectrum of the target signal, partially masking it. Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen. Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a frame to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality. In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)). ===== Speech encoding ===== Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate. If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals. This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches: Only encoding sounds that could be made by a single human voice. Throwing away more of the data in the signalŠ—ç’ ’–keeping just enough to reconstruct an intelligible voice rather than the full frequency range of human hearing. Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the ’ÇÎµ-law algorithm. ==== History ==== A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee. The worlds first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies. === Video === Video compression uses modern coding techniques to reduce redundancy in video data. Most video compression algorithms and codecs combine spatial image compression and temporal motion compensation. Video compression is a practical implementation of source coding in information theory. In practice, most video codecs also use audio compression techniques in parallel to compress the separate, but combined data streams as one package. The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5-12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts. Some video compression schemes typically operate on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate. ==== Encoding theory ==== Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video. One of the most powerful techniques for compressing video is interframe compression. Interframe compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression. The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited. Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making cuts in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesnt want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as I frames in MPEG-2) arent allowed to copy data from other frames, so they require much more data than other frames nearby. It is possible to build a computer-based video editor that spots problems caused when I frames are edited out while other frames need them. This has allowed newer formats like HDV to be used for editing. However, this process demands a lot more computing power than editing intraframe compressed video with the same picture quality. Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods. ==== Timeline ==== The following table is a partial history of international video compression standards. === Genetics === Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-foldŠ—ç’ ’–allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes). === Emulation === In order to emulate CD-based consoles such as the PlayStation 2, data compression is desirable to reduce huge amounts of disk space used by ISOs. For example, Final Fantasy XII (USA) is normally 2.9 gigabytes. With proper compression, it is reduced to around 90% of that size. == Outlook and currently unused potential == It is estimated that the total amount of data that is stored on the worlds storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information. == See also == == References == == External links == Data Compression Basics (Video) Video compression 4:2:2 10-bit and its benefits Why does 10-bit save bandwidth (even when content is 8-bit)? Which compression technology should be used Wiley - Introduction to Compression Theory EBU subjective listening tests on low-bitrate audio codecs Audio Archiving Guide: Music Formats (Guide for helping a user pick out the right codec) MPEG 1&2 video compression intro (pdf format) at the Wayback Machine (archived September 28, 2007) hydrogenaudio wiki comparison Introduction to Data Compression by Guy E Blelloch from CMU HD Greetings - 1080p Uncompressed source material for compression testing and research Explanation of lossless signal compression method used by most codecs Interactive blind listening tests of audio codecs over the internet TestVid - 2,000+ HD and other uncompressed source video clips for compression testing Videsignline - Intro to Video Compression Data Footprint Reduction Technology What is Run length Coding in video compression."
Human-computer interaction,Human-computer interaction,,,"Human-computer interaction (commonly referred to as HCI) researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field. == Introduction == Humans interact with computers in many ways; the interface between humans and computers is crucial to facilitating this interaction. Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today. Voice user interfaces (VUI) are used for speech recognition and synthesising systems, and the emerging multi-modal and gestalt User Interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human-computer interaction field has been in quality of interaction, and in different branching in its history. Instead of designing regular interfaces, the different research branches have had a different focus on the concepts of multimodality rather than unimodality, intelligent adaptive interfaces rather than command/action based ones, and finally active rather than passive interfaces. The Association for Computing Machinery (ACM) defines human-computer interaction as a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them. An important facet of HCI is the securing of user satisfaction (or simply End User Computing Satisfaction). Because human-computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant. Due to the multidisciplinary nature of HCI, people with a variety of different backgrounds contribute to its success. HCI is also sometimes termed human-machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI). Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster. Similarly, accidents in aviation have resulted from manufacturers decisions to use non-standard flight instrument or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the standard layout and thus the conceptually good idea actually had undesirable results. == Goals == Human-computer interaction studies the ways in which humans make, or do not make, use of computational artifacts, systems and infrastructures. In doing so, much of the research in the field seeks to improve human-computer interaction by improving the usability of computer interfaces. How usability is to be precisely understood, how it relates to other social and cultural values and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated. Much of the research in the field of human-computer interaction takes an interest in: Methods for designing novel computer interfaces, thereby optimizing a design for a desired property such as, e.g., learnability or efficiency of use. Methods for implementing interfaces, e.g., by means of software libraries. Methods for evaluating and comparing interfaces with respect to their usability and other desirable properties. Methods for studying human computer use and its sociocultural implications more broadly. Models and theories of human computer use as well as conceptual frameworks for the design of computer interfaces, such as, e.g., cognitivist user models, Activity Theory or ethnomethodological accounts of human computer use. Perspectives that critically reflect upon the values that underlie computational design, computer use and HCI research practice. Visions of what researchers in the field seek to achieve vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values. Researchers in HCI are interested in developing new design methodologies, experimenting with new devices, prototyping new software and hardware systems, exploring new interaction paradigms, and developing models and theories of interaction. == Differences with related fields == HCI differs from human factors and ergonomics as HCI focuses more on users working specifically with computers, rather than other kinds of machines or designed artifacts. There is also a focus in HCI on how to implement the computer software and hardware mechanisms to support human-computer interaction. Thus, human factors is a broader term; HCI could be described as the human factors of computers - although some experts try to differentiate these areas. HCI also differs from human factors in that there is less of a focus on repetitive work-oriented tasks and procedures, and much less emphasis on physical stress and the physical form or industrial design of the user interface, such as keyboards and mouse devices. Three areas of study have substantial overlap with HCI even as the focus of inquiry shifts. In the study of personal information management (PIM), human interactions with the computer are placed in a larger informational context - people may work with many forms of information, some computer-based, many not (e.g., whiteboards, notebooks, sticky notes, refrigerator magnets) in order to understand and effect desired changes in their world. In computer-supported cooperative work (CSCW), emphasis is placed on the use of computing systems in support of the collaborative work of a group of people. The principles of human interaction management (HIM) extend the scope of CSCW to an organizational level and can be implemented without use of computers. == Design == === Principles === When evaluating a current user interface, or designing a new user interface, it is important to keep in mind the following experimental design principles: Early focus on user(s) and task(s): Establish how many users are needed to perform the task(s) and determine who the appropriate users should be; someone who has never used the interface, and will not use the interface in the future, is most likely not a valid user. In addition, define the task(s) the users will be performing and how often the task(s) need to be performed. Empirical measurement: Test the interface early on with real users who come in contact with the interface on a daily basis. Keep in mind that results may vary with the performance level of the user and may not be an accurate depiction of the typical human-computer interaction. Establish quantitative usability specifics such as: the number of users performing the task(s), the time to complete the task(s), and the number of errors made during the task(s). Iterative design: After determining the users, tasks, and empirical measurements to include, perform the following iterative design steps: Design the user interface Test Analyze results Repeat Repeat the iterative design process until a sensible, user-friendly interface is created. === Methodologies === A number of diverse methodologies outlining techniques for human-computer interaction design have emerged since the rise of the field in the 1980s. Most design methodologies stem from a model for how users, designers, and technical systems interact. Early methodologies, for example, treated users cognitive processes as predictable and quantifiable and encouraged design practitioners to look to cognitive science results in areas such as memory and attention when designing user interfaces. Modern models tend to focus on a constant feedback and conversation between users, designers, and engineers and push for technical systems to be wrapped around the types of experiences users want to have, rather than wrapping user experience around a completed system. Activity theory: used in HCI to define and study the context in which human interactions with computers take place. Activity theory provides a framework to reason about actions in these contexts, analytical tools with the format of checklists of items that researchers should consider, and informs design of interactions from an activity-centric perspective. User-centered design: user-centered design (UCD) is a modern, widely practiced design philosophy rooted in the idea that users must take center-stage in the design of any computer system. Users, designers and technical practitioners work together to articulate the wants, needs and limitations of the user and create a system that addresses these elements. Often, user-centered design projects are informed by ethnographic studies of the environments in which users will be interacting with the system. This practice is similar but not identical to participatory design, which emphasizes the possibility for end-users to contribute actively through shared design sessions and workshops. Principles of user interface design: these are seven principles of user interface design that may be considered at any time during the design of a user interface in any order: tolerance, simplicity, visibility, affordance, consistency, structure and feedback. Value sensitive design: Value Sensitive Design (VSD) is a method for building technology that account for the values of the people who use the technology directly, as well as those who the technology affects, either directly or indirectly. VSD uses an iterative design process that involves three types of investigations: conceptual, empirical and technical. Conceptual investigations aim at understanding and articulating the various stakeholders of the technology, as well as their values and any values conflicts that might arise for these stakeholders through the use of the technology. Empirical investigations are qualitative or quantitative design research studies used to inform the designers understanding of the users values, needs, and practices. Technical investigations can involve either analysis of how people use related technologies, or the design of systems to support values identified in the conceptual and empirical investigations. == Display designs == Displays are human-made artifacts designed to support the perception of relevant system variables and to facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g. navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information that a system generates and displays; therefore, the information must be displayed according to principles in a manner that will support perception, situation awareness, and understanding. === Thirteen principles of display design === Christopher Wickens et al. defined 13 principles of display design in their book An Introduction to Human Factors Engineering. These principles of human perception and information processing can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved through utilization of these principles. Certain principles may not be applicable to different displays or situations. Some principles may seem to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design. ==== Perceptual principles ==== 1. Make displays legible (or audible). A displays legibility is critical and necessary for designing a usable display. If the characters or objects being displayed cannot be discernible, then the operator cannot effectively make use of them. 2. Avoid absolute judgment limits. Do not ask the user to determine the level of a variable on the basis of a single sensory variable (e.g. colour, size, loudness). These sensory variables can contain many possible levels. 3. Top-down processing. Signals are likely perceived and interpreted in accordance with what is expected based on a users experience. If a signal is presented contrary to the users expectation, more physical evidence of that signal may need to be presented to assure that it is understood correctly. 4. Redundancy gain. If a signal is presented more than once, it is more likely that it will be understood correctly. This can be done by presenting the signal in alternative physical forms (e.g. colour and shape, voice and print, etc.), as redundancy does not imply repetition. A traffic light is a good example of redundancy, as colour and position are redundant. 5. Similarity causes confusion: Use distinguishable elements. Signals that appear to be similar will likely be confused. The ratio of similar features to different features causes signals to be similar. For example, A423B9 is more similar to A423B8 than 92 is to 93. Unnecessarily similar features should be removed and dissimilar features should be highlighted. ==== Mental model principles ==== 6. Principle of pictorial realism. A display should look like the variable that it represents (e.g. high temperature on a thermometer shown as a higher vertical level). If there are multiple elements, they can be configured in a manner that looks like it would in the represented environment. 7. Principle of the moving part. Moving elements should move in a pattern and direction compatible with the users mental model of how it actually moves in the system. For example, the moving element on an altimeter should move upward with increasing altitude. ==== Principles based on attention ==== 8. Minimizing information access cost. When the users attention is diverted from one location to another to access necessary information, there is an associated cost in time or effort. A display design should minimize this cost by allowing for frequently accessed sources to be located at the nearest possible position. However, adequate legibility should not be sacrificed to reduce this cost. 9. Proximity compatibility principle. Divided attention between two information sources may be necessary for the completion of one task. These sources must be mentally integrated and are defined to have close mental proximity. Information access costs should be low, which can be achieved in many ways (e.g. proximity, linkage by common colours, patterns, shapes, etc.). However, close display proximity can be harmful by causing too much clutter. 10. Principle of multiple resources. A user can more easily process information across different resources. For example, visual and auditory information can be presented simultaneously rather than presenting all visual or all auditory information. ==== Memory principles ==== 11. Replace memory with visual information: knowledge in the world. A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, the use of memory may sometimes benefit the user by eliminating the need to reference some type of knowledge in the world (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a users head and knowledge in the world must be balanced for an effective design. 12. Principle of predictive aiding. Proactive actions are usually more effective than reactive actions. A display should attempt to eliminate resource-demanding cognitive tasks and replace them with simpler perceptual tasks to reduce the use of the users mental resources. This will allow the user to focus on current conditions, and to consider possible future conditions. An example of a predictive aid is a road sign displaying the distance to a certain destination. 13. Principle of consistency. Old habits from other displays will easily transfer to support processing of new displays if they are designed consistently. A users long-term memory will trigger actions that are expected to be appropriate. A design must accept this fact and utilize consistency among different displays. == Human-computer interface == The human-computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including: Visual Based :The visual based human computer inter-action is probably the most widespread area in HCI(Human Computer Interaction) research. Audio Based : The audio based interaction between a computer and a human is another important area of in HCI systems. This area deals with information acquired by different audio signals. Task environment: The conditions and goals set upon the user. Machine environment: The environment that the computer is connected to, e.g. a laptop in a college students dorm room. Areas of the interface: Non-overlapping areas involve processes of the human and computer not pertaining to their interaction. Meanwhile, the overlapping areas only concern themselves with the processes pertaining to their interaction. Input flow: The flow of information that begins in the task environment, when the user has some task that requires using their computer. Output: The flow of information that originates in the machine environment. Feedback: Loops through the interface that evaluate, moderate, and confirm processes as they pass from the human through the interface to the computer and back. Fit: This is the match between the computer design, the user and the task to optimize the human resources needed to accomplish the task. == Current research == Topics in HCI include: === User customization === End-user development studies how ordinary users could routinely tailor applications to their own needs and use this power to invent new applications based on their understanding of their own domains. With their deeper knowledge of their own knowledge domains, users could increasingly be important sources of new applications at the expense of generic systems programmers (with systems expertise but low domain expertise). === Embedded computation === Computation is passing beyond computers into every object for which uses can be found. Embedded systems make the environment alive with little computations and automated processes, from computerized cooking appliances to lighting and plumbing fixtures to window blinds to automobile braking systems to greeting cards. To some extent, this development is already taking place. The expected difference in the future is the addition of networked communications that will allow many of these embedded computations to coordinate with each other and with the user. Human interfaces to these embedded devices will in many cases be very different from those appropriate to workstations. === Augmented reality === A common staple of science fiction, augmented reality refers to the notion of layering relevant information into our vision of the world. Existing projects show real-time statistics to users performing difficult tasks, such as manufacturing. Future work might include augmenting our social interactions by providing additional information about those we converse with. === Social computing === In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a mans name to cost more than a machine with a womans name. Other research finds that individuals perceive their interactions with computers more positively than humans, despite behaving the same way towards these machines. === Knowledge-driven human-computer interaction === In human and computer interactions, there usually exists a semantic gap between human and computers understandings towards mutual behaviors. Ontology (information science), as a formal representation of domain-specific knowledge, can be used to address this problem, through solving the semantic ambiguities between the two parties. == Factors of change == Traditionally, as explained in a journal article discussing user modeling and user-adapted interaction, computer use was modeled as a human-computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. However, as stated in the introduction, there is much room for mishaps and failure. Because of these potential issues, human-computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: If ease of use was the only valid criterion, people would stick to tricycles and never try bicycles. The means by which humans interact with computers continues to evolve rapidly. Human-computer interaction is affected by the forces shaping the nature of future computing. These forces include: Decreasing hardware costs leading to larger memory and faster systems Miniaturization of hardware leading to portability Reduction in power requirements leading to portability New display technologies leading to the packaging of computational devices in new forms Specialized hardware leading to new functions Increased development of network communication and distributed computing Increasingly widespread use of computers, especially by people who are outside of the computing profession Increasing innovation in input techniques (e.g., voice, gesture, pen), combined with lowering cost, leading to rapid computerization by people formerly left out of the computer revolution. Wider social concerns leading to improved access to computers by currently disadvantaged groups The future for HCI, based on current promising research, is expected to include the following characteristics: Ubiquitous computing and communication. Computers are expected to communicate through high speed local networks, nationally over wide-area networks, and portably via infrared, ultrasonic, cellular, and other technologies. Data and computational services will be portably accessible from many if not most locations to which a user travels. High-functionality systems. Systems can have large numbers of functions associated with them. There are so many systems that most users, technical or non-technical, do not have time to learn them in the traditional way (e.g., through thick manuals). Mass availability of computer graphics. Computer graphics capabilities such as image processing, graphics transformations, rendering, and interactive animation are becoming widespread as inexpensive chips become available for inclusion in general workstations and mobile devices. Mixed media. Commercial systems can handle images, voice, sounds, video, text, formatted data. These are exchangeable over communication links among users. The separate fields of consumer electronics (e.g., stereo sets, VCRs, televisions) and computers are merging partly. Computer and print fields are expected to cross-assimilate. High-bandwidth interaction. The rate at which humans and machines interact is expected to increase substantially due to the changes in speed, computer graphics, new media, and new input/output devices. This can lead to some qualitatively different interfaces, such as virtual reality or computational video. Large and thin displays. New display technologies are finally maturing, enabling very large displays and displays that are thin, lightweight, and low in power use. This is having large effects on portability and will likely enable developing paper-like, pen-based computer interaction systems very different in feel from desktop workstations of the present. Information utilities. Public information utilities (such as home banking and shopping) and specialized industry services (e.g., weather for pilots) are expected to proliferate. The rate of proliferation can accelerate with the introduction of high-bandwidth interaction and the improvement in quality of interfaces. == Scientific conferences == One of the main conferences for new research in human-computer interaction is the annually held Association for Computing Machinerys (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or khai). CHI is organized by ACM Special Interest Group on Computer-Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners and industry people, with company sponsors such as Google, Microsoft, and PayPal. There are also dozens of other smaller, regional or specialized HCI-related conferences held around the world each year, including: == See also == Outline of human-computer interaction Information design Experience design Information architecture Physiological interaction User experience design Mindfulness and technology HCI Bibliography, a web-based project to provide a bibliography of Human Computer Interaction literature == Footnotes == == Further reading == Academic overviews of the field Julie A. Jacko (Ed.). (2012). Human-Computer Interaction Handbook (3rd Edition). CRC Press. ISBN 1-4398-2943-8 Andrew Sears and Julie A. Jacko (Eds.). (2007). Human-Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9 Julie A. Jacko and Andrew Sears (Eds.). (2003). Human-Computer Interaction Handbook. Mahwah: Lawrence Erlbaum & Associates. ISBN 0-8058-4468-6 Historically important classic Stuart K. Card, Thomas P. Moran, Allen Newell (1983): The Psychology of Human-Computer Interaction. Erlbaum, Hillsdale 1983 ISBN 0-89859-243-7 Overviews of history of the field Jonathan Grudin: A moving target: The evolution of human-computer interaction. In Andrew Sears and Julie A. Jacko (Eds.). (2007). Human-Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9 Myers, Brad (1998). A brief history of human-computer interaction technology.. Interactions. 5 (2): 44-54. doi:10.1145/274430.274436. John M. Carroll: Human Computer Interaction: History and Status. Encyclopedia Entry at Interaction-Design.org Carroll, John M. (2010). Conceptualizing a possible discipline of human-computer interaction. Interacting with Computers. 22 (1): 3-12. doi:10.1016/j.intcom.2009.11.008. Sara Candeias, S. and A. Veiga The dialogue between man and machine: the role of language theory and technology, Sandra M. Alu’‘ sio & Stella E. O. Tagnin, New Language Technologies and Linguistic Research, A Two-Way Road: cap. 11. Cambridge Scholars Publishing. (ISBN 978-1-4438-5377-4) Social science and HCI Nass, Clifford; Fogg, B. J.; Moon, Youngme (1996). Can computers be teammates?. International Journal of Human-Computer Studies. 45 (6): 669-678. doi:10.1006/ijhc.1996.0073. Nass, Clifford; Moon, Youngme (2000). Machines and mindlessness: Social responses to computers. Journal of social issues. 56 (1): 81-103. doi:10.1111/0022-4537.00153. Posard, Marek N (2014). Status processes in human-computer interactions: Does gender matter?. Computers in Human Behavior. 37: 189-195. doi:10.1016/j.chb.2014.04.025. Posard, Marek N.; Rinderknecht, R. Gordon (2015). Do people like working with computers more than human beings?.. Computers in Human Behavior. 51: 232-238. doi:10.1016/j.chb.2015.04.057. Academic journals ACM Transactions on Computer-Human Interaction Behaviour & Information Technology [1] Interacting with Computers International Journal of Human-Computer Interaction International Journal of Human-Computer Studies Human-Computer Interaction [2] [3] Collection of papers Ronald M. Baecker, Jonathan Grudin, William A. S. Buxton, Saul Greenberg (Eds.) (1995): Readings in human-computer interaction. Toward the Year 2000. 2. ed. Morgan Kaufmann, San Francisco 1995 ISBN 1-55860-246-1 Mithun Ahamed, Developing a Message Interface Architecture for Android Operating Systems, (2015). [4] Treatments by one or few authors, often aimed at a more general audience Jakob Nielsen: Usability Engineering. Academic Press, Boston 1993 ISBN 0-12-518405-0 Donald A. Norman: The Psychology of Everyday Things. Basic Books, New York 1988 ISBN 0-465-06709-3 Jef Raskin: The Humane Interface. New directions for designing interactive systems. Addison-Wesley, Boston 2000 ISBN 0-201-37937-6 Bruce Tognazzini: Tog on Interface. Addison-Wesley, Reading 1991 ISBN 0-201-60842-1 Textbooks Alan Dix, Janet Finlay, Gregory Abowd, and Russell Beale (2003): Human-Computer Interaction. 3rd Edition. Prentice Hall, 2003. http://hcibook.com/e3/ ISBN 0-13-046109-1 Yvonne Rogers, Helen Sharp & Jenny Preece: Interaction Design: Beyond Human-Computer Interaction, 3rd ed. John Wiley & Sons Ltd., 2011 ISBN 0-470-66576-9 Helen Sharp, Yvonne Rogers & Jenny Preece: Interaction Design: Beyond Human-Computer Interaction, 2nd ed. John Wiley & Sons Ltd., 2007 ISBN 0-470-01866-6 Matt Jones (interaction designer) and Gary Marsden (2006). Mobile Interaction Design, John Wiley and Sons Ltd. == External links == Bad Human Factors Designs The HCI Wiki Bibliography with over 100,000 publications. The HCI Bibliography Over 100,000 publications about HCI. Human-Centered Computing Education Digital Library HCI Webliography"
Information visualization,Information visualization,,,"Information visualization or information visualisation is the study of (interactive) visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. However, information visualization differs from scientific visualization: itŠ—ç’ Îés infovis [information visualization] when the spatial representation is chosen, and itŠ—ç’ Îés scivis [scientific visualization] when the spatial representation is given. == Overview == The field of information visualization has emerged from research in human-computer interaction, computer science, graphics, visual design, psychology, and business methods. It is increasingly applied as a critical component in scientific research, digital libraries, data mining, financial data analysis, market studies, manufacturing production control, and drug discovery. Information visualization presumes that visual representations and interaction techniques take advantage of the human eyeŠ—ç’ Îés broad bandwidth pathway into the mind to allow users to see, explore, and understand large amounts of information at once. Information visualization focused on the creation of approaches for conveying abstract information in intuitive ways. Data analysis is an indispensable part of all applied research and problem solving in industry. The most fundamental data analysis approaches are visualization (histograms, scatter plots, surface plots, tree maps, parallel coordinate plots, etc.), statistics (hypothesis test, regression, PCA, etc.), data mining (association mining, etc.), and machine learning methods (clustering, classification, decision trees, etc.). Among these approaches, information visualization, or visual data analysis, is the most reliant on the cognitive skills of human analysts, and allows the discovery of unstructured actionable insights that are limited only by human imagination and creativity. The analyst does not have to learn any sophisticated methods to be able to interpret the visualizations of the data. Information visualization is also a hypothesis generation scheme, which can be, and is typically followed by more analytical or formal analysis, such as statistical hypothesis testing. == History == The modern study of visualization started with computer graphics, which has from its beginning been used to study scientific problems. However, in its early days the lack of graphics power often limited its usefulness. The recent emphasis on visualization started in 1987 with the special issue of Computer Graphics on Visualization in Scientific Computing. Since then there have been several conferences and workshops, co-sponsored by the IEEE Computer Society and ACM SIGGRAPH. They have been devoted to the general topics of data visualisation, information visualization and scientific visualisation, and more specific areas such as volume visualization. In 1786, William Playfair published the first presentation graphics. == Specific methods and techniques == Cladogram (phylogeny) Concept Mapping Dendrogram (classification) Information visualization reference model Graph drawing Heatmap HyperbolicTree Multidimensional scaling Parallel coordinates Problem solving environment Treemapping == Applications == Information visualization insights are being applied in areas such as: scientific research digital libraries data mining information graphics financial data analysis market studies manufacturing production control crime mapping eGovernance and Policy Modeling == Organization == Notable academic and industry laboratories in the field are: Adobe Research IBM Research Google Research Microsoft Research Panopticon Software Scientific Computing and Imaging Institute Tableau Software University of Maryland Human-Computer Interaction Lab Vvi Conferences in this field, ranked by significance in data visualization research, are: IEEE Visualization ACM SIGGRAPH EuroVis Conference on Human Factors in Computing Systems (CHI) Eurographics PacificVis See further: Computer graphics organizations == See also == Computational visualistics Data Presentation Architecture Data visualization Geovisualization Infographics Infonomics Patent visualisation Software visualization Visual analytics List of information graphics software List of countries by economic complexity, example of Treemapping. == References == == Further reading == Ben Bederson and Ben Shneiderman (2003). The Craft of Information Visualization: Readings and Reflections. Morgan Kaufmann. Stuart K. Card, Jock D. Mackinlay and Ben Shneiderman (1999). Readings in Information Visualization: Using Vision to Think, Morgan Kaufmann Publishers. Jeffrey Heer, Stuart K. Card, James Landay (2005). Prefuse: a toolkit for interactive information visualization. In: ACM Human Factors in Computing Systems CHI 2005. Andreas Kerren, John T. Stasko, Jean-Daniel Fekete, and Chris North (2008). Information Visualization - Human-Centered Issues and Perspectives. Volume 4950 of LNCS State-of-the-Art Survey, Springer. Riccardo Mazza (2009). Introduction to Information Visualization, Springer. Spence, Robert Information Visualization: Design for Interaction (2nd Edition), Prentice Hall, 2007, ISBN 0-13-206550-9. Colin Ware (2000). Information Visualization: Perception for design. San Francisco, CA: Morgan Kaufmann. Kawa Nazemi (2014). Adaptive Semantics Visualization Eurographics Association. == External links == Information Visualization at DMOZ"
Search engine indexing,Search engine indexing,,,"Search engine indexing collects, parses, and stores data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process in the context of search engines designed to find web pages on the Internet is web indexing. Popular engines focus on the full-text indexing of online, natural language documents. Media types such as video and audio and graphics are also searchable. Meta search engines reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time. == Indexing == The purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power. For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval. === Index design factors === Major factors in designing a search engines architecture include: Merge factors How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the data collection policy. Search engine index merging is similar in concept to the SQL Merge command and other merge algorithms. Storage techniques How to store the index data, that is, whether information should be data compressed or filtered. Index size How much computer storage is required to support the index. Lookup speed How quickly a word can be found in the Inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science. Maintenance How the index is maintained over time. Fault tolerance How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, partitioning, and schemes such as hash-based or composite partitioning, as well as replication. === Index data structures === Search engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors. Suffix tree Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of trie. Tries support extendable hashing, which is important for search engine indexing. Used for searching for patterns in DNA sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself. An alternate representation is a suffix array, which is considered to require less virtual memory and supports data compression such as the BWT algorithm. Inverted index Stores a list of occurrences of each atomic search criterion, typically in the form of a hash table or binary tree. Citation index Stores citations or hyperlinks between documents to support citation analysis, a subject of Bibliometrics. Ngram index Stores sequences of length of data to support other types of retrieval or text mining. Document-term matrix Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional sparse matrix. === Challenges in parallelism === A major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for race conditions and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or corpus). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a producer-consumer model. The indexer is the producer of searchable information and users are the consumers that need to search. The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engines architecture may involve distributed computing, where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture. === Inverted indices === Many search engines incorporate an inverted index when evaluating a search query to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct access to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index: This index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a boolean index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document. Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of information retrieval. The inverted index is a sparse matrix, since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional array. The index is similar to the term document matrices employed by latent semantic analysis. The inverted index can be considered a form of a hash table. In some cases the index is a form of a binary tree, which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a distributed hash table. === Index merging === The inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing, where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives. After parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is so named because it is an inversion of the forward index. === The forward index === The forward index stores a list of words for each document. The following is a simplified form of the forward index: The rationale behind developing a forward index is that as documents are parsed, it is better to immediately store the words per document. The delineation enables Asynchronous system processing, which partially circumvents the inverted index update bottleneck. The forward index is sorted to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index. === Compression === Generating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on disk. Consider the following scenario for a full text, Internet search engine. It takes 8 bits (or 1 byte) to store a single character. Some encodings use 2 bytes per character The average number of characters in any given word on a page may be estimated at 5 (Wikipedia:Size comparisons) Given this scenario, an uncompressed index (assuming a non-conflated, simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression. Notably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost. == Document parsing == Document parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called tokens, and so, in the context of search engine indexing and natural language processing, parsing is more commonly referred to as tokenization. It is also sometimes called word boundary disambiguation, tagging, text segmentation, content analysis, text analysis, text mining, concordance generation, speech segmentation, lexing, or lexical analysis. The terms indexing, parsing, and tokenization are used interchangeably in corporate slang. Natural language processing is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets. === Challenges in natural language processing === Word Boundary Ambiguity Native English speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a multilingual indexer. In digital form, the texts of other languages such as Chinese, Japanese or Arabic represent a greater challenge, as words are not clearly delineated by whitespace. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax). Language Ambiguity To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its language or lexical category (part of speech). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document. Diverse File Formats In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document. Faulty Storage The quality of the natural language data may not always be perfect. An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol. Binary characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade. === Tokenization === Unlike literate humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not know that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word referred to as a token. Such a program is commonly called a tokenizer or parser or lexer. Many search engines, as well as other natural language processing software, incorporate specialized programs for parsing, such as YACC or Lex. During tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify entities such as email addresses, phone numbers, and URLs. When identifying each token, several characteristics may be stored, such as the tokens case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like noun or verb), position, sentence number, sentence position, length, and line number. === Language recognition === If the search engine supports multiple languages, a common initial step during tokenization is to identify each documents language; many of the subsequent steps are language dependent (such as stemming and part of speech tagging). Language recognition is the process by which a computer program attempts to automatically identify, or categorize, the language of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in natural language processing. Finding which language the words belongs to may involve the use of a language recognition chart. === Format analysis === If the search engine supports multiple document formats, documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content. For example, HTML documents contain HTML tags, which specify formatting information such as new line starts, bold emphasis, and font size or style. If the search engine were to ignore the difference between content and markup, extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include: HTML ASCII text files (a text document without specific computer readable formatting) Adobes Portable Document Format (PDF) PostScript (PS) LaTeX UseNet netnews server formats XML and derivatives like RSS SGML Multimedia meta data formats like ID3 Microsoft Word Microsoft Excel Microsoft PowerPoint IBM Lotus Notes Options for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom parser. Some search engines support inspection of files that are stored in a compressed or encrypted file format. When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported compressed file formats include: ZIP - Zip archive file RAR - Roshal ARchive file CAB - Microsoft Windows Cabinet File Gzip - File compressed with gzip BZIP - File compressed using bzip2 Tape ARchive (TAR), Unix archive file, not (itself) compressed TAR.Z, TAR.GZ or TAR.BZ2 - Unix archive files compressed with Compress, GZIP or BZIP2 Format analysis can involve quality improvement methods to avoid including bad information in the index. Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for spamdexing: Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden div tag in HTML, which may incorporate the use of CSS or JavaScript to do so). Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer. === Section recognition === Some search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages. Many documents on the web, such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted: Content in different sections is treated as related in the index, when in reality it is not Organizational side bar content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents. Section analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not see this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the Noscript tag to ensure that the web page is indexed properly. At the same time, this fact can also be exploited to cause the search engine indexer to see different content than the viewer. === HTML Priority System === Indexing often has to recognize the HTML tags to organize priority. Indexing low priority to high margin to labels like strong and link to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like Google and Bing ensure that the search engine does not take the large texts as relevant source due to strong type system compatibility. === Meta tag indexing === Specific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the meta tag contains keywords which are also included in the index. Earlier Internet search engine technology would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was computer hardware able to support such technology. The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization. As the Internet grew through the 1990s, many brick-and-mortar corporations went online and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to spamdexing, which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many marketing keywords into the content of a webpage before draining it of all interesting and useful information. Given that conflict of interest with the business goal of designing user-oriented websites which were sticky, the customer lifetime value equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies. In Desktop search, many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index. == See also == == References == == Further reading == R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972. Donald E. Knuth. The Art of Computer Programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997. Donald E. Knuth. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998. Gerald Salton. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988. Gerard Salton. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986. Gerard Salton. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968. Gerard Salton. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971. Gerard Salton. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999. G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949. Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962). Edward H. Sussenguth Jr., Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p. 272-279, May 1963 Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 28-43, 1992. Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133-146, 2001. Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003. Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349-379, October 1996, Volume 14, Number 4. Mehlhorn, K.: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984. Mehlhorn, K., Overmars, M.H.: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 93-98, 1981. Mehlhorn, K.: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 1-16, 1981. Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Intl World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp. 175-182) Serge Abiteboul and Victor Vianu. Queries and Computation on the Web. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997. Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994. A. Emtage and P. Deutsch, Archie--An Electronic Directory Service for the Internet. Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp. 93-110. M. Gray, World Wide Web Wanderer. D. Cutting and J. Pedersen. Optimizations for Dynamic Inverted Index Maintenance. Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp. 405-411, September 1990. Stefan B’‘ ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010."
Web search engine,Web search engine,,,"A web search engine is a software system that is designed to search for information on the World Wide Web. The search results are generally presented in a line of results often referred to as search engine results pages (SERPs). The information may be a mix of web pages, images, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. == History == Internet search engines themselves predate the debut of the Web in December 1990. The Who is user search dates back to 1982 and the Knowbot Information Service multi-network user search was first implemented in 1989. The first well documented search engine that searched content files, namely FTP files was Archie, which debuted on 10 September 1990. Prior to September 1993 the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One historical snapshot of the list in 1992 remains, but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title Whats New! The first tool used for searching content (as opposed to users) on the Internet was Archie. The name stands for archive without the v. It was created by Alan Emtage, Bill Heelan and J. Peter Deutsch, computer science students at McGill University in Montreal. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzys Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine Archie Search Engine was not a reference to the Archie comic book series, Veronica and Jughead are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the webs first primitive search engine, released on September 2, 1993. In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called Wandex. The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The webs second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. NCSAs MosaicŠ—ç’£Î¢ - Mosaic (web browser) wasnt the first Web browser. But it was the first to make a major splash. In November 1993, Mosaic v 1.0 broke away from the small pack of existing browsers by including featuresŠ—ç’ ’–like icons, bookmarks, a more attractive interface, and picturesŠ—ç’ ’–that made the software easy to use and appealing to non-geeks. JumpStation (created in December 1993 by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first all text crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the first one widely known by the public. Also in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. Soon after, many search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Yahoo! was among the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscapes web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite. Google adopted the idea of selling search terms in 1998, from a small search engine company named goto.com. This move had a significant effect on the SE business, which went from struggling to one of the most profitable businesses in the internet. Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s. Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in 1999 and ended in 2001. Around 2000, Googles search engine rose to prominence. The company achieved better results for many searches with an innovation called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google. This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomis search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Googles search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999 the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsofts rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. == How web search engines work == A search engine maintains the following processes in near real time: Web crawling Indexing Searching Web search engines get their information by web crawling from site to site. The spider checks for the standard filename robots.txt, addressed to it, before sending certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, as evidenced by the standard HTML markup of the informational content, or its metadata in HTML meta tags. Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are made in a public database, made available for web search queries. A query from a user can be a single word. The index helps find information relating to the query as quickly as possible. Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case the page may differ from the search terms indexed. The cached page holds the appearance of the version whose words were indexed, so a cached version of a page can be useful to the web site when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically when a user enters a query into a search engine it is a few keywords. The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes. Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking Show search tools in the leftmost column of the initial search results page, and then selecting the desired date range. Its also possible to weight by date because each page has a modification time. Most search engines support the use of the boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords. There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for. As well, natural language queries allow the user to type a question in the same form one would ask it to a human. A site like this would be ask.com. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the best results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another. The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an inverted index by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads. == Market share == Google is the worlds most popular search engine, with a market share of 80.52 percent as of March, 2017. The worlds most popular search engines (with >1% market share) are: === East Asia and Russia === In some East Asian countries and Russia, Google is not the most popular search engine. In Russia, Yandex commands a marketshare of 61.9 percent, compared to Googles 28.3 percent. In China, Baidu is the most popular search engine. South Koreas homegrown search portal, Naver, is used for 70 percent of online searches in the country. Yahoo! Japan and Yahoo! Taiwan are the most popular avenues for internet search in Japan and Taiwan, respectively. === Europe === Most countries markets in Western Europe are dominated by Google, except for Czech Republic, where Seznam is a strong competitor. == Search engine bias == Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide and the underlying assumptions about the technology. These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws). For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more popular results. Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries. Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines, and the representation of certain controversial topics in their results, such as terrorism in Ireland and conspiracy theories. == Customized results and filter bubbles == Many search engines such as Google and Bing provide customized results based on the users activity history. This leads to an effect that has been called a filter bubble. The term describes a phenomenon in which websites use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the users past viewpoint, effectively isolating the user in a bubble that tends to exclude contrary information. Prime examples are Googles personalized search results and Facebooks personalized news stream. According to Eli Pariser, who coined the term, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Pariser related an example in which one user searched Google for BP and got investment news about British Petroleum while another searcher got information about the Deepwater Horizon oil spill and that the two search results pages were strikingly different. The bubble effect may have negative implications for civic discourse, according to Pariser. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or bubbling users, such as DuckDuckGo. Other scholars do not share Parisers view, finding the evidence in support of his thesis unconvincing. == Christian, Islamic and Jewish search engines == The global growth of the Internet and electronic media in the Arab and Muslim World during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either halal or haram, based on modern, expert, interpretation of the Law of Islam. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and other). While lack of investment and slow pace in technologies in the Muslim World has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim, a Muslim lifestyle site, did receive millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewgle, the Jewish version of Google, and SeekFind.org, which is Christian. SeekFind filters sites that attack or degrade their faith. == Search engine submission == Search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers, that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web sites record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also add links to websites from their own pages. This could appear helpful in increasing a websites ranking, because external links are one of the most important factors determining a websites ranking. However John Mueller of Google has stated that this can lead to a tremendous number of unnatural links for your site with a negative impact on site ranking. == See also == == References == == Further reading == Steve Lawrence; C. Lee Giles (1999). Accessibility of information on the web. Nature. 400 (6740): 107-9. PMID 10428673. doi:10.1038/21987. Bing Liu (2007), Web Data Mining: Exploring Hyperlinks, Contents and Usage Data. Springer,ISBN 3-540-37881-2 Bar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231-288. Levene, Mark (2005). An Introduction to Search Engines and Web Navigation. Pearson. Hock, Randolph (2007). The Extreme Searchers Handbook. ISBN 978-0-910965-76-7 Javed Mostafa (February 2005). Seeking Better Web Searches. Scientific American. Ross, Nancy; Wolfram, Dietmar (2000). End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine. Journal of the American Society for Information Science. 51 (10): 949-958. doi:10.1002/1097-4571(2000)51:10<949::AID-ASI70>3.0.CO;2-5. Xie, M.; et al. (1998). Quality dimensions of Internet search engines. Journal of Information Science. 24 (5): 365-372. doi:10.1177/016555159802400509. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press. 2010. == External links == Search Engines at DMOZ"
Information retrieval,Information retrieval,,,"Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called information overload. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications. == Overview == An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy. An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata. Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. == History == The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a statistical machine - filed by Emanuel Goldberg in the 1920s and 30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further. == Model types == For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. === First dimension: mathematical basis === Set-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are: Standard Boolean model Extended Boolean model Fuzzy retrieval Algebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. Vector space model Generalized vector space model (Enhanced) Topic-based Vector Space Model Extended Boolean model Latent semantic indexing a.k.a. latent semantic analysis Probabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes theorem are often used in these models. Binary Independence Model Probabilistic relevance model on which is based the okapi (BM25) relevance function Uncertain inference Language models Divergence-from-randomness model Latent Dirichlet allocation Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature. === Second dimension: properties of the model === Models without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables. Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents. Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.) == Performance and correctness measures == The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy. Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks. The mathematical symbols used in the formulas below mean: X Š—ç’ Î© Y { X Y} - Intersection - in this case, specifying the documents in both sets X and Y | X | { |X|} - Cardinality - in this case, the number of documents in set X Š—ç’ ’ö { } - Integral Š—ç’ ’” { } - Summation ’â’– { } - Symmetric difference === Precision === Precision is the fraction of the documents retrieved that are relevant to the users information need. precision = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { retrieved documents } | {={} }}|}{|}}|}}} In binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n. Note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics. === Recall === Recall is the fraction of the documents that are relevant to the query that are successfully retrieved. recall = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { relevant documents } | {={} }}|}{|}}|}}} In binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query. It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision. === Fall-out === The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available: fall-out = | { non-relevant documents } Š—ç’ Î© { retrieved documents } | | { non-relevant documents } | {={} }}|}{|}}|}}} In binary classification, fall-out is closely related to specificity and is equal to ( 1 Š—ç’ ’« specificity ) { (1-{})} . It can be looked at as the probability that a non-relevant document is retrieved by the query. It is trivial to achieve fall-out of 0% by returning zero documents in response to any query. === F-score / F-measure === The weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is: F = 2 Š—ç’“’ p r e c i s i o n Š—ç’“’ r e c a l l ( p r e c i s i o n + r e c a l l ) { F={ }{( + )}}} This is also known as the F 1 { F {1}} measure, because recall and precision are evenly weighted. The general formula for non-negative real ’â { } is: F ’â = ( 1 + ’â 2 ) Š—ç’“’ ( p r e c i s i o n Š—ç’“’ r e c a l l ) ( ’â 2 Š—ç’“’ p r e c i s i o n + r e c a l l ) { F { }={) ( )}{( ^{2} + )}},} Two other commonly used F measures are the F 2 { F {2}} measure, which weights recall twice as much as precision, and the F 0.5 { F {0.5}} measure, which weights precision twice as much as recall. The F-measure was derived by van Rijsbergen (1979) so that F ’â { F { }} measures the effectiveness of retrieval with respect to a user who attaches ’â { } times as much importance to recall as precision. It is based on van Rijsbergens effectiveness measure E = 1 Š—ç’ ’« 1 ’âÎ± P + 1 Š—ç’ ’« ’âÎ± R { E=1-{{{{P}}+{{R}}}}} . Their relationship is: F ’â = 1 Š—ç’ ’« E { F { }=1-E} where ’âÎ± = 1 1 + ’â 2 { ={{1+ ^{2}}}} F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it. === Average precision === Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision p ( r ) { p(r)} as a function of recall r { r} . Average precision computes the average value of p ( r ) { p(r)} over the interval from r = 0 { r=0} to r = 1 { r=1} : AveP = Š—ç’ ’ö 0 1 p ( r ) d r { = {0}^{1}p(r)dr} That is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents: AveP = Š—ç’ ’” k = 1 n P ( k ) ’â’– r ( k ) { = {k=1}^{n}P(k) r(k)} where k { k} is the rank in the sequence of retrieved documents, n { n} is the number of retrieved documents, P ( k ) { P(k)} is the precision at cut-off k { k} in the list, and ’â’– r ( k ) { r(k)} is the change in recall from items k Š—ç’ ’« 1 { k-1} to k { k} . This finite sum is equivalent to: AveP = Š—ç’ ’” k = 1 n ( P ( k ) ’‘’• rel Š—çÎ’ ( k ) ) number of relevant documents { ={^{n}(P(k) (k))}{}}!} where rel Š—çÎ’ ( k ) { (k)} is an indicator function equaling 1 if the item at rank k { k} is a relevant document, zero otherwise. Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero. Some authors choose to interpolate the p ( r ) { p(r)} function to reduce the impact of wiggles in the curve. For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}: AveP = 1 11 Š—ç’ ’” r Š—ç’ ’ { 0 , 0.1 , Š—ç’ , 1.0 } p interp ( r ) { ={{11}} {r }p { }(r)} where p interp ( r ) { p { }(r)} is an interpolated precision that takes the maximum precision over all recalls greater than r { r} : p interp ( r ) = max r ~ : r ~ Š—ç’ Î‚ r Š—çÎ’ p ( r ~ ) { p { }(r)= {{}:{} r}p({})} . An alternative is to derive an analytical p ( r ) { p(r)} function by assuming a particular parametric distribution for the underlying decision values. For example, a binormal precision-recall curve can be obtained by assuming decision values in both classes to follow a Gaussian distribution. === Precision at K === For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or Precision at 10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k. Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1. It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not. === R-Precision === R-precision requires knowing all documents that are relevant to a query. The number of relevant documents, R { R} , is used as the cutoff for calculation, and this varies from query to query. For example, if there are 15 documents relevant to red in a corpus (R=15), R-precision for red looks at the top 15 documents returned, counts the number that are relevant r { r} turns that into a relevancy fraction: r / R = r / 15 { r/R=r/15} . Precision is equal to recall at the R-th position. Empirically, this measure is often highly correlated to mean average precision. === Mean average precision === Mean average precision for a set of queries is the mean of the average precision scores for each query. MAP = Š—ç’ ’” q = 1 Q A v e P ( q ) Q { ={^{Q} }{Q}}!} where Q is the number of queries. === Discounted cumulative gain === DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The DCG accumulated at a particular rank position p { p} is defined as: D C G p = r e l 1 + Š—ç’ ’” i = 2 p r e l i log 2 Š—çÎ’ i . { } =rel {1}+ {i=2}^{p}{}{ {2}i}}.} Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( I D C G p { IDCG {p}} ), which normalizes the score: n D C G p = D C G p I D C G p . { } ={}{IDCG{p}}}.} The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the D C G p { DCG {p}} will be the same as the I D C G p { IDCG {p}} producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable. === Other measures === Mean reciprocal rank Spearmans rank correlation coefficient bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents GMAP - geometric mean of (per-topic) average precision Measures based on marginal relevance and document diversity - see Relevance (information retrieval) ’ÇÎ Problems and alternatives === Visualization === Visualizations of information retrieval performance include: Graphs which chart precision on one axis and recall on the other Histograms of average precision over various topics Receiver operating characteristic (ROC curve) Confusion matrix == Timeline == Before the 1900s 1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations. 1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium. 1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data. 1920s-1930s Emanuel Goldberg submits patents for his Statistical MachineŠ—ç’ Î a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents. 1940s-1950s late 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans. 1945: Vannevar Bushs As We May Think appeared in Atlantic Monthly. 1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds. 1950s: Growing concern in the US for a science gap with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield). 1950: The term information retrieval was coined by Calvin Mooers. 1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT. 1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed framework for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved. 1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959) 1959: Hans Peter Luhn published Auto-encoding of documents for information retrieval. 1960s: early 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell. 1960: Melvin Earl Maron and John Lary Kuhns published On relevance, probabilistic indexing, and information retrieval in the Journal of the ACM 7(3):216-244, July 1960. 1962: Cyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems. Cranfield Collection of Aeronautics, Cranfield, England, 1962. Kent published Information Analysis and Retrieval. 1963: Weinberg report Science, Government and Information gave a full articulation of the idea of a crisis of scientific information. The report was named after Dr. Alvin Weinberg. Joseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963). 1964: Karen Sp’‘ rck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR. The National Bureau of Standards sponsored a symposium titled Statistical Association Methods for Mechanized Documentation. Several highly significant papers, including G. Saltons first published reference (we believe) to the SMART system. mid-1960s: National Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system. Project Intrex at MIT. 1965: J. C. R. Licklider published Libraries of the Future. 1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs. late 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval. 1968: Gerard Salton published Automatic Information Organization and Retrieval. John W. Sammon, Jr.s RADC Tech report Some Mathematics of Information Storage and Retrieval... outlined the vector model. 1969: Sammons A nonlinear mapping for data structure analysis (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system. 1970s early 1970s: First online systemsŠ—ç’ ’–NLMs AIM-TWX, MEDLINE; Lockheeds Dialog; SDCs ORBIT. Theodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines. 1971: Nicholas Jardine and Cornelis J. van Rijsbergen published The use of hierarchic clustering in information retrieval, which articulated the cluster hypothesis. 1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model: A Theory of Indexing (Society for Industrial and Applied Mathematics) A Theory of Term Importance in Automatic Text Analysis (JASIS v. 26) A Vector Space Model for Automatic Indexing (CACM 18:11) 1978: The First ACM SIGIR conference. 1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models. 1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback. 1980s 1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge. 1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing. 1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models. 1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System mid-1980s: Efforts to develop end-user versions of commercial IR systems. 1985-1993: Key papers on and experimental systems for visualization interfaces. Work by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others. 1989: First World Wide Web proposals by Tim Berners-Lee at CERN. 1990s 1992: First TREC conference. 1997: Publication of Korfhages Information Storage and Retrieval with emphasis on visualization and multi-reference point systems. late 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models. == Major Conferences == SIGIR: Conference on Research and Development in Information Retrieval ECIR: European Conference on Information Retrieval CIKM: Conference on Information and Knowledge Management WWW: International World Wide Web Conference WSDM: Conference on Web Search and Data Mining ICTIR: International Conference on Theory of Information Retrieval == Awards in the field == Tony Kent Strix award Gerard Salton Award == Leading IR Research Groups == Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts Amherst Information Retrieval Group at the University of Glasgow Information and Language Processing Systems (ILPS) at the University of Amsterdam == See also == == References == == Further reading == Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch’‘ tze. Introduction to Information Retrieval. Cambridge University Press, 2008. Stefan B’‘ ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010. == External links == ACM SIGIR: Information Retrieval Special Interest Group BCS IRSG: British Computer Society - Information Retrieval Specialist Group Text Retrieval Conference (TREC) Forum for Information Retrieval Evaluation (FIRE) Information Retrieval (online book) by C. J. van Rijsbergen Information Retrieval Wiki Information Retrieval Facility Information Retrieval @ DUTH TREC report on information retrieval evaluation techniques How eBay measures search relevance Information retrieval performance evaluation tool @ Athena Research Centre"
Human-computer information retrieval,Human-computer information retrieval,,,"Human-computer information retrieval (HCIR) is the study and engineering of information retrieval techniques that bring human intelligence into the search process. It combines the fields of human-computer interaction (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback. == History == This term human-computer information retrieval was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006. MarchioniniŠ—ç’ Îés main thesis is that HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy. In 1996 and 1998, a pair of workshops at the University of Glasgow on information retrieval and human-computer interaction sought to address the overlap between these two fields. Marchionini notes the impact of the World Wide Web and the sudden increase in information literacy - changes that were only embryonic in the late 1990s. A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the University of Maryland Human-Computer Interaction Lab in 2005, alternates between the Association for Computing Machinery Special Interest Group on Information Retrieval (SIGIR) and Special Interest Group on Computer-Human Interaction (CHI) conferences. Also in 2005, the European Science Foundation held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the Massachusetts Institute of Technology. == Description == HCIR includes various aspects of IR and HCI. These include exploratory search, in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system. A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users. Most modern IR systems employ a ranked retrieval model, in which the documents are scored based on the probability of the documents relevance to the query. In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their mean average precision over a set of benchmark queries from organizations like the Text Retrieval Conference (TREC). Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models - one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkins 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of precision and recall but apply them to the results of multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlunds IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc. == Goals == HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results. Systems should no longer only deliver the relevant documents, but must also provide semantic information along with those documents increase user responsibility as well as control; that is, information systems require human intellectual effort have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases aim to be part of information ecology of personal and shared memories and tools rather than discrete standalone services support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase support tuning by end users and especially by information professionals who add value to information resources be engaging and fun to use In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process Š—ç’ ’– the information professional Š—ç’ ’– to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs). == Techniques == The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift. Many search engines have features that incorporate HCIR techniques. Spelling suggestions and automatic query reformulation provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the userŠ—ç’ Îés hands. Faceted search enables users to navigate information hierarchically, going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging. Faceted navigation, like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking. Lookahead provides a general approach to penalty-free exploration. For example, various web applications employ AJAX to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., metadata about the objects) and snippets of document text that are most pertinent to the words in the search query. Relevance feedback allows users to guide an IR system by indicating whether particular results are more or less relevant. Summarization and analytics help users digest the results that come back from the query. Summarization here is intended to encompass any means of aggregating or compressing the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is clustering, which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for java might return clusters for Java (programming language), Java (island), or Java (coffee). Visual representation of data is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of information visualization that allow users access to summary views of search results include tag clouds and treemapping. == Related Areas == Exploratory Video Search == References == == External links == Workshops on Human Computer Information Retrieval. ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)."
Lemmatisation,Lemmatisation,,,"Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the words lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research. == Description == In many languages, words appear in several inflected forms. For example, in English, the verb to walk may appear as walk, walked, walks, walking. The base form, walk, that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word. Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster. The reduced accuracy may not matter for some applications. In fact, when used within information retrieval systems, stemming improves query recall accuracy, or true positive rate, when compared to lemmatisation. Nonetheless, stemming reduces precision, or true negative rate, for such systems. For instance: The word better has good as its lemma. This link is missed by stemming, as it requires a dictionary look-up. The word walk is the base form for word walking, and hence this is matched in both stemming and lemmatisation. The word meeting can be either the base form of a noun or a form of a verb (to meet) depending on the context; e.g., in our last meeting or We are meeting again tomorrow. Unlike stemming, lemmatisation attempts to select the correct lemma depending on the context. Document indexing software like Lucene can store the base stemmed format of the word without the knowledge of meaning, but only considering word formation grammar rules. The stemmed word itself might not be a valid word: lazy, as seen in the example below, is stemmed by many stemmers to lazi. This is because the purpose of stemming is not to produce the appropriate lemma - that is a more challenging task that requires knowledge of context. The main purpose of stemming is to map different forms of a word to a single form. As a rules-based algorithm, dependent only upon the spelling of a word, it sacrifices accuracy to ensure that, for example, when laziness is stemmed to lazi, it has the same stem as lazy. == Use in biomedicine == Morphological analysis of published biomedical literature can yield useful results. Morphological processing of biomedical text can be more effective by a specialised lemmatisation program for biomedicine, and may improve the accuracy of practical information extraction tasks. == See also == Canonicalization == References == == External links =="
Stemming,Stemming,,,"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formŠ—ç’ ’–generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation. Stemming programs are commonly referred to as stemming algorithms or stemmers. == Examples == A stemmer for English, for example, should identify the string cats (and possibly catlike, catty etc.) as based on the root cat, and stems, stemmer, stemming, stemmed as based on stem. A stemming algorithm reduces the words fishing, fished, and fisher to the root word, fish. On the other hand, argue, argued, argues, arguing, and argus reduce to the stem argu (illustrating the case where the stem is not itself a word or root) but argument and arguments reduce to the stem argument. == History == The first published stemmer was written by Julie Beth Lovins in 1968. This paper was remarkable for its early date and had great influence on later work in this area. A later stemmer was written by Martin Porter and was published in the July 1980 issue of the journal Program. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the Tony Kent Strix award in 2000 for his work on stemming and information retrieval. Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official free software (mostly BSD-licensed) implementation of the algorithm around the year 2000. He extended this work over the next few years by building Snowball, a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages. == Algorithms == There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome. A simple stemmer looks up the inflected form in a lookup table. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root. A lookup approach may use preliminary part-of-speech tagging to avoid overstemming. === The production technique === The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is run, then the inverted algorithm might automatically generate the forms running, runs, runned, and runly. The last two forms are valid constructions, but they are unlikely. === Suffix-stripping algorithms === Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of rules is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include: if the word ends in ed, remove the ed if the word ends in ing, remove the ing if the word ends in ly, remove the ly Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like ran and run). The solutions produced by suffix stripping algorithms are limited to those lexical categories which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge. Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing. ==== Additional algorithm criteria ==== Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules. It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term friendlies, the algorithm may identify the ies suffix and apply the appropriate rule and achieve the result of friendl. friendl is likely not found in the lexicon, and therefore the rule is rejected. One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ies with y. How this affects the algorithm varies on the algorithms design. To illustrate, the algorithm may identify that both the ies suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, friendlies becomes friendly instead of friendl. Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term friendly, where the ly stripping rule is likely identified and accepted. In summary, friendlies becomes (via substitution) friendly which becomes (via stripping) friend. This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for friendlies in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form friend. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best. === Lemmatisation algorithms === A more complex approach to the problem of determining a stem of a word is lemmatisation. This process involves first determining the part of speech of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a words part of speech. This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem). === Stochastic algorithms === Stochastic algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they learn) on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured). Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form. === n-gram analysis === Some stemming techniques use the n-gram context of a word to choose the correct stem for a word. === Hybrid approaches === Hybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of frequent exceptions like ran => run. If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result. === Affix stemmers === In linguistics, the term affix refers to either a prefix or a suffix. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word indefinitely, identify that the leading in is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name affix stripping. A study of affix stemming for several European languages can be found here. === Matching algorithms === Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the brows in browse and in browsing). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix be, which is the stem of such words as be, been and being, would not be considered as the stem of the word beside). == Language challenges == While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated. Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as dries being the third-person singular present form of the verb dry, axes being the plural of axe as well as axis); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun declensions), a Hebrew one is even more complex (due to nonconcatenative morphology, a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on. === Multilingual stemming === Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist. == Error metrics == There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have beenŠ—ç’ ’–a false positive. Understemming is an error where two separate inflected words should be stemmed to the same root, but are notŠ—ç’ ’–a false negative. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other. For example, the widely used Porter stemmer stems universal, university, and universe to univers. This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results. An example of understemming in the Porter stemmer is alumnus Š—çÎ¾’« alumnu, alumni Š—çÎ¾’« alumni, alumna/alumnae Š—çÎ¾’« alumna. This English word keeps Latin morphology, and so these near-synonyms are not conflated. == Applications == Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning daffodils is probably closely related to a text mentioning daffodil (without the s). But in some cases, words with the same morphological stem have idiomatic meanings which are not closely related: a user searching for marketing will not be satisfied by most documents mentioning markets but not marketing. === Information retrieval === Stemmers are common elements in query systems such as Web search engines. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early information retrieval researchers to deem stemming irrelevant in general. An alternative approach, based on searching for n-grams rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English. === Domain analysis === Stemming is used to determine domain vocabularies in domain analysis. === Use in commercial products === Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages. The Snowball stemmers have been compared with commercial lexical stemmers with varying results. Google search adopted word stemming in 2003. Previously a search for fish would not have returned fishing. Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find fish in fishing but when searching for fishes will not find occurrences of the word fish. == See also == Root (linguistics) - linguistic definition of the term root Stem (linguistics) - linguistic definition of the term stem Morphology (linguistics) Lemma (morphology) - linguistic definition Lemmatization Lexeme Inflection Derivation - stemming is a form of reverse derivation Natural language processing - stemming is generally regarded as a form of NLP Text mining - stemming algorithms play a major role in commercial NLP software Computational linguistics Snowball (programming language) - designed for creating stemming algorithms == References == == Further reading == == External links == Apache OpenNLP includes Porter and Snowball stemmers SMILE Stemmer - free online service, includes Porter and Paice/Husk Lancaster stemmers (Java API) Themis - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API) Snowball - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages Snowball on C# - port of Snowball stemmers for C# (14 languages) Python bindings to Snowball API Ruby-Stemmer - Ruby extension to Snowball API PECL - PHP extension to the Snowball API Oleander Porters algorithm - stemming library in C++ released under BSD Unofficial home page of the Lovins stemming algorithm - with source code in a couple of languages Official home page of the Porter stemming algorithm - including source code in several languages Official home page of the Lancaster stemming algorithm - Lancaster University, UK Official home page of the UEA-Lite Stemmer - University of East Anglia, UK Overview of stemming algorithms PTStemmer - A Java/Python/.Net stemming toolkit for the Portuguese language jsSnowball - open source JavaScript implementation of Snowball stemming algorithms for many languages Snowball Stemmer - implementation for Java hindi stemmer - open source stemmer for Hindi czech stemmer - open source stemmer for Czech Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers Tamil Stemmer This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the relicensing terms of the GFDL, version 1.3 or later."
Vector space model,Vector space model,,,"Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System. == Definitions == Documents and queries are represented as vectors. d j = ( w 1 , j , w 2 , j , Š—ç’ , w t , j ) { d {j}=(w {1,j},w {2,j}, ,w {t,j})} q = ( w 1 , q , w 2 , q , Š—ç’ , w n , q ) { q=(w {1,q},w {2,q}, ,w {n,q})} Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below). The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). Vector operations can be used to compare documents with queries. == Applications == Relevance rankings of documents in a keyword search can be calculated, using the assumptions of document similarities theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents. In practice, it is easier to calculate the cosine of the angle between the vectors, instead of the angle itself: cos Š—çÎ’ ’â’ = d 2 Š—ç’“’ q Š—ç’ Î‚ d 2 Š—ç’ Î‚ Š—ç’ Î‚ q Š—ç’ Î‚ { ={} }{|} || |}}} Where d 2 Š—ç’“’ q { } } is the intersection (i.e. the dot product) of the document (d2 in the figure to the right) and the query (q in the figure) vectors, Š—ç’ Î‚ d 2 Š—ç’ Î‚ { |} |} is the norm of vector d2, and Š—ç’ Î‚ q Š—ç’ Î‚ { | |} is the norm of vector q. The norm of a vector is calculated as such: Š—ç’ Î‚ q Š—ç’ Î‚ = Š—ç’ ’” i = 1 n q i 2 { | |={^{n}q {i}^{2}}}} As all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are orthogonal and have no match (i.e. the query term does not exist in the document being considered). See cosine similarity for further information. == Example: tf-idf weights == In the classic vector space model proposed by Salton, Wong and Yang the term-specific weights in the document vectors are products of local and global parameters. The model is known as term frequency-inverse document frequency model. The weight vector for document d is v d = [ w 1 , d , w 2 , d , Š—ç’ , w N , d ] T { {d}=[w {1,d},w {2,d}, ,w {N,d}]^{T}} , where w t , d = t f t , d Š—ç’“’ log Š—çÎ’ | D | | { d Š—ç’ Š—ç’ ’ D | t Š—ç’ ’ d Š—ç’ } | { w {t,d}= {t,d} {||}}} and t f t , d { {t,d}} is term frequency of term t in document d (a local parameter) log Š—çÎ’ | D | | { d Š—ç’ Š—ç’ ’ D | t Š—ç’ ’ d Š—ç’ } | { {||}}} is inverse document frequency (a global parameter). | D | { |D|} is the total number of documents in the document set; | { d Š—ç’ Š—ç’ ’ D | t Š—ç’ ’ d Š—ç’ } | { ||} is the number of documents containing the term t. Using the cosine the similarity between document dj and query q can be calculated as: s i m ( d j , q ) = d j Š—ç’“’ q Š—ç’ Î‚ d j Š—ç’ Î‚ Š—ç’ Î‚ q Š—ç’ Î‚ = Š—ç’ ’” i = 1 N w i , j w i , q Š—ç’ ’” i = 1 N w i , j 2 Š—ç’ ’” i = 1 N w i , q 2 { (d {j},q)={} }{|} || |}}={^{N}w {i,j}w {i,q}}{{^{N}w {i,j}^{2}}}{^{N}w {i,q}^{2}}}}}} == Advantages == The vector space model has the following advantages over the Standard Boolean model: Simple model based on linear algebra Term weights not binary Allows computing a continuous degree of similarity between queries and documents Allows ranking documents according to their possible relevance Allows partial matching Most of these advantages are a consequence of the difference in the density of the document collection representation between Boolean and tf-idf approaches. When using Boolean weights, any document lies in a vertex in a n-dimensional hypercube. Therefore, the possible document representations are 2 n { 2^{n}} and the maximum Euclidean distance between pairs is n {}} . As documents are added to the document collection, the region defined by the hypercubes vertices become more populated and hence denser. Unlike Boolean, when a document is added using tf-idf weights, the idfs of the terms in the new document decrease while that of the remaining terms increase. In average, as documents are added, the region where documents lie expands regulating the density of the entire collection representation. This behavior models the original motivation of Salton and his colleagues that a document collection represented in a low density region could yield better retrieval results. == Limitations == The vector space model has the following limitations: Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality) Search keywords must precisely match document terms; word substrings might result in a false positive match Semantic sensitivity; documents with similar context but different term vocabulary wont be associated, resulting in a false negative match. The order in which the terms appear in the document is lost in the vector space representation. Theoretically assumed terms are statistically independent. Weighting is intuitive but not very formal. Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as singular value decomposition and lexical databases such as WordNet. == Models based on and extending the vector space model == Models based on and extending the vector space model include: Generalized vector space model Latent semantic analysis Term Discrimination Rocchio Classification Random Indexing == Software that implements the vector space model == The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them. === Free open source software === Apache Lucene. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. Gensim is a Python+NumPy framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for Tf-idf, Latent Semantic Indexing, Random Projections and Latent Dirichlet Allocation. Weka. Weka is a popular data mining package for Java including WordVectors and Bag Of Words models. == Further reading == G. Salton, A. Wong, and C. S. Yang (1975), A Vector Space Model for Automatic Indexing, Communications of the ACM, vol. 18, nr. 11, pages 613-620. (Article in which a vector space model was presented) David Dubin (2004), The Most Influential Paper Gerard Salton Never Wrote (Explains the history of the Vector Space Model and the non-existence of a frequently cited publication) Description of the vector space model Description of the classic vector space model by Dr E. Garcia Relationship of vector space search to the k-Nearest Neighbor search == See also == Bag-of-words model Compound term processing Conceptual space Eigenvalues and eigenvectors Inverted index Nearest neighbor search Sparse distributed memory w-shingling == References =="
Generalized vector space model,Generalized vector space model,,,"The Generalized vector space model is a generalization of the vector space model used in information retrieval. Wong et al. presented an analysis of the problems that the pairwise orthogonality assumption of the vector space model (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM). == Definitions == GVSM introduces term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector ti was expressed as a linear combination of 2n vectors mr where r = 1...2n. For a document dk and a query q the similarity function now becomes: s i m ( d k , q ) = Š—ç’ ’” j = 1 n Š—ç’ ’” i = 1 n w i , k Š—ç’ ’• w g m a j , q Š—ç’ ’• t i Š—ç’“’ t j Š—ç’ ’” i = 1 n w i , k 2 Š—ç’ ’• Š—ç’ ’” i = 1 n w i , q 2 { sim(d {k},q)={^{n} {i=1}^{n}w {i,k}*wgma {j,q}*t {i} t {j}}{{^{n}w {i,k}^{2}}}*{^{n}w {i,q}^{2}}}}}} where ti and tj are now vectors of a 2n dimensional space. Term correlation t i Š—ç’“’ t j { t {i} t {j}} can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence and the output is the term correlation between any pair of index terms. == Semantic information on GVSM == There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model: compute semantic correlations between terms compute frequency co-occurrence statistics from large corpora Recently Tsatsaronis focused on the first approach. They measure semantic relatedness (SR) using a thesaurus (O) like WordNet. It considers the path length, captured by compactness (SCM), and the path depth, captured by semantic path elaboration (SPE). They estimate the t i Š—ç’“’ t j { t {i} t {j}} inner product by: t i Š—ç’“’ t j = S R ( ( t i , t j ) , ( s i , s j ) , O ) { t {i} t {j}=SR((t {i},t {j}),(s {i},s {j}),O)} where si and sj are senses of terms ti and tj respectively, maximizing S C M Š—ç’“’ S P E { SCM SPE} . Building also on the first approach, Waitelonis et. al. have computed semantic relatedness from Linked Open Data resources including DBpedia as well as the YAGO taxonomy. Thereby they exploits taxonomic relationships among semantic entities in documents and queries after named entity linking. == References =="
Bias-variance tradeoff,Bias-variance tradeoff,,,"In statistics and machine learning, the bias-variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs. The bias-variance decomposition is a way of analyzing a learning algorithms expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning. == Motivation == The bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that dont tend to overfit, but may underfit their training data, failing to capture important regularities. Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set. == Bias-variance decomposition of squared error == Suppose that we have a training set consisting of a set of points x 1 , Š—ç’ , x n { x {1}, ,x {n}} and real values y i { y {i}} associated with each point x i { x {i}} . We assume that there is a function with noise y = f ( x ) + ’ŒÎµ { y=f(x)+ } , where the noise, ’ŒÎµ { } , has zero mean and variance ’Œ’Ü 2 { ^{2}} . We want to find a function f ^ ( x ) {}(x)} , that approximates the true function f ( x ) { f(x)} as well as possible, by means of some learning algorithm. We make as well as possible precise by measuring the mean squared error between y { y} and f ^ ( x ) {}(x)} : we want ( y Š—ç’ ’« f ^ ( x ) ) 2 { (y-{}(x))^{2}} to be minimal, both for x 1 , Š—ç’ , x n { x {1}, ,x {n}} and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the y i { y {i}} contain noise ’ŒÎµ { } ; this means we must be prepared to accept an irreducible error in any function we come up with. Finding an f ^ {}} that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function f ^ {}} we select, we can decompose its expected error on an unseen sample x { x} as follows: E [ ( y Š—ç’ ’« f ^ ( x ) ) 2 ] = B i a s [ f ^ ( x ) ] 2 + V a r [ f ^ ( x ) ] + ’Œ’Ü 2 { { [}{ (}y-{}(x){ )}^{2}{ ]}&= { [}{}(x){ ]}^{2}+ { [}{}(x){ ]}+ ^{2}}} Where: B i a s [ f ^ ( x ) ] = E [ f ^ ( x ) Š—ç’ ’« f ( x ) ] { { [}{}(x){ ]}= { [}{}(x)-f(x){ ]}}} and V a r [ f ^ ( x ) ] = E [ f ^ ( x ) 2 ] Š—ç’ ’« E [ f ^ ( x ) ] 2 { {}(x){ ]}= [{}(x)^{2}]- [{}(x)]^{2}}} The expectation ranges over different choices of the training set x 1 , Š—ç’ , x n , y 1 , Š—ç’ , y n { x {1},,y {1}, ,y {n}} , all sampled from the same joint distribution P ( x , y ) { P(x,y)} . The three terms represent: the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function f ( x ) { f(x)} using a learning method for linear models, there will be error in the estimates f ^ ( x ) {}(x)} due to this assumption; the variance of the learning method, or, intuitively, how much the learning method f ^ ( x ) {}(x)} will move around its mean; the irreducible error ’Œ’Ü 2 { ^{2}} . Since all three terms are non-negative, this forms a lower bound on the expected error on unseen samples. The more complex the model f ^ ( x ) {}(x)} is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model move more to capture the data points, and hence its variance will be larger. === Derivation === The derivation of the bias-variance decomposition for squared error proceeds as follows. For notational convenience, abbreviate f = f ( x ) { f=f(x)} and f ^ = f ^ ( x ) {}={}(x)} . First, recall that, by definition, for any random variable X { X} , we have V a r [ X ] = E [ X 2 ] Š—ç’ ’« E [ X ] 2 { [X]= [X^{2}]- [X]^{2}}} Rearranging, we get: E [ X 2 ] = V a r [ X ] + E [ X ] 2 { [X^{2}]= [X]+ [X]^{2}}} Since f { f} is deterministic E [ f ] = f { [f]=f}} . This, given y = f + ’ŒÎµ { y=f+ } and E [ ’ŒÎµ ] = 0 { [ ]=0} , implies E [ y ] = E [ f + ’ŒÎµ ] = E [ f ] = f { [y]= [f+ ]= [f]=f} . Also, since V a r [ ’ŒÎµ ] = ’Œ’Ü 2 { [ ]= ^{2}} V a r [ y ] = E [ ( y Š—ç’ ’« E [ y ] ) 2 ] = E [ ( y Š—ç’ ’« f ) 2 ] = E [ ( f + ’ŒÎµ Š—ç’ ’« f ) 2 ] = E [ ’ŒÎµ 2 ] = V a r [ ’ŒÎµ ] + E [ ’ŒÎµ ] 2 = ’Œ’Ü 2 { [y]= [(y- [y])^{2}]= [(y-f)^{2}]= [(f+ -f)^{2}]= [ ^{2}]= [ ]+ [ ]^{2}= ^{2}}} Thus, since ’ŒÎµ { } and f ^ {}} are independent, we can write E [ ( y Š—ç’ ’« f ^ ) 2 ] = E [ y 2 + f ^ 2 Š—ç’ ’« 2 y f ^ ] = E [ y 2 ] + E [ f ^ 2 ] Š—ç’ ’« E [ 2 y f ^ ] = V a r [ y ] + E [ y ] 2 + V a r [ f ^ ] + E [ f ^ ] 2 Š—ç’ ’« 2 f E [ f ^ ] = V a r [ y ] + V a r [ f ^ ] + ( f 2 Š—ç’ ’« 2 f E [ f ^ ] + E [ f ^ ] 2 ) = V a r [ y ] + V a r [ f ^ ] + ( f Š—ç’ ’« E [ f ^ ] ) 2 = ’Œ’Ü 2 + V a r [ f ^ ] + B i a s [ f ^ ] 2 { { [}(y-{})^{2}{ ]}&= [y^{2}+{}^{2}-2y{}]&= [y^{2}]+ [{}^{2}]- [2y{}]&= [y]+ [y]^{2}+ [{}]+ [{}]^{2}-2f [{}]&= [y]+ [{}]+(f^{2}-2f [{}]+ [{}]^{2})&= [y]+ [{}]+(f- [{}])^{2}&= ^{2}+ [{}]+ [{}]^{2}}} == Application to regression == The bias-variance decomposition forms the conceptual basis for regression regularization methods such as Lasso and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the OLS solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance. == Application to classification == The bias-variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), its possible to find a similar decomposition. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before. == Approaches == Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.: (Generalized) linear models can be regularized to decrease their variance at the cost of increasing their bias. In artificial neural networks, the variance increases and the bias decreases with the number of hidden units. Like in GLMs, regularization is typically applied. In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below). In Instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars. In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance. One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many weak (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines strong learners in a way that reduces their variance. === K-nearest neighbors === In the case of k-nearest neighbors regression, a closed-form expression exists that relates the bias-variance decomposition to the parameter k: E [ ( y Š—ç’ ’« f ^ ( x ) ) 2 ] = ( f ( x ) Š—ç’ ’« 1 k Š—ç’ ’” i = 1 k f ( N i ( x ) ) ) 2 + ’Œ’Ü 2 k + ’Œ’Ü 2 { [(y-{}(x))^{2}]=(f(x)-{{k}} {i=1}^{k}f(N {i}(x)))^{2}+{}{k}}+ ^{2}} where N 1 ( x ) , Š—ç’ , N k ( x ) { N {1}(x), ,N {k}(x)} are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under reasonable assumptions the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity. == Application to human learning == While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations. Geman et al. argue that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of Š—ç’ ’hard wiringŠ—ç’ Î that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance. == See also == == References == == External links == Fortmann-Roe, Scott (June 2012). Understanding the Bias-Variance Tradeoff."
Supervised learning,Supervised learning,,,"Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias). The parallel task in human and animal psychology is often referred to as concept learning. == Overview == In order to solve a given problem of supervised learning, one has to perform the following steps: Determine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting. Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements. Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output. Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support vector machines or decision trees. Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation. Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set. A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem). There are four major issues to consider in supervised learning: === Bias-variance tradeoff === A first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input x { x} if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for x { x} . A learning algorithm has high variance for a particular input x { x} if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be flexible so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust). === Function complexity and amount of training data === The second issue is the amount of training data available relative to the complexity of the true function (classifier or regression function). If the true function is simple, then an inflexible learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a flexible learning algorithm with low bias and high variance. === Dimensionality of the input space === A third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many extra dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. === Noise in the output values === A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled corrupts your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator. In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance. === Other factors to consider (important) === Other factors to consider when choosing and applying a learning algorithm include the following: Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including Support Vector Machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data. Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization. Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, Support Vector Machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them. When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms. The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron). == How supervised learning algorithms work == Given a set of N { N} training examples of the form { ( x 1 , y 1 ) , . . . , ( x N , y N ) } { ,y {1}),...,(x {N},;y {N})}} such that x i { x {i}} is the feature vector of the i-th example and y i { y {i}} is its label (i.e., class), a learning algorithm seeks a function g : X Š—çÎ¾’« Y { g:X Y} , where X { X} is the input space and Y { Y} is the output space. The function g { g} is an element of some space of possible functions G { G} , usually called the hypothesis space. It is sometimes convenient to represent g { g} using a scoring function f : X ’‘’• Y Š—çÎ¾’« R { f:X Y}} such that g { g} is defined as returning the y { y} value that gives the highest score: g ( x ) = arg Š—çÎ’ max y f ( x , y ) { g(x)={{ }};f(x,y)} . Let F { F} denote the space of scoring functions. Although G { G} and F { F} can be any space of functions, many learning algorithms are probabilistic models where g { g} takes the form of a conditional probability model g ( x ) = P ( y | x ) { g(x)=P(y|x)} , or f { f} takes the form of a joint probability model f ( x , y ) = P ( x , y ) { f(x,y)=P(x,y)} . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model. There are two basic approaches to choosing f { f} or g { g} : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimize includes a penalty function that controls the bias/variance tradeoff. In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, ( x i , y i ) { (x {i},;y {i})} . In order to measure how well a function fits the training data, a loss function L : Y ’‘’• Y Š—çÎ¾’« R Š—ç’ Î‚ 0 { L:Y Y}^{ 0}} is defined. For training example ( x i , y i ) { (x {i},;y {i})} , the loss of predicting the value y ^ {}} is L ( y i , y ^ ) { L(y {i},{})} . The risk R ( g ) { R(g)} of function g { g} is defined as the expected loss of g { g} . This can be estimated from the training data as R e m p ( g ) = 1 N Š—ç’ ’” i L ( y i , g ( x i ) ) { R {emp}(g)={{N}} {i}L(y {i},g(x {i}))} . === Empirical risk minimization === In empirical risk minimization, the supervised learning algorithm seeks the function g { g} that minimizes R ( g ) { R(g)} . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find g { g} . When g { g} is a conditional probability distribution P ( y | x ) { P(y|x)} and the loss function is the negative log likelihood: L ( y , y ^ ) = Š—ç’ ’« log Š—çÎ’ P ( y | x ) { L(y,{})=- P(y|x)} , then empirical risk minimization is equivalent to maximum likelihood estimation. When G { G} contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting. === Structural risk minimization === Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occams razor that prefers simpler functions over more complex ones. A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function g { g} is a linear function of the form g ( x ) = Š—ç’ ’” j = 1 d ’â j x j { g(x)= {j=1}^{d} {j}x {j}} . A popular regularization penalty is Š—ç’ ’” j ’â j 2 { {j} {j}^{2}} , which is the squared Euclidean norm of the weights, also known as the L 2 { L {2}} norm. Other norms include the L 1 { L {1}} norm, Š—ç’ ’” j | ’â j | { {j}| {j}|} , and the L 0 { L {0}} norm, which is the number of non-zero ’â j { {j}} s. The penalty will be denoted by C ( g ) { C(g)} . The supervised learning optimization problem is to find the function g { g} that minimizes J ( g ) = R e m p ( g ) + ’â’ C ( g ) . { J(g)=R {emp}(g)+ C(g).} The parameter ’â’ { } controls the bias-variance tradeoff. When ’â’ = 0 { =0} , this gives empirical risk minimization with low bias and high variance. When ’â’ { } is large, the learning algorithm will have high bias and low variance. The value of ’â’ { } can be chosen empirically via cross validation. The complexity penalty has a Bayesian interpretation as the negative log prior probability of g { g} , Š—ç’ ’« log Š—çÎ’ P ( g ) { - P(g)} , in which case J ( g ) { J(g)} is the posterior probabability of g { g} . == Generative training == The training methods described above are discriminative training methods, because they seek to find a function g { g} that discriminates well between the different output values (see discriminative model). For the special case where f ( x , y ) = P ( x , y ) { f(x,y)=P(x,y)} is a joint probability distribution and the loss function is the negative log likelihood Š—ç’ ’« Š—ç’ ’” i log Š—çÎ’ P ( x i , y i ) , { - {i} P(x {i},y {i}),} a risk minimization algorithm is said to perform generative training, because f { f} can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis. == Generalizations == There are several ways in which the standard supervised learning problem can be generalized: Semi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled. Active learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning. Structured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended. Learning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended. == Approaches and algorithms == Analytical learning Artificial neural network Backpropagation Boosting (meta-algorithm) Bayesian statistics Case-based reasoning Decision tree learning Inductive logic programming Gaussian process regression Group method of data handling Kernel estimators Learning Automata Learning Classifier Systems Minimum message length (decision trees, decision graphs, etc.) Multilinear subspace learning Naive bayes classifier Maximum entropy classifier Conditional random field Nearest Neighbor Algorithm Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Subsymbolic machine learning algorithms Support vector machines Minimum Complexity Machines (MCM) Random Forests Ensembles of Classifiers Ordinal classification Data Pre-processing Handling imbalanced datasets Statistical relational learning Proaftn, a multicriteria classification algorithm == Applications == Bioinformatics Cheminformatics Quantitative structure-activity relationship Database marketing Handwriting recognition Information retrieval Learning to rank Information extraction Object recognition in computer vision Optical character recognition Spam detection Pattern recognition Speech recognition Supervised learning is a special case of Downward causation in biological systems == General issues == Computational learning theory Inductive bias Overfitting (machine learning) (Uncalibrated) Class membership probabilities Unsupervised learning Version spaces == See also == List of datasets for machine learning research == References == == External links == Machine Learning Open Source Software (MLOSS)"
Data compression,Data compression,,,"In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding. Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space-time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data. == Lossless == Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding red pixel, red pixel, ... the data may be encoded as 279 red pixels. This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy. The Lempel-Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip, and PNG. LZW (Lempel-Ziv-Welch) is used in GIF images. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX). Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsofts CAB format. The best modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows-Wheeler transform can also be viewed as an indirect form of statistical modelling. The class of grammar-based codes are gaining popularity because they can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which software is publicly available. In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.264/MPEG-4 AVC and HEVC for video coding. == Lossy == Lossy data compression is the converse of lossless data compression. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video. Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression. In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from audio compression. Different audio and speech compression standards are listed under audio coding formats. Voice compression is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players. == Theory == The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate-distortion theory for lossy compression. These areas of study were essentially forged by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference. === Machine learning === There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for general intelligence. === Data differencing === Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a difference given a source and a target, with patching producing a target given a source and a difference, while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a difference from nothing. This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data. When one wishes to emphasize the connection, one may use the term differential compression to refer to data differencing. == Uses == === Audio === Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them. In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data. The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640MB. Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50-60% of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten, and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression. When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies. A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apples Apple Lossless (ALAC), MPEG-4 ALS, Microsofts Windows Media Audio 9 Lossless (WMA Lossless), Monkeys Audio, TTA, and WavPack. See list of lossless codecs for a complete listing. Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream. Other formats are associated with a distinct system, such as: Direct Stream Transfer, used in Super Audio CD Meridian Lossless Packing, used in DVD-Audio, Dolby TrueHD, Blu-ray and HD DVD ==== Lossy audio compression ==== Lossy audio compression is used in a wide range of applications. In addition to the direct applications (MP3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data. The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all. Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minutes worth of music at adequate quality. ===== Coding methods ===== To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous maskingŠ—ç’ ’–the phenomenon wherein a signal is masked by another signal separated by frequencyŠ—ç’ ’–and, in some cases, temporal maskingŠ—ç’ ’–where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models. Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sounds generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coders quantization noise into the spectrum of the target signal, partially masking it. Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen. Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a frame to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality. In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)). ===== Speech encoding ===== Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate. If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals. This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches: Only encoding sounds that could be made by a single human voice. Throwing away more of the data in the signalŠ—ç’ ’–keeping just enough to reconstruct an intelligible voice rather than the full frequency range of human hearing. Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the ’ÇÎµ-law algorithm. ==== History ==== A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee. The worlds first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies. === Video === Video compression uses modern coding techniques to reduce redundancy in video data. Most video compression algorithms and codecs combine spatial image compression and temporal motion compensation. Video compression is a practical implementation of source coding in information theory. In practice, most video codecs also use audio compression techniques in parallel to compress the separate, but combined data streams as one package. The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5-12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts. Some video compression schemes typically operate on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate. ==== Encoding theory ==== Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video. One of the most powerful techniques for compressing video is interframe compression. Interframe compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression. The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited. Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making cuts in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesnt want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as I frames in MPEG-2) arent allowed to copy data from other frames, so they require much more data than other frames nearby. It is possible to build a computer-based video editor that spots problems caused when I frames are edited out while other frames need them. This has allowed newer formats like HDV to be used for editing. However, this process demands a lot more computing power than editing intraframe compressed video with the same picture quality. Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods. ==== Timeline ==== The following table is a partial history of international video compression standards. === Genetics === Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-foldŠ—ç’ ’–allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes). === Emulation === In order to emulate CD-based consoles such as the PlayStation 2, data compression is desirable to reduce huge amounts of disk space used by ISOs. For example, Final Fantasy XII (USA) is normally 2.9 gigabytes. With proper compression, it is reduced to around 90% of that size. == Outlook and currently unused potential == It is estimated that the total amount of data that is stored on the worlds storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information. == See also == == References == == External links == Data Compression Basics (Video) Video compression 4:2:2 10-bit and its benefits Why does 10-bit save bandwidth (even when content is 8-bit)? Which compression technology should be used Wiley - Introduction to Compression Theory EBU subjective listening tests on low-bitrate audio codecs Audio Archiving Guide: Music Formats (Guide for helping a user pick out the right codec) MPEG 1&2 video compression intro (pdf format) at the Wayback Machine (archived September 28, 2007) hydrogenaudio wiki comparison Introduction to Data Compression by Guy E Blelloch from CMU HD Greetings - 1080p Uncompressed source material for compression testing and research Explanation of lossless signal compression method used by most codecs Interactive blind listening tests of audio codecs over the internet TestVid - 2,000+ HD and other uncompressed source video clips for compression testing Videsignline - Intro to Video Compression Data Footprint Reduction Technology What is Run length Coding in video compression."
Controlled vocabulary,Controlled vocabulary,,,"Controlled vocabularies provide a way to organize knowledge for subsequent retrieval. They are used in subject indexing schemes, subject headings, thesauri, taxonomies and other forms of knowledge organization systems. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction. == In library and information science == In library and information science controlled vocabulary is a carefully selected list of words and phrases, which are used to tag units of information (document or work) so that they may be more easily retrieved by a search. Controlled vocabularies solve the problems of homographs, synonyms and polysemes by a bijection between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency. For example, in the Library of Congress Subject Headings (a subject heading system that uses a controlled vocabulary), authorized termsŠ—ç’ ’–subject headings in this caseŠ—ç’ ’–have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (cockroach versus Periplaneta americana), and choices between synonyms (automobile versus car), among other difficult issues. Choices of authorized terms are based on the principles of user warrant (what terms users are likely to use), literary warrant (what terms are generally used in the literature and documents), and structural warrant (terms chosen by considering the structure, scope of the controlled vocabulary). Controlled vocabularies also typically handle the problem of homographs, with qualifiers. For example, the term pool has to be qualified to refer to either swimming pool or the game pool to ensure that each authorized term or heading refers to only one concept. There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences. Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not. For example, the Library of Congress Subject Heading itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term Broader term and Narrow term. The terms are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the documents text. Well known subject heading systems include the Library of Congress system, MeSH, and Sears. Well known thesauri include the Art and Architecture Thesaurus and the ERIC Thesaurus. Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue. Controlled vocabulary elements (terms/phrases) employed as tags, to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as metadata. == Indexing languages == There are three main types of indexing languages. Controlled indexing language - only approved terms can be used by the indexer to describe the document Natural language indexing language - any term from the document in question can be used to describe the document Free indexing language - any term (not only from the document) can be used to describe the document When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document. In recent years free text search as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is indexed). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors. Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce irrelevant items in the retrieval list. These irrelevant items (false positives) are often caused by the inherent ambiguity of natural language. Take the English word football for example. Football is the name given to a number of different team sports. Worldwide the most popular of these team sports is association football, which also happens to be called soccer in several countries. The word football is also applied to rugby football (rugby union and rugby league), American football, Australian rules football, Gaelic football, and Canadian football. A search for football therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by tagging the documents in such a way that the ambiguities are eliminated. Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually relevant to the search topic). In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you dont need to worry about searching for other terms that might be synonyms of that term. However, a controlled vocabulary search may also lead to unsatisfactory recall, in that it will fail to retrieve some documents that are actually relevant to the search question. This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer. Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with football because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless. On the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision. Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the authors own words. The use of controlled vocabularies can be costly compared to free text searches because human experts or expensive automated systems are necessary to index each entry. Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision. Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including faceted classification, which enables a given data record or document to be described in multiple ways. == Applications == Controlled vocabularies, such as the Library of Congress Subject Headings, are an essential component of bibliography, the study and classification of books. They were initially developed in library and information science. In the 1950s, government agencies began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the Medical Subject Headings (MeSH) developed by the U.S. National Library of Medicine. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup X.25 networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first full text databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library. In large organizations, controlled vocabularies may be introduced to improve technical communication. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing. This consistency of terms is one of the most important concepts in technical writing and knowledge management, where effort is expended to use the same word throughout a document or organization instead of slightly different ones to refer to the same thing. Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a Semantic Web, in which the content of Web pages is described using a machine-readable metadata scheme. One of the first proposals for such a scheme is the Dublin Core Initiative. An example of a controlled vocabulary which is usable for indexing web pages is PSH. It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web. To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web pages contents. The eXchangeable Faceted Metadata Language (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on faceted classification principles. Controlled vocabularies of the Semantic Web define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of Š—ç’ ’PersonŠ—ç’ Î, such as the Friend of a Friend (FOAF) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of Schema.org. Similarly, a book can be described using the Book vocabulary of Schema.org and general publication terms from the Dublin Core vocabulary, an event with the Event vocabulary of Schema.org, and so on. To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, HTML5 Microdata, or JSON-LD in the markup, or RDF serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files. == See also == Authority control Controlled natural language IMS Vocabulary Definition Exchange Named-entity recognition Nomenclature Ontology (computer science) Terminology Thesaurus Universal Data Element Framework Vocabulary-based transformation == References == == External links == controlledvocabulary.com Š—ç’ ’– explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases. photo-keywords.com/ Š—ç’ ’– useful guides to creating and editing your own controlled vocabulary suitable for image cataloging. ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies"
Vocabulary,Vocabulary,,,"A vocabulary is a set of familiar words within a persons language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language. == Definition and usage == Vocabulary is commonly defined as all the words known and used by a particular person. Knowing a word, however, is not as simple as merely being able to recognize or use it. There are several aspects of word knowledge that are used to measure word knowledge. === Productive and receptive knowledge === The first major distinction that must be made when evaluating word knowledge is whether the knowledge is productive (also called achieve) or receptive (also called receive); even within those opposing categories, there is often no clear distinction. Words that are generally understood when heard or read or seen constitute a persons receptive vocabulary. These words may range from well-known to barely known (see degree of knowledge below). A persons receptive vocabulary is the larger of the two. For example, although a young child may not yet be able to speak, write, or sign, he or she may be able to follow simple commands and appear to understand a good portion of the language to which he or she is exposed. In this case, the childs receptive vocabulary is likely tens, if not hundreds of words, but his or her active vocabulary is zero. When that child learns to speak or sign, however, the childs active vocabulary begins to increase. It is also possible for the productive vocabulary to be larger than the receptive vocabulary, for example in a second-language learner who has learned words through study rather than exposure, and can produce them, but has difficulty recognizing them in conversation. Productive vocabulary, therefore, generally refers to words that can be produced within an appropriate context and match the intended meaning of the speaker or signer. As with receptive vocabulary, however, there are many degrees at which a particular word may be considered part of an active vocabulary. Knowing how to pronounce, sign, or write a word does not necessarily mean that the word that has been used correctly or accurately reflects the intended message; but it does reflect a minimal amount of productive knowledge. === Degree of knowledge === Within the receptive-productive distinction lies a range of abilities that are often referred to as degree of knowledge. This simply indicates that a word gradually enters a persons vocabulary over a period of time as more aspects of word knowledge are learnt. Roughly, these stages could be described as: Never encountered the word. Heard the word, but cannot define it. Recognize the word due to context or tone of voice. Able to use the word and understand the general and/or intended meaning, but cannot clearly explain it Fluent with the word - its use and definition. === Depth of knowledge === The differing degrees of word knowledge imply a greater depth of knowledge, but the process is more complex than that. There are many facets to knowing a word, some of which are not hierarchical so their acquisition does not necessarily follow a linear progression suggested by degree of knowledge. Several frameworks of word knowledge have been proposed to better operationalise this concept. One such framework includes nine facets: orthography - written form phonology - spoken form reference - meaning semantics - concept and reference register - appropriacy of use collocation - lexical neighbours word associations syntax - grammatical function morphology - word parts === Definition of word === Words can be defined in various ways, and estimates of vocabulary size differ depending on the definition used. The most common definition is that of a lemma (the uninflected or dictionary form; this includes walk, but not walks, walked or walking). Most of the time lemmas do not include proper nouns (names of people, places, companies, ...). Another definition often used in research of vocabulary size is that of word family. These are all the words that can be derived from a ground word (e.g., the words effortless, effortlessly, effortful, effortfully are all part of the word family effort). Estimates of vocabulary size range from as high as 200 thousand to as low as 10 thousand, depending on the definition used. == Types of vocabulary == Listed in order of most ample to most limited: === Reading vocabulary === A literate persons vocabulary is all the words he or she can recognize when reading. This is generally the largest type of vocabulary simply because a reader tends to be exposed to more words by reading than by listening. === Listening vocabulary === A persons listening vocabulary is all the words he or she can recognize when listening to speech. People may still understand words they were not exposed to before using cues such as tone, gestures, the topic of discussion and the social context of the conversation. === Speaking vocabulary === A persons speaking vocabulary is all the words he or she uses in speech. It is likely to be a subset of the listening vocabulary. Due to the spontaneous nature of speech, words are often misused. This misuse - though slight and unintentional - may be compensated by facial expressions, tone of voice. === Writing vocabulary === Words are used in various forms of writing from formal essays to social media feeds. Many written words do not commonly appear in speech. Writers generally use a limited set of words when communicating: for example if there are a number of synonyms, a writer will have his own preference as to which of them to use. he is unlikely to use technical vocabulary relating to a subject in which he has no knowledge or interest. == Focal vocabulary == Focal vocabulary is a specialized set of terms and distinctions that is particularly important to a certain group: those with a particular focus of experience or activity. A lexicon, or vocabulary, is a languages dictionary: its set of names for things, events, and ideas. Some linguists believe that lexicon influences peoples perception of things, the Sapir-Whorf hypothesis. For example, the Nuer of Sudan have an elaborate vocabulary to describe cattle. The Nuer have dozens of names for cattle because of the cattles particular histories, economies, and environments. This kind of comparison has elicited some linguistic controversy, as with the number of Eskimo words for snow. English speakers with relevant specialised knowledge can also display elaborate and precise vocabularies for snow and cattle when the need arises. == Vocabulary growth == During its infancy, a child instinctively builds a vocabulary. Infants imitate words that they hear and then associate those words with objects and actions. This is the listening vocabulary. The speaking vocabulary follows, as a childs thoughts become more reliant on his/her ability to self-express without relying on gestures or babbling. Once the reading and writing vocabularies start to develop, through questions and education, the child starts to discover the anomalies and irregularities of language. In first grade, a child who can read learns about twice as many words as one who cannot. Generally, this gap does not narrow later. This results in a wide range of vocabulary by age five or six, when an English-speaking child will have learned about 1500 words. Vocabulary grows throughout our entire life. Between the ages of 20 and 60, people learn some 6,000 more lemmas, or one every other day. An average 20-year-old knows 42,000 words coming from 11,100 word families; an average 60-year-old knows 48,200 lemmas coming from 13,400 word families. People expand their vocabularies by e.g. reading, playing word games, and participating in vocabulary-related programs. Exposure to traditional print media teaches correct spelling and vocabulary, while exposure to text messaging leads to more relaxed word acceptability constraints. == Importance == An extensive vocabulary aids expression and communication. Vocabulary size has been directly linked to reading comprehension. Linguistic vocabulary is synonymous with thinking vocabulary. A person may be judged by others based on his or her vocabulary. Wilkins (1972) once said, Without grammar, very little can be conveyed, without vocabulary, nothing can be conveyed. == Vocabulary size == === Native-language vocabulary === Estimating average vocabulary size poses various difficulties and limitations due to the different definitions and methods employed such as what is the word, what is to know a word, what sample dictionaries were used, how tests were conducted, and so on. Native speakers vocabularies also vary widely within a language, and are dependent on the level of the speakers education. As a result estimates vary from as little as 10,000 to as many as over 50,000 for young adult native speakers of English. One most recent 2016 study shows that 20-year-old English native speakers recognize on average 42,000 lemmas, ranging from 27,100 for the lowest 5% of the population to 51,700 lemmas for the highest 5%. These lemmas come from 6,100 word families in the lowest 5% of the population and 14,900 word families in the highest 5%. 60-year-olds know on average 6,000 lemmas more. According to another, earlier 1995 study junior-high students would be able to recognize the meanings of about 10,000-12,000 words, whereas for college students this number grows up to about 12,000-17,000 and for elderly adults up to about 17,000 or more. For native speakers of German average absolute vocabulary sizes range from 5,900 lemmas in first grade to 73,000 for adults. === Foreign-language vocabulary === ==== The effects of vocabulary size on language comprehension ==== The knowledge of the 3000 most frequent English word families or the 5000 most frequent words provides 95% vocabulary coverage of spoken discourse. For minimal reading comprehension a threshold of 3,000 word families (5,000 lexical items) was suggested and for reading for pleasure 5,000 word families (8,000 lexical items) are required. An optimal threshold of 8,000 word families yields the coverage of 98% (including proper nouns). ==== Second language vocabulary acquisition ==== Learning vocabulary is one of the first steps in learning a second language, but a learner never finishes vocabulary acquisition. Whether in ones native language or a second language, the acquisition of new vocabulary is an ongoing process. There are many techniques that help one acquire new vocabulary. ==== Memorization ==== Although memorization can be seen as tedious or boring, associating one word in the native language with the corresponding word in the second language until memorized is considered one of the best methods of vocabulary acquisition. By the time students reach adulthood, they generally have gathered a number of personalized memorization methods. Although many argue that memorization does not typically require the complex cognitive processing that increases retention (Sagarra and Alba, 2006), it does typically require a large amount of repetition, and spaced repetition with flashcards is an established method for memorization, particularly used for vocabulary acquisition in computer-assisted language learning. Other methods typically require more time and longer to recall. Some words cannot be easily linked through association or other methods. When a word in the second language is phonologically or visually similar to a word in the native language, one often assumes they also share similar meanings. Though this is frequently the case, it is not always true. When faced with a false friend, memorization and repetition are the keys to mastery. If a second language learner relies solely on word associations to learn new vocabulary, that person will have a very difficult time mastering false friends. When large amounts of vocabulary must be acquired in a limited amount of time, when the learner needs to recall information quickly, when words represent abstract concepts or are difficult to picture in a mental image, or when discriminating between false friends, rote memorization is the method to use. A neural network model of novel word learning across orthographies, accounting for L1-specific memorization abilities of L2-learners has recently been introduced (Hadzibeganovic and Cannas, 2009). ==== The Keyword Method ==== One useful method of building vocabulary in a second language is the keyword method. If time is available or one wants to emphasize a few key words, one can create mnemonic devices or word associations. Although these strategies tend to take longer to implement and may take longer in recollection, they create new or unusual connections that can increase retention. The keyword method requires deeper cognitive processing, thus increasing the likelihood of retention (Sagarra and Alba, 2006). This method uses fits within Paivios (1986) dual coding theory because it uses both verbal and image memory systems. However, this method is best for words that represent concrete and imageable things. Abstract concepts or words that do not bring a distinct image to mind are difficult to associate. In addition, studies have shown that associative vocabulary learning is more successful with younger students (Sagarra and Alba, 2006). Older students tend to rely less on creating word associations to remember vocabulary. === Word lists === Several word lists have been developed to provide people with a limited vocabulary either for the purpose of rapid language proficiency or for effective communication. These include Basic English (850 words), Special English (1,500 words), General Service List (2,000 words), and Academic Word List. Some learners dictionaries have developed defining vocabularies which contain only most common and basic words. As a result world definitions in such dictionaries can be understood even by learners with a limited vocabulary. Some publishers produce dictionaries based on word frequency or thematic groups. The Swadesh list was made for investigation in linguistics. == See also == Differences between American and British English (vocabulary) Language proficiency: the ability of an individual to speak or perform in an acquired language Longest word in English: lots of the longest words in the English language == Footnotes == == References == Barnhart, Clarence Lewis (ed.) (1968). The World Book Dictionary. Chicago: Thorndike-Barnhart, OCLC 437494 Brysbaert M, Stevens M, Mandera P and Keuleers E (2016) How Many Words Do We Know? Practical Estimates of Vocabulary Size Dependent on Word Definition, the Degree of Language Input and the ParticipantŠ—ç’ Îés Age. Front. Psychol. 7:1116. doi: 10.3389/fpsyg.2016.01116. Flynn, James Robert (2008). Where have all the liberals gone? : race, class, and ideals in America. Cambridge University Press; 1st edition. ISBN 978-0-521-49431-1 OCLC 231580885 Lenkeit, Roberta Edwards (2007) Introducing cultural anthropology Boston: McGraw-Hill (3rd. ed.) OCLC 64230435 Liu, Na and I. S. P. Nation. Factors affecting guessing vocabulary in context, RELC Journal, 1985,16 1, pp. 33-42. doi:10.1177/003368828501600103 Miller, Barbara D. (1999). Cultural Anthropology(4th ed.) Boston: Allyn and Bacon, p. 315 OCLC 39101950 Schonell, Sir Fred Joyce, Ivor G. Meddleton and B. A. Shaw, A study of the oral vocabulary of adults : an investigation into the spoken vocabulary of the Australian worker, University of Queensland Press, Brisbane, 1956. OCLC 606593777 West, Michael (1953). A general service list of English words, with semantic frequencies and a supplementary word-list for the writing of popular science and technology London, New York: Longman, Green OCLC 318957 == External links == Open Dictionary of English (ODE) Multi-media dictionary developed for learning vocabulary. Offers audio from around the world, images, video clips, usage samples, multiple definitions, correlations, idioms and much more. ODE is also part of LearnThatWords vocabulary quizzes. Bibliography on vocabulary I.S.P. Nations extensive collection of research on vocabulary. Vocabulary Acquisition Research Group Archive An extensive bibliographic database on vocabulary acquisition maintained by Paul Meara and the Vocabulary Acquisition Research Group at Swansea University. VocabularySize.com - a free web-based service that implements the I.S.P. Nations English Vocabulary Size Test in an online format. Vocabulary test - a free four-minute English vocabulary size test, accurate within 10%, on which Brysbaert et al.s (2016) estimates of vocabulary size are based. Vocabulary test - in 30+ languages. TestYourVocab.com - a free five-minute English vocabulary size test, accurate within 10% WordsinaSentence.com - a free online dictionary that defines vocabulary words with contextual sentences."
Data pre-processing,Data pre-processing,,,"Data pre-processing is an important step in the [data mining] process. The phrase garbage in, garbage out is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: Š—ç’ ’«100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data pre-processing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. Kotsiantis et al. (2006) present a well-known algorithm for each step of data pre-processing. == See also == Data cleansing Data editing Data reduction Data wrangling == References == == External links == Online Data Processing Compendium"
Electronic data processing,Electronic data processing,,,"Electronic data processing (EDP) can refer to the use of automated methods to process commercial data. Typically, this uses relatively simple, repetitive activities to process large volumes of similar information. For example: stock updates applied to an inventory, banking transactions applied to account and customer master files, booking and ticketing transactions to an airlines reservation system, billing for utility services. The modifier electronic or automatic was used with data processing (DP), especially c. 1960, to distinguish human clerical data processing from that done by computer. == History == The first commercial business computer was developed in the United Kingdom in 1951, by the J. Lyons and Co. catering organization. This was known as the Lyons Electronic Office - or LEO for short. It was developed further and used widely during the 1960s and early 1970s. (Joe Lyons formed a separate company to develop the LEO computers and this subsequently merged to form English Electric Leo Marconi and then International Computers Limited. By the end of the 1950s punched card manufacturers, Hollerith, Powers-Samas, IBM and others, were also marketing an array of computers. Early commercial systems were installed exclusively by large organizations. These could afford to invest the time and capital necessary to purchase hardware, hire specialist staff to develop bespoke software and work through the consequent (and often unexpected) organizational and cultural changes. At first, individual organizations developed their own software, including data management utilities, themselves. Different products might also have one-off bespoke software. This fragmented approach led to duplicated effort and the production of management information needed manual effort. High hardware costs and relatively slow processing speeds forced developers to use resources efficiently. Data storage formats were heavily compacted, for example. A common example is the removal of the century from dates, which eventually led to the millennium bug. Data input required intermediate processing via punched paper tape or punched card and separate input to a repetitive, labor-intensive task, removed from user control and error-prone. Invalid or incorrect data needed correction and resubmission with consequences for data and account reconciliation. Data storage was strictly serial on paper tape, and then later to magnetic tape: the use of data storage within readily accessible memory was not cost-effective. Significant developments took place in 1959 with IBM announcing the 1401 computer and in 1962 with ICT (International Computers & Tabulators) making delivery of the ICT 1301. Like all machines during this time the core processor together with the peripherals - magnetic tape decks, discs, drums, printers and card and paper tape input and output required considerable space in specially constructed air conditioned accommodation. Often parts of the punched card installation, in particular sorters, were retained to present the card input to the computer in a pre-sort form that reduced the processing time involved in sorting large amounts of data. Data processing facilities became available to smaller organization in the form of the computer services bureau. These offered processing of specific applications e.g. payroll and were often a prelude to the purchase of customers own computer. Organizations used these facilities for testing programs while awaiting the arrival of their own machine. These initial machines were delivered to customers with limited software. The design staff was divided into two groups. Systems analysts produced a systems specification and programmers translated the specification into machine language. Literature on computers and EDP was sparse through articles appearing in accountancy publications and material supplied by the equipment manufacturers. The first issue of The Computer Journal published by The British Computer Society appeared in mid 1958. The UK Accountancy Body now named The Association of Chartered Certified Accountants formed an Electronic Data Processing Committee in July 1958 with the purpose of informing its members of the opportunities created by the computer. The Committee produced its first booklet in 1959, An Introduction to Electronic Computers. Also in 1958 The Institute of Chartered Accountants in England and Wales produced a paper Accounting by Electronic Methods. The notes indicated what appears capable and the possible implications of using a computer. Progressive organizations attempted to go beyond the straight systems transfer from punched card equipment and unit accounting machines to the computer, to producing accounts to the trial balance stage and integrated management information systems. New procedures redesigned the way paper flowed, changed organizational structures, called for a rethink of the way information was presented to management and challenged the internal control principles adopted by the designers of accounting systems. But the full realization of these benefits had to await the arrival of the next generation of computers == Today == As with other industrial processes commercial IT has moved in most cases from a custom-order, craft-based industry where the product was tailored to fit the customer; to multi-use components taken off the shelf to find the best-fit in any situation. Mass-production has greatly reduced costs and IT is available to the smallest organization. LEO was hardware tailored for a single client. Today, Intel Pentium and compatible chips are standard and become parts of other components which are combined as needed. One individual change of note was the freeing of computers and removable storage from protected, air-filtered environments. Microsoft and IBM at various times have been influential enough to impose order on IT and the resultant standardizations allowed specialist software to flourish. Software is available off the shelf: apart from products such as Microsoft Office and IBM Lotus, there are also specialist packages for payroll and personnel management, account maintenance and customer management, to name a few. These are highly specialized and intricate components of larger environments, but they rely upon common conventions and interfaces. Data storage has also standardized. Relational databases are developed by different suppliers to common formats and conventions. Common file formats can be shared by large main-frames and desk-top personal computers, allowing online, real time input and validation. In parallel, software development has fragmented. There are still specialist technicians, but these increasingly use standardized methodologies where outcomes are predictable and accessible. At the other end of the scale, any office manager can dabble in spreadsheets or databases and obtain acceptable results (but there are risks). Specialized software is software that is written for a specific task rather for a broad application area. These programs provide facilities specifically for the purpose for which they were designed. == See also == Computing Data processing Data processing system Information Technology == References =="
Document classification,Document classification,,,"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done manually (or intellectually) or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied. Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach. == Content-based versus request-based classification == Content-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document. Request-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: Š—ç’ ’Under which descriptors should this entity be found?Š—ç’ Î and Š—ç’ ’think of all the possible queries and decide for which ones the entity at hand is relevantŠ—ç’ Î (Soergel, 1985, p. 230). Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library. It is probably better, however, to understand request-oriented classification as policy-based classification: The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach. == Classification versus indexing == Sometimes a distinction is made between assigning documents to classes (classification) versus assigning subjects to documents (subject indexing) but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. These terminological distinctions,Š—ç’ Î he writes, Š—ç’ ’are quite meaningless and only serve to cause confusionŠ—ç’ Î (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004; Broughton, 2008; Riesthuis & Bliedung, 1991). Therefore, is the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents). == Automatic document classification (ADC) == Automatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available. == Techniques == Automatic document classification techniques include: Expectation maximization (EM) Naive Bayes classifier tf-idf Instantaneously trained neural networks Latent semantic indexing Support vector machines (SVM) Artificial neural network K-nearest neighbour algorithms Decision trees such as ID3 or C4.5 Concept Mining Rough set-based classifier Soft set-based classifier Multiple-instance learning Natural language processing approaches == Applications == Classification techniques have been applied to spam filtering, a process which tries to discern E-mail spam messages from legitimate emails email routing, sending an email sent to a general address to a specific address or mailbox depending on topic language identification, automatically determining the language of a text genre classification, automatically determining the genre of a text readability assessment, automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger text simplification system sentiment analysis, determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. Article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology. == See also == == Further reading == Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1-47, 2002. Stefan B’‘ ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010. == References == == External links == Introduction to document classification Bibliography on Automated Text Categorization Bibliography on Query Classification Text Classification analysis page Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python (available online) TechTC - Technion Repository of Text Categorization Datasets David D. Lewiss Datasets BioCreative III ACT (article classification task) dataset"
Classification,Classification,,,"Classification is a general process related to categorization, the process in which ideas and objects are recognized, differentiated, and understood. A classification system is an approach to accomplishing classification. Classification may refer specifically to: == Mathematics == Statistical classification, identifying to which of a set of categories a new observation belongs, on the basis of a training set of data Mathematical classification, a collection of sets which can be unambiguously defined by a property that all its members share Classification theorems in mathematics Attribute-value system, a basic knowledge representation framework == Media == Document classification, a problem in library science, information science and computer science Library classification, system of coding, assorting and organizing library materials according to their subject Classified information, sensitive information to which access is restricted by law or regulation to particular classes of people Motion picture rating system, for film classification Classification (literature), a figure of speech linking a proper noun to a common noun using the or other articles == Science == Scientific classification (disambiguation) Chemical classification Taxonomic classification, also known as classification of species Cladistics, an approach using similarities Biological classification of organisms Medical classification, the process of transforming descriptions of medical diagnoses and procedures into universal medical code numbers == Business, organizations, and economics == Classification of customers, for marketing (as in Master data management) or for profitability (e.g. by Activity-Based Costing) Standard Industrial Classification, economic activities Job classification, as in job analysis == Other uses == Civil service classification, personnel grades in government Classification society, a non-governmental organization that establishes and maintains technical standards for the construction and operation of ships and offshore structures Product classification Locomotive classification Classification of wine An industrial process such as mechanical screening for sorting materials by size, shape, and density, etc Classification of swords Security classification == Organizations involved in classification == International Society for Knowledge Organization == See also == Class (disambiguation) Data classification (disambiguation) Classified (disambiguation) Classifier (disambiguation) Taxonomy (disambiguation) == External links == Media related to Classification at Wikimedia Commons"
Document clustering,Document clustering,,,"Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering. == Overview == Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users. The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared to offline applications.Text clustering may be used for different tasks, such as grouping similar documents (news, tweets, etc.) and the analysis of customer/employee feedback, discovering meaningful implicit subjects across all documents. In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Wards method. By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the K-means algorithm and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the K-means algorithm are more efficient and provide sufficient information for most purposes. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment - each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft - a documentŠ—ç’ Îés assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Dimensionality reduction methods can be considered a subtype of soft clustering; for documents, these include latent semantic indexing (truncated singular value decomposition on term histograms) and topic models. Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering. Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. Various methods exist for this purpose. == Clustering in search engines == A web search engine often returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information. Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by e.g. open source software such as Carrot2. == Procedures == In practice, document clustering often takes the following steps: 1. Tokenization Tokenization is the process of parsing text data into smaller units (tokens) such as words and phrases. Commonly used tokenization methods include Bag-of-words model and N-gram model. 2. Stemming and lemmatization Different tokens might carry out similar information (e.g. tokenization and tokenizing). And we can avoid calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries. 3. Removing stop words and punctuation Some tokens are less important than others. For instance, common words such as the might not be very helpful for revealing the essential characteristics of a text. So usually it is a good idea to eliminate stop words and punctuation marks before doing further analysis. 4. Computing term frequencies or tf-idf After pre-processing the text data, we can then proceed to generate features. For document clustering, one of the most common ways to generate features for a document is to calculate the term frequencies of all its tokens. Although not perfect, these frequencies can usually provide some clues about the topic of the document. And sometimes it is also useful to weight the term frequencies by the inverse document frequencies. See tf-idf for detailed discussions. 5. Clustering We can then cluster different documents based on the features we have generated. See the algorithm section in cluster analysis for different types of clustering methods. 6. Evaluation and visualization Finally, the clustering models can be assessed by various metrics. And it is sometimes helpful to visualize the results by plotting the clusters into low (two) dimensional space. See multidimensional scaling as a possible approach. == Clustering v. Classifying == Clustering algorithms in computational text analysis groups documents into grouping a set of text what are called subsets or clusters where the algorithms goal is to create internally coherent clusters that are distinct from one another. Classification on the other hand, is a form of supervised learning where the features of the documents are used to predict the type of documents. == References == Publications: Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch’‘ tze. Flat Clustering in Introduction to Information Retrieval. Cambridge University Press. 2008 Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [1] Claudio Carpineto, Stanislaw OsiÎ’£ski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys, Volume 41, Issue 3 (July 2009), Article No. 17, ISSN 0360-0300 Wui Lee Chang, Kai Meng Tay, and Chee Peng Lim, A New Evolving Tree-Based Model with Local Re-learning for Document Clustering and Visualization, Neural Processing Letters, DOI: 10.1007/s11063-017-9597-3. https://link.springer.com/article/10.1007/s11063-017-9597-3 == See also == Cluster Analysis Fuzzy clustering"
Hierarchical clustering,Hierarchical clustering,,,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types: Agglomerative: This is a bottom up approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive: This is a top down approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram. In the general case, the complexity of agglomerative clustering is O ( n 2 log Š—çÎ’ ( n ) ) { O(n^{2}(n))} , which makes them too slow for large data sets. Divisive clustering with an exhaustive search is O ( 2 n ) { O(2^{n})} , which is even worse. However, for some special cases, optimal efficient agglomerative methods (of complexity O ( n 2 ) ) { O(n^{2}))} ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. == Cluster dissimilarity == In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate metric (a measure of distance between pairs of observations), and a linkage criterion which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets. === Metric === The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a 2-dimensional space, the distance between the point (1,0) and the origin (0,0) is always 1 according to the usual norms, but the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, 2 { }} under Euclidean distance, or 1 under maximum distance. Some commonly used metrics for hierarchical clustering are: For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance. === Linkage criteria === The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations. Some commonly used linkage criteria between two sets of observations A and B are: where d is the chosen metric. Other linkage criteria include: The sum of all intra-cluster variance. The decrease in variance for the cluster being merged (Wards criterion). The probability that candidate clusters spawn from the same distribution function (V-linkage). The product of in-degree and out-degree on a k-nearest-neighbour graph (graph degree linkage). The increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters. == Discussion == Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. == Agglomerative clustering example == For example, suppose this data is to be clustered, and the Euclidean distance is the distance metric. Cutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row of the dendrogram will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number but larger clusters. The hierarchical clustering dendrogram would be as such: This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance. Optionally, one can also construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the single-linkage clustering page; it can easily be adapted to different types of linkage (see below). Suppose we have merged the two closest elements b and c, we now have the following clusters {a}, {b, c}, {d}, {e} and {f}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters. Usually the distance between two clusters A {}} and B {}} is one of the following: The maximum distance between elements of each cluster (also called complete-linkage clustering): max { d ( x , y ) : x Š—ç’ ’ A , y Š—ç’ ’ B } . { },,y},}.} The minimum distance between elements of each cluster (also called single-linkage clustering): min { d ( x , y ) : x Š—ç’ ’ A , y Š—ç’ ’ B } . { },,y},}.} The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in UPGMA): 1 | A | Š—ç’“’ | B | Š—ç’ ’” x Š—ç’ ’ A Š—ç’ ’” y Š—ç’ ’ B d ( x , y ) . {}| |{}|}} {x}} {y}}d(x,y).} The sum of all intra-cluster variance. The decrease in variance for the cluster being merged (Wards method) The probability that candidate clusters spawn from the same distribution function (V-linkage). One can always decide to stop clustering when there is a sufficiently small number of clusters (number criterion). Some linkages may also guarantee that agglomeration occurs at a greater distance between clusters than the previous agglomeration, and then one can stop clustering when the clusters are too far apart to be merged (distance criterion). However, this is not the case of, e.g., the centroid linkage where the so-called reversals (inversions, departures from ultrametricity) may occur. == Divisive clustering == The basic principle of divisive clustering was published as the DIANA (DIvisive ANAlysis Clustering) algorithm. Initially, all data is in the same cluster, and the largest cluster is split until every object is separate. Because there exist O ( 2 n ) { O(2^{n})} ways of splitting each cluster, heuristics are needed. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder. An obvious alternate choice is k-means clustering with k = 2 { k=2} , but any other clustering algorithm can be used that always produces at least two clusters. == Software == === Open source implementations === Cluster 3.0 provides a Graphical User Interface to access to different clustering routines and is available for Windows, Mac OS X, Linux, Unix. ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms. Octave, the GNU analog to MATLAB implements hierarchical clustering in linkage function Orange, a free data mining software suite, module orngClustering for scripting in Python, or cluster analysis through visual programming. R has several functions for hierarchical clustering: see CRAN Task View: Cluster Analysis & Finite Mixture Models for more information. SCaViS computing environment in Java that implements this algorithm. scikit-learn implements a hierarchical clustering in Python. Weka includes hierarchical cluster analysis. === Commercial Implementations === MATLAB includes hierarchical cluster analysis. SAS includes hierarchical cluster analysis in PROC CLUSTER. Mathematica includes a Hierarchical Clustering Package. NCSS (statistical software) includes hierarchical cluster analysis. SPSS includes hierarchical cluster analysis. Qlucore Omics Explorer includes hierarchical cluster analysis. Stata includes hierarchical cluster analysis. == See also == Statistical distance Brown clustering Cladistics Cluster analysis Computational phylogenetics CURE data clustering algorithm Dendrogram Determining the number of clusters in a data set Hierarchical clustering of networks Nearest-neighbor chain algorithm Numerical taxonomy OPTICS algorithm Nearest neighbor search Locality-sensitive hashing Bounding volume hierarchy Binary space partitioning == References == == Further reading == Kaufman, L.; Rousseeuw, P.J. (1990). Finding Groups in Data: An Introduction to Cluster Analysis (1 ed.). New York: John Wiley. ISBN 0-471-87876-6. Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2009). 14.3.12 Hierarchical clustering. The Elements of Statistical Learning (PDF) (2nd ed.). New York: Springer. pp. 520-528. ISBN 0-387-84857-6. Retrieved 2009-10-20."
Evaluation measures (information retrieval),Evaluation measures (information retrieval),,,"Evaluation measures for an information retrieval system are used to assess how well the search results satisfied the users query intent. Such metrics are often split into kinds: online metrics look at users interactions with the search system, while offline metrics measure relevance, in other words how likely each result, or SERP page as a whole, is to meet the information needs of the user. The mathematical symbols used in the formulas below mean: X Š—ç’ Î© Y { X Y} - Intersection - in this case, specifying the documents in both sets X and Y | X | { |X|} - Cardinality - in this case, the number of documents in set X Š—ç’ ’ö { } - Integral Š—ç’ ’” { } - Summation ’â’– { } - Symmetric difference == Online metrics == Online metrics are generally created from data mined from search logs. The metrics are often used to determine the success of an A/B test. === Session abandonment rate === Session abandonment rate is a ratio of search session which do not result in a click. === Click-through rate === Click-through rate (CTR) is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns. === Session success rate === Session success rate measures the ratio of user sessions that lead to a success. Defining success is often dependent on context, but for search a successful result is often measured using dwell time as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet. === Zero result rate === Zero result rate (ZRR) is the ratio of SERPs which returned with zero results. The metric either indicates a recall issue, or that the information being searched for is not in the index. == Offline metrics == Offline metrics are generally created from relevance judgment sessions where the judges score the quality of the search results. Both binary (relevant/non-relevant) and multi-level (e.g., relevance from 0 to 5) scales can be used to score each document returned in response to a query. In practice, queries may be ill-posed, and there may be different shades of relevance. For instance, there is ambiguity in the query mars: the judge does not know if the user is searching for the planet Mars, the Mars chocolate bar, or the singer Bruno Mars. === Precision === Precision is the fraction of the documents retrieved that are relevant to the users information need. precision = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { retrieved documents } | {={} }}|}{|}}|}}} In binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n. Note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics. === Recall === Recall is the fraction of the documents that are relevant to the query that are successfully retrieved. recall = | { relevant documents } Š—ç’ Î© { retrieved documents } | | { relevant documents } | {={} }}|}{|}}|}}} In binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query. It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision. === Fall-out === The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available: fall-out = | { non-relevant documents } Š—ç’ Î© { retrieved documents } | | { non-relevant documents } | {={} }}|}{|}}|}}} In binary classification, fall-out is closely related to specificity and is equal to ( 1 Š—ç’ ’« specificity ) { (1-{})} . It can be looked at as the probability that a non-relevant document is retrieved by the query. It is trivial to achieve fall-out of 0% by returning zero documents in response to any query. === F-score / F-measure === The weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is: F = 2 Š—ç’“’ p r e c i s i o n Š—ç’“’ r e c a l l ( p r e c i s i o n + r e c a l l ) { F={ }{( + )}}} This is also known as the F 1 { F {1}} measure, because recall and precision are evenly weighted. The general formula for non-negative real ’â { } is: F ’â = ( 1 + ’â 2 ) Š—ç’“’ ( p r e c i s i o n Š—ç’“’ r e c a l l ) ( ’â 2 Š—ç’“’ p r e c i s i o n + r e c a l l ) { F { }={) ( )}{( ^{2} + )}},} Two other commonly used F measures are the F 2 { F {2}} measure, which weights recall twice as much as precision, and the F 0.5 { F {0.5}} measure, which weights precision twice as much as recall. The F-measure was derived by van Rijsbergen (1979) so that F ’â { F { }} measures the effectiveness of retrieval with respect to a user who attaches ’â { } times as much importance to recall as precision. It is based on van Rijsbergens effectiveness measure E = 1 Š—ç’ ’« 1 ’âÎ± P + 1 Š—ç’ ’« ’âÎ± R { E=1-{{{{P}}+{{R}}}}} . Their relationship is: F ’â = 1 Š—ç’ ’« E { F { }=1-E} where ’âÎ± = 1 1 + ’â 2 { ={{1+ ^{2}}}} F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it. === Average precision === Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision p ( r ) { p(r)} as a function of recall r { r} . Average precision computes the average value of p ( r ) { p(r)} over the interval from r = 0 { r=0} to r = 1 { r=1} : AveP = Š—ç’ ’ö 0 1 p ( r ) d r { = {0}^{1}p(r)dr} That is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents: AveP = Š—ç’ ’” k = 1 n P ( k ) ’â’– r ( k ) { = {k=1}^{n}P(k) r(k)} where k { k} is the rank in the sequence of retrieved documents, n { n} is the number of retrieved documents, P ( k ) { P(k)} is the precision at cut-off k { k} in the list, and ’â’– r ( k ) { r(k)} is the change in recall from items k Š—ç’ ’« 1 { k-1} to k { k} . This finite sum is equivalent to: AveP = Š—ç’ ’” k = 1 n ( P ( k ) ’‘’• rel Š—çÎ’ ( k ) ) number of relevant documents { ={^{n}(P(k) (k))}{}}!} where rel Š—çÎ’ ( k ) { (k)} is an indicator function equaling 1 if the item at rank k { k} is a relevant document, zero otherwise. Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero. Some authors choose to interpolate the p ( r ) { p(r)} function to reduce the impact of wiggles in the curve. For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}: AveP = 1 11 Š—ç’ ’” r Š—ç’ ’ { 0 , 0.1 , Š—ç’ , 1.0 } p interp ( r ) { ={{11}} {r }p { }(r)} where p interp ( r ) { p { }(r)} is an interpolated precision that takes the maximum precision over all recalls greater than r { r} : p interp ( r ) = max r ~ : r ~ Š—ç’ Î‚ r Š—çÎ’ p ( r ~ ) { p { }(r)= {{}:{} r}p({})} . An alternative is to derive an analytical p ( r ) { p(r)} function by assuming a particular parametric distribution for the underlying decision values. For example, a binormal precision-recall curve can be obtained by assuming decision values in both classes to follow a Gaussian distribution. === Precision at K === For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or Precision at 10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k. Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1. It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not. === R-Precision === R-precision requires knowing all documents that are relevant to a query. The number of relevant documents, R { R} , is used as the cutoff for calculation, and this varies from query to query. For example, if there are 15 documents relevant to red in a corpus (R=15), R-precision for red looks at the top 15 documents returned, counts the number that are relevant r { r} turns that into a relevancy fraction: r / R = r / 15 { r/R=r/15} . Precision is equal to recall at the R-th position. Empirically, this measure is often highly correlated to mean average precision. === Mean average precision === Mean average precision for a set of queries is the mean of the average precision scores for each query. MAP = Š—ç’ ’” q = 1 Q A v e P ( q ) Q { ={^{Q} }{Q}}!} where Q is the number of queries. === Discounted cumulative gain === DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The DCG accumulated at a particular rank position p { p} is defined as: D C G p = r e l 1 + Š—ç’ ’” i = 2 p r e l i log 2 Š—çÎ’ i . { } =rel {1}+ {i=2}^{p}{}{ {2}i}}.} Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( I D C G p { IDCG {p}} ), which normalizes the score: n D C G p = D C G p I D C G p . { } ={}{IDCG{p}}}.} The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the D C G p { DCG {p}} will be the same as the I D C G p { IDCG {p}} producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable. === Other measures === Mean reciprocal rank Spearmans rank correlation coefficient bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents GMAP - geometric mean of (per-topic) average precision Measures based on marginal relevance and document diversity - see Relevance (information retrieval) ’ÇÎ Problems and alternatives === Visualization === Visualizations of information retrieval performance include: Graphs which chart precision on one axis and recall on the other Histograms of average precision over various topics Receiver operating characteristic (ROC curve) Confusion matrix == Non-metrics == === Top queries list === Top queries is noting the most common queries over a fixed amount of time. The top queries list assists in knowing the style of queries entered by users. == Non-relevance metrics == === Queries per time === Measuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency. == References =="
Evaluation of binary classifiers,Evaluation of binary classifiers,,,"The evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence - both types are useful, but they have very different properties. == Contingency table == Given a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification - ideally a perfect classification, but in practice the output of another gold standard test - and cross tabulates the data into a 2’‘’•2 contingency table, comparing the two classifications. One then evaluates the classifier relative to the gold standard by computing summary statistics of these 4 numbers. Generally these statistics will be scale invariant (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of homogeneous functions, most simply homogeneous linear or homogeneous quadratic functions. Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called true positives (TP). Some have the disease, but the test incorrectly claims they dont. They are called false negatives (FN). Some dont have the disease, and the test says they dont - true negatives (TN). Finally, there might be healthy people who have a positive test result - false positives (FP). These can be arranged into a 2’‘’•2 contingency table (confusion matrix), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis. These numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2’‘’•2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2’‘’•2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2’‘’•2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions. The contingency table and the most common derived ratios are summarized below; see sequel for details. Note that the columns correspond to the test being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the rows correspond to the condition actually being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above. == Sensitivity and specificity == The fundamental prevalence-independent statistics are sensitivity and specificity. Sensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as the probability that the test is positive given that the patient is sick. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market). Specificity (SPC) or True Negative Rate (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as the probability that the test result is negative given that the patient is not sick. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarded). The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the Receiver Operating Characteristic (ROC) curve. In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator. Modern pregnancy tests do not use the pregnancy itself to determine pregnancy status; rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a surrogate marker to indicate that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100% (in that false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100% (becaause false negatives are possible). === Likelihood ratios === == Positive and negative predictive values == In addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question If the test result is positive, how well does that predict an actual presence of disease?. It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally. === Impact of prevalence on prediction values === Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result. However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease - that means, intuitively, that given that a patients test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patients test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result. === Likelihood ratios === == Precision and recall == === Relationships === There are various relationships between these ratios. If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity: PPV = ( sensitivity ) ( prevalence ) ( sensitivity ) ( prevalence ) + ( 1 Š—ç’ ’« specificity ) ( 1 Š—ç’ ’« prevalence ) {={)({})}{({})({})+(1-{})(1-{})}}} If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity: NPV = ( specificity ) ( 1 Š—ç’ ’« prevalence ) ( specificity ) ( 1 Š—ç’ ’« prevalence ) + ( 1 Š—ç’ ’« sensitivity ) ( prevalence ) . {={)(1-{})}{({})(1-{})+(1-{})({})}}.} == Single metrics == In addition to the paired metrics, there are also single metrics that give a single number to evaluate the test. Perhaps the simplest statistic is accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/Total Population = (TP + TN)/(TP + TN + FP + FN). This is often not very useful, compared to the marginal ratios, as it does not yield useful marginal interpretations, due to mixing true positives (test positive, condition positive) and true negatives (test negative, condition negative) - in terms of the condition table, it sums the diagonal; further, it is prevalence-dependent. The complement is the Fraction Incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) - this is the sum of the antidiagonal, divided by the total population. The diagnostic odds ratio (DOR) is a more useful overall metric, which can be defined directly as (TP’‘’•TN)/(FP’‘’•FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of True Rates or Prediction Values). This has a useful interpretation - as an odds ratio - and is prevalence-independent. An F-score is a combination of the precision and the recall, providing a single score. There is a one-parameter family of statistics, with parameter ’â , which determines the relative weights of precision and recall. The traditional or balanced F-score (F1 score) is the harmonic mean of precision and recall: F 1 = 2 Š—ç’“’ p r e c i s i o n Š—ç’“’ r e c a l l p r e c i s i o n + r e c a l l { F {1}=2 }{ + }}} . === Alternative metrics === Note, however, that the F-scores do not take the true negative rate into account, and that measures such as the Phi coefficient, Matthews correlation coefficient, Informedness or Cohens kappa may be preferable to assess the performance of a binary classifier. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (deltap). Other metrics include Youdens J statistic. == See also == Population Impact Measures Attributable risk Attributable risk percent == References =="
Binary classification,Binary classification,,,"Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a classification rule. Instancing a decision whether an item has or not some qualitative property, some specified characteristic, some typical binary classification tasks are: Medical testing to determine if a patient has certain disease or not - the classification property is the presence of the disease. A pass or fail test method or quality control in factories, i.e. deciding if a specification has or has not been met - a Go/no go classification. Information retrieval, namely deciding whether a page or an article should be in the result set of a search or not - the classification property is the relevance of the article, or the usefulness to the user. Binary classification is dichotomization applied to practical purposes, and therefore an important point is that in many practical binary classification problems, the two groups are not symmetric - rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present). Porting human discriminative abilities to scientific soundness and technical practice is far from trivial. == Statistical binary classification == Statistical classification is a problem studied in machine learning. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification. Some of the methods commonly used for binary classification are: Decision trees Random forests Bayesian networks Support vector machines Neural networks Logistic regression Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example random forests perform better than SVM classifiers for 3D point clouds. == Evaluation of binary classifiers == There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence - both types are useful, but they have very different properties. Given a classification of a specific data set, there are four basic data: the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These can be arranged into a 2’‘’•2 contingency table, with columns corresponding to actual value - condition positive (CP) or condition negative (CN) - and rows corresponding to classification value - test outcome positive or test outcome negative. There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form true positive row ratio or false negative column ratio, though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair - the other four numbers are the complements. The column ratios are True Positive Rate (TPR, aka Sensitivity or recall), with complement the False Negative Rate (FNR); and True Negative Rate (TNR, aka Specificity, SPC), with complement False Positive Rate (FPR). These are the proportion of the population with the condition (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence. The row ratios are Positive Predictive Value (PPV, aka precision), with complement the False Discovery Rate (FDR); and Negative Predictive Value (NPV), with complement the False Omission Rate (FOR). These are the proportion of the population with a given test result for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence. In diagnostic testing, the main ratios used are the true column ratios - True Positive Rate and True Negative Rate - where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) - Positive Predictive Value and True Positive Rate - where they are known as precision and recall. One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP’‘’•TN)/(FP’‘’•FN) = (TP/FN)/(FP/TN); this has a useful interpretation - as an odds ratio - and is prevalence-independent. There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youdens J statistic, the uncertainty coefficient, the Phi coefficient, and Cohens kappa. == Converting continuous values to binary == Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff. However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as positive with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as positive as the one of 52 mIU/ml. == See also == Examples of Bayesian inference Classification rule Detection theory Kernel methods Matthews correlation coefficient Multiclass classification Multi-label classification One-class classification Prosecutors fallacy Receiver operating characteristic Thresholding (image processing) Type I and type II errors Uncertainty coefficient, aka Proficiency Qualitative property == References == == Bibliography == Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book) John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 ([2] Kernel Methods Book) Bernhard Sch’‘Î lkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, MA, 2002. (Partly available on line: [3].) ISBN 0-262-19475-9"
Extended Boolean model,Extended Boolean model,,,"The Extended Boolean model was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in information retrieval. The Boolean model doesnt consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the Vector Space Model with the properties of Boolean algebra and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the Standard Boolean model it wasnt. Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing. Other research has shown that relevance feedback and query expansion can be integrated with extended Boolean query processing. == Definitions == In the Extended Boolean model, a document is represented as a vector (similarly to in the vector model). Each i dimension corresponds to a separate term associated with the document. The weight of term Kx associated with document dj is measured by its normalized Term frequency and can be defined as: w x , j = f x , j Š—ç’ ’• I d f x m a x i I d f i { w {x,j}=f {x,j}*{}{max {i}Idf {i}}}} where Idfx is inverse document frequency. The weight vector associated with document dj can be represented as: v d j = [ w 1 , j , w 2 , j , Š—ç’ , w i , j ] { {d {j}}=[w {1,j},w {2,j}, ,w {i,j}]} == The 2 Dimensions Example == Considering the space composed of two terms Kx and Ky only, the corresponding term weights are w1 and w2. Thus, for query qor = (Kx Š—ç’ ÎŒ Ky), we can calculate the similarity with the following formula: s i m ( q o r , d ) = w 1 2 + w 2 2 2 { sim(q {or},d)={^{2}+w {2}^{2}}{2}}}} For query qand = (Kx Š—ç’ Î Ky), we can use: s i m ( q a n d , d ) = 1 Š—ç’ ’« ( 1 Š—ç’ ’« w 1 ) 2 + ( 1 Š—ç’ ’« w 2 ) 2 2 { sim(q {and},d)=1-{)^{2}+(1-w {2})^{2}}{2}}}} == Generalizing the idea and P-norms == We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances. This can be done using P-norms which extends the notion of distance to include p-distances, where 1 Š—ç’ p Š—ç’ Š—ç’ is a new parameter. A generalized conjunctive query is given by: q o r = k 1 Š—ç’ ÎŒ p k 2 Š—ç’ ÎŒ p . . . . Š—ç’ ÎŒ p k t { q {or}=k {1} ^{p}k {2} ^{p}.... ^{p}k {t}} The similarity of q o r { q {or}} and d j { d {j}} can be defined as: : s i m ( q o r , d j ) = w 1 p + w 2 p + . . . . + w t p t p { sim(q {or},d {j})={[{p}]{^{p}+w {2}^{p}+....+w {t}^{p}}{t}}}} A generalized disjunctive query is given by: q a n d = k 1 Š—ç’ Î p k 2 Š—ç’ Î p . . . . Š—ç’ Î p k t { q {and}=k {1} ^{p}k {2} ^{p}.... ^{p}k {t}} The similarity of q a n d { q {and}} and d j { d {j}} can be defined as: s i m ( q a n d , d j ) = 1 Š—ç’ ’« ( 1 Š—ç’ ’« w 1 ) p + ( 1 Š—ç’ ’« w 2 ) p + . . . . + ( 1 Š—ç’ ’« w t ) p t p { sim(q {and},d {j})=1-{[{p}]{)^{p}+(1-w {2})^{p}+....+(1-w {t})^{p}}{t}}}} == Examples == Consider the query q = (K1 Š—ç’ Î K2) Š—ç’ ÎŒ K3. The similarity between query q and document d can be computed using the formula: s i m ( q , d ) = ( 1 Š—ç’ ’« ( ( 1 Š—ç’ ’« w 1 ) p + ( 1 Š—ç’ ’« w 2 ) p 2 p ) ) p + w 3 p 2 p { sim(q,d)={[{p}]{]{({)^{p}+(1-w {2})^{p}}{2}}}}))^{p}+w {3}^{p}}{2}}}} == Improvements over the Standard Boolean Model == Lee and Fox compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC. Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively. The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even Fuzzy retrieval techniques. The Standard Boolean model is still the most efficient. == Further reading == Adaptive Feedback Methods in an Extended Boolean Model by Dr.Jongpill Choi Interpolation of the extended Boolean retrieval model Fox, E.; Betrabet, S.; Koushik, M.; Lee, W. (1992), Information Retrieval: Algorithms and Data structures; Extended Boolean model, Prentice-Hall, Inc. Skorkovsk’‘’, Lucie; Ircing, Pavel (2009), Experiments with Automatic Query Formulation in the Extended Boolean Model, Springer Berlin / Heidelberg == See also == Information retrieval == References =="
Fuzzy retrieval,Fuzzy retrieval,,,"Fuzzy retrieval techniques are based on the Extended Boolean model and the Fuzzy set theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the P-norms algorithm. == Mixed Min and Max model (MMM) == In fuzzy-set theory, an element has a varying degree of membership, say dA, to a given set A instead of the traditional membership choice (is an element/is not an element). In MMM each index term has a fuzzy set associated with it. A documents weight with respect to an index term A is considered to be the degree of membership of the document in the fuzzy set associated with A. The degree of membership for union and intersection are defined as follows in Fuzzy set theory: d A Š—ç’ Î© B = m i n ( d A , d B ) { d {A B}=min(d {A},d {B})} d A Š—ç’ ÎŽ B = m a x ( d A , d B ) { d {A B}=max(d {A},d {B})} According to this, documents that should be retrieved for a query of the form A or B, should be in the fuzzy set associated with the union of the two sets A and B. Similarly, the documents that should be retrieved for a query of the form A and B, should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the or query to be max(dA, dB) and the similarity of the document to the and query to be min(dA, dB). The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the min and max document weights. Given a document D with index-term weights dA1, dA2, ..., dAn for terms A1, A2, ..., An, and the queries: Qor = (A1 or A2 or ... or An)Qand = (A1 and A2 and ... and An) the query-document similarity in the MMM model is computed as follows: SlM(Qor, D) = Cor1 * max(dA1, dA2, ..., dAn) + Cor2 * min(dA1, dA2, ..., dAn)SlM(Qand, D) = Cand1 * min(dA1, dA2, ..., dAn) + Cand2 * max(dA1, dA2 ..., dAn) where Cor1, Cor2 are softness coefficients for the or operator, and Cand1, Cand2 are softness coefficients for the and operator. Since we would like to give the maximum of the document weights more importance while considering an or query and the minimum more importance while considering an and query, generally we have Cor1 > Cor2 and Cand1 > Cand2. For simplicity it is generally assumed that Cor1 = 1 - Cor2 and Cand1 = 1 - Cand2. Lee and Fox experiments indicate that the best performance usually occurs with Cand1 in the range [0.5, 0.8] and with Cor1 > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the Standard Boolean model. == Paice model == The Paice model is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity: S ( D , Q ) = Š—ç’ ’” i = 1 n r i Š—ç’ ’« 1 Š—ç’ ’• w d i Š—ç’ ’” j = 1 n r j Š—ç’ ’« 1 { S(D,Q)= {i=1}^{n}{*w {di}}{ {j=1}^{n}r^{j-1}}}} where r is a constant coefficient and wdi is arranged in ascending order for and queries and descending order for or queries. When n = 2 the Paice model shows the same behavior as the MMM model. The experiments of Lee and Fox have shown that setting the r to 1.0 for and queries and 0.7 for or queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of min or max of a set of term weights each time an and or or clause is considered, which can be done in O(n). The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an and clause or an or clause is being considered. This requires at least an 0(n log n) sorting algorithm. A good deal of floating point calculation is needed too. == Improvements over the Standard Boolean model == Lee and Fox compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement: These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three. == Recent work == Recently Kang et al.. have devised a fuzzy retrieval system indexed by concept identification. If we look at documents on a pure Tf-idf approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document. They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents. Zadrozny revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by: assuming linguistic terms as importance weights of keywords also in documents taking into account the uncertainty concerning the representation of documents and queries interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the ZadehŠ—ç’ Îés fuzzy logic (calculus of linguistic statements) addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval. == See also == Information retrieval == Further reading == Fox, E.; S. Betrabet; M. Koushik; W. Lee (1992), Information Retrieval: Algorithms and Data structures; Extended Boolean model, Prentice-Hall, Inc. == References =="
Feature selection,Feature selection,,,"In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons: simplification of models to make them easier to interpret by researchers/users, shorter training times, to avoid the curse of dimensionality, enhanced generalization by reducing overfitting (formally, reduction of variance) The central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant or irrelevant features are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated. Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples. == IntroductionEdit == A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods. Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model. Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set. Common measures include the mutual information, the pointwise mutual information, Pearson product-moment correlation coefficient, inter/intra class distance or the scores of significance tests for each class/feature combinations. Filters are usually less computationally intensive than wrappers, but they produce a feature set which is not tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than the set from a wrapper, usually giving lower prediction performance than a wrapper. However the feature set doesnt contain the assumptions of a prediction model, and so is more useful for exposing the relationships between the features. Many filters provide a feature ranking rather than an explicit best feature subset, and the cut off point in the ranking is chosen via cross-validation. Filter methods have also been used as a preprocessing step for wrapper methods, allowing a wrapper to be used on larger problems. Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process. The exemplar of this approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are selected by the LASSO algorithm. Improvements to the LASSO include Bolasso which bootstraps samples, and FeaLect which scores all the features based on combinatorial analysis of regression coefficients. One other popular approach is the Recursive Feature Elimination algorithm, commonly used with Support Vector Machines to repeatedly construct a model and remove features with low weights. These approaches tend to be between filters and wrappers in terms of computational complexity. In traditional statistics, the most popular form of feature selection is stepwise regression, which is a wrapper technique. It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round. The main control issue is deciding when to stop the algorithm. In machine learning, this is typically done by cross-validation. In statistics, some criteria are optimized. This leads to the inherent problem of nesting. More robust methods have been explored, such as branch and bound and piecewise linear network. == Subset selectionEdit == Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into Wrappers, Filters and Embedded. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to Wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in and specific to a model. Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features. Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset. The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a programs maximum allowed run time has been surpassed, etc. Alternative search-based techniques are based on targeted projection pursuit which finds low-dimensional projections of the data that score highly: the features that have the largest projections in the lower-dimensional space are then selected. Search approaches include: Exhaustive Best first Simulated annealing Genetic algorithm Greedy forward selection Greedy backward elimination Particle swarm optimization Targeted projection pursuit Scatter Search Variable Neighborhood Search Two popular filter metrics for classification problems are correlation and mutual information, although neither are true metrics or distance measures in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual distance - they should rather be regarded as scores. These scores are computed between a candidate feature (or set of features) and the desired output category. There are, however, true metrics that are a simple function of the mutual information; see here. Other available filter metrics include: Class separability Error probability Inter-class distance Probabilistic distance Entropy Consistency-based feature selection Correlation-based feature selection == Optimality criteriaEdit == The choice of optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common ones incorporate a measure of accuracy, penalised by the number of features selected (e.g. the Bayesian information criterion). The oldest are Mallowss Cp statistic and Akaike information criterion (AIC). These add variables if the t-statistic is bigger than 2 {}} . Other criteria are Bayesian information criterion (BIC) which uses log Š—çÎ’ n {}}} , minimum description length (MDL) which asymptotically uses log Š—çÎ’ n {}}} , Bonferroni / RIC which use 2 log Š—çÎ’ p {}}} , maximum dependency feature selection, and a variety of new criteria that are motivated by false discovery rate (FDR) which use something close to 2 log Š—çÎ’ p q {{q}}}}} . == Structure learningEdit == Filter feature selection is a specific case of a more general paradigm called Structure Learning. Feature selection finds the relevant feature set for a specific target variable whereas structure learning finds the relationships between all the variables, usually by expressing these relationships as a graph. The most common structure learning algorithms assume the data is generated by a Bayesian Network, and so the structure is a directed graphical model. The optimal solution to the filter feature selection problem is the Markov blanket of the target node, and in a Bayesian Network, there is a unique Markov Blanket for each node. == Minimum-redundancy-maximum-relevance (mRMR) feature selectionEdit == Peng et al. proposed a feature selection method that can use either mutual information, correlation, or distance/similarity scores to select features. The aim is to penalise a features relevancy by its redundancy in the presence of the other selected features. The relevance of a feature set S for the class c is defined by the average value of all mutual information values between the individual feature fi and the class c as follows: D ( S , c ) = 1 | S | Š—ç’ ’” f i Š—ç’ ’ S I ( f i ; c ) { D(S,c)={{|S|}} {f {i} S}I(f {i};c)} . The redundancy of all features in the set S is the average value of all mutual information values between the feature fi and the feature fj: R ( S ) = 1 | S | 2 Š—ç’ ’” f i , f j Š—ç’ ’ S I ( f i ; f j ) { R(S)={{|S|^{2}}} {f {i},f {j} S}I(f {i};f {j})} The mRMR criterion is a combination of two measures given above and is defined as follows: m R M R = max S [ 1 | S | Š—ç’ ’” f i Š—ç’ ’ S I ( f i ; c ) Š—ç’ ’« 1 | S | 2 Š—ç’ ’” f i , f j Š—ç’ ’ S I ( f i ; f j ) ] . { = {S}[{{|S|}} {f {i} S}I(f {i};c)-{{|S|^{2}}} {f {i},f {j} S}I(f {i};f {j})].} Suppose that there are n full-set features. Let xi be the set membership indicator function for feature fi, so that xi=1 indicates presence and xi=0 indicates absence of the feature fi in the globally optimal feature set. Let c i = I ( f i ; c ) { c {i}=I(f {i};c)} and a i j = I ( f i ; f j ) { a {ij}=I(f {i};f {j})} . The above may then be written as an optimization problem: m R M R = max x Š—ç’ ’ { 0 , 1 } n [ Š—ç’ ’” i = 1 n c i x i Š—ç’ ’” i = 1 n x i Š—ç’ ’« Š—ç’ ’” i , j = 1 n a i j x i x j ( Š—ç’ ’” i = 1 n x i ) 2 ] . { = {x ^{n}}[{^{n}c {i}x {i}}{ {i=1}^{n}x {i}}}-{^{n}a {ij}x {i}x {j}}{( {i=1}^{n}x {i})^{2}}}].} The mRMR algorithm is an approximation of the theoretically optimal maximum-dependency feature selection algorithm that maximizes the mutual information between the joint distribution of the selected features and the classification variable. As mRMR approximates the combinatorial estimation problem with a series of much smaller problems, each of which only involves two variables, it thus uses pairwise joint probabilities which are more robust. In certain situations the algorithm may underestimate the usefulness of features as it has no way to measure interactions between features which can increase relevancy. This can lead to poor performance when the features are individually useless, but are useful when combined (a pathological case is found when the class is a parity function of the features). Overall the algorithm is more efficient (in terms of the amount of data required) than the theoretically optimal max-dependency selection, yet produces a feature set with little pairwise redundancy. mRMR is an instance of a large class of filter methods which trade off between relevancy and redundancy in different ways. === Global optimization formulationsEdit === mRMR is a typical example of an incremental greedy strategy for feature selection: once a feature has been selected, it cannot be deselected at a later stage. While mRMR could be optimized using floating search to reduce some features, it might also be reformulated as a global quadratic programming optimization problem as follows: Q P F S : min x { ’âÎ± x T H x Š—ç’ ’« x T F } s.t. Š—ç’ ’” i = 1 n x i = 1 , x i Š—ç’ Î‚ 0 { : { } ^{T}H - ^{T}F} {i=1}^{n}x {i}=1,x {i} 0} where F n ’‘’• 1 = [ I ( f 1 ; c ) , Š—ç’ , I ( f n ; c ) ] T { F {n 1}=[I(f {1};c), ,I(f {n};c)]^{T}} is the vector of feature relevancy assuming there are n features in total, H n ’‘’• n = [ I ( f i ; f j ) ] i , j = 1 Š—ç’ n { H {n n}=[I(f {i};f {j})] {i,j=1 n}} is the matrix of feature pairwise redundancy, and x n ’‘’• 1 { {n 1}} represents relative feature weights. QPFS is solved via quadratic programming. It is recently shown that QFPS is biased towards features with smaller entropy, due to its placement of the feature self redundancy term I ( f i ; f i ) { I(f {i};f {i})} on the diagonal of H. Another global formulation for the mutual information based feature selection problem is based on the conditional relevancy: S P E C C M I : max x { x T Q x } s.t. Š—ç’ Î‚ x Š—ç’ Î‚ = 1 , x i Š—ç’ Î‚ 0 { } : { } ^{T}Q } | |=1,x {i} 0} where Q i i = I ( f i ; c ) { Q {ii}=I(f {i};c)} and Q i j = I ( f i ; c | f j ) , i Š—ç’ ’ j { Q {ij}=I(f {i};c|f {j}),i j} . An advantage of SPECCMI is that it can be solved simply via finding the dominant eigenvector of Q, thus is very scalable. SPECCMI also handles second-order feature interaction. For high-dimensional and small sample data (e.g., dimensionality > 105 and the number of samples < 103), the Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) is useful. HSIC Lasso optimization problem is given as H S I C L a s s o : min x 1 2 Š—ç’ ’” k , l = 1 n x k x l HSIC ( f k , f l ) Š—ç’ ’« Š—ç’ ’” k = 1 n x k HSIC ( f k , c ) + ’â’ Š—ç’ Î‚ x Š—ç’ Î‚ 1 , s.t. x 1 , Š—ç’ , x n Š—ç’ Î‚ 0 , { } : { }{{2}} {k,l=1}^{n}x {k}x {l}{}(f {k},f {l})- {k=1}^{n}x {k}{}(f {k},c)+ | | {1}, x {1}, ,x {n} 0,} where HSIC ( f k , c ) = tr ( K ’Ç’Ù ( k ) L ’Ç’Ù ) {(f {k},c)={}({ }}^{(k)}{ }})} is a kernel-based independence measure called the (empirical) Hilbert-Schmidt independence criterion (HSIC), tr ( Š—ç’“’ ) {( )} denotes the trace, ’â’ { } is the regularization parameter, K ’Ç’Ù ( k ) = ’â’ê K ( k ) ’â’ê { }}^{(k)}= ^{(k)} } and L ’Ç’Ù = ’â’ê L ’â’ê { }}= } are input and output centered Gram matrices, K i , j ( k ) = K ( u k , i , u k , j ) { K {i,j}^{(k)}=K(u {k,i},u {k,j})} and L i , j = L ( c i , c j ) { L {i,j}=L(c {i},c {j})} are Gram matrices, K ( u , u Š—ç’ ) { K(u,u)} and L ( c , c Š—ç’ ) { L(c,c)} are kernel functions, ’â’ê = I m Š—ç’ ’« 1 m 1 m 1 m T { = {m}-{{m}} {m} {m}^{T}} is the centering matrix, I m { {m}} is the m-dimensional identity matrix (m: the number of samples), 1 m { {m}} is the m-dimensional vector with all ones, and Š—ç’ Î‚ Š—ç’“’ Š—ç’ Î‚ 1 { | | {1}} is the Š—ç’£’ê 1 { {1}} -norm. HSIC always takes a non-negative value, and is zero if and only if two random variables are statistically independent when a universal reproducing kernel such as the Gaussian kernel is used. The HSIC Lasso can be written as H S I C L a s s o : min x 1 2 Š—ç’ Î‚ L ’Ç’Ù Š—ç’ ’« Š—ç’ ’” k = 1 n x k K ’Ç’Ù ( k ) Š—ç’ Î‚ F 2 + ’â’ Š—ç’ Î‚ x Š—ç’ Î‚ 1 , s.t. x 1 , Š—ç’ , x n Š—ç’ Î‚ 0 , { } : { }{{2}}|{ }}- {k=1}^{n}x {k}{ }}^{(k)}| {F}^{2}+ | | {1}, x {1}, ,x {n} 0,} where Š—ç’ Î‚ Š—ç’“’ Š—ç’ Î‚ F { | | {F}} is the Frobenius norm. The optimization problem is a Lasso problem, and thus it can be efficiently solved with a state-of-the-art Lasso solver such as the dual augmented Lagrangian method. == Correlation feature selectionEdit == The Correlation Feature Selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other. The following equation gives the merit of a feature subset S consisting of k features: M e r i t S k = k r c f ’Ç’Ù k + k ( k Š—ç’ ’« 1 ) r f f ’Ç’Ù . { {S {k}}={}}}{}}}}}.} Here, r c f ’Ç’Ù {}}} is the average value of all feature-classification correlations, and r f f ’Ç’Ù {}}} is the average value of all feature-feature correlations. The CFS criterion is defined as follows: C F S = max S k [ r c f 1 + r c f 2 + Š—ç’“’Ù + r c f k k + 2 ( r f 1 f 2 + Š—ç’“’Ù + r f i f j + Š—ç’“’Ù + r f k f 1 ) ] . { = {S {k}}[{}+r {cf {2}}+ +r {cf {k}}}{f {2}}+ +r {f {i}f {j}}+ +r {f {k}f {1}})}}}].} The r c f i { r {cf {i}}} and r f i f j { r {f {i}f {j}}} variables are referred to as correlations, but are not necessarily Pearsons correlation coefficient or Spearmans ’ŒÎ. Dr. Mark Halls dissertation uses neither of these, but uses three different measures of relatedness, minimum description length (MDL), symmetrical uncertainty, and relief. Let xi be the set membership indicator function for feature fi; then the above can be rewritten as an optimization problem: C F S = max x Š—ç’ ’ { 0 , 1 } n [ ( Š—ç’ ’” i = 1 n a i x i ) 2 Š—ç’ ’” i = 1 n x i + Š—ç’ ’” i Š—ç’ ’ j 2 b i j x i x j ] . { = {x ^{n}}[{^{n}a {i}x {i})^{2}}{ {i=1}^{n}x {i}+ {i j}2b {ij}x {i}x {j}}}].} The combinatorial problems above are, in fact, mixed 0-1 linear programming problems that can be solved by using branch-and-bound algorithms. == Regularized treesEdit == The features from a decision tree or a tree ensemble are shown to be redundant. A recent method called regularized tree can be used for feature subset selection. Regularized trees penalize using a variable similar to the variables selected at previous tree nodes for splitting the current node. Regularized trees only need build one tree model (or one tree ensemble model) and thus are computationally efficient. Regularized trees naturally handle numerical and categorical features, interactions and nonlinearities. They are invariant to attribute scales (units) and insensitive to outliers, and thus, require little data preprocessing such as normalization. Regularized random forest (RRF) is one type of regularized trees. The guided RRF is an enhanced RRF which is guided by the importance scores from an ordinary random forest. == Overview on metaheuristics methodsEdit == A metaheuristic is a general description of an algorithm dedicated to solve difficult (typically NP-hard problem) optimization problems for which there is no classical solving methods. Generally, a metaheuristic is a stochastics algorithm tending to reach a global optima. There are many metaheuristics, from a simple local search to a complex global search algorithm. === Main principlesEdit === The feature selection methods are typically presented in three classes based on how they combine the selection algorithm and the model building. ==== Filter methodEdit ==== Filter type methods select variables regardless of the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting. However, filter methods tend to select redundant variables because they do not consider the relationships between variables. Therefore, they are mainly used as a pre-process method. ==== Wrapper methodEdit ==== Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. The significant computation time when the number of variables is large. ==== Embedded methodEdit ==== Embedded methods have been recently proposed that try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously. === Application of feature selection metaheuristicsEdit === This is a survey of the application of feature selection metaheuristics lately used in the literature. This survey was realized by J. Hammon in her thesis. == Feature selection embedded in learning algorithmsEdit == Some learning algorithms perform feature selection as part of their overall operation. These include: l 1 { l {1}} -regularization techniques, such as sparse regression, LASSO, and l 1 { l {1}} -SVM Regularized trees, e.g. regularized random forest implemented in the RRF package Decision tree Memetic algorithm Random multinomial logit (RMNL) Auto-encoding networks with a bottleneck-layer Submodular feature selection Local learning based feature selection. Compared with traditional methods, it does not involve any heuristic search, can easily handle multi-class problems, and works for both linear and nonlinear problems. It is also supported by a strong theoretical foundation. Numeric experiments showed that the method can achieve a close-to-optimal solution even when data contains >1M irrelevant features. == See alsoEdit == Cluster analysis Data mining Dimensionality reduction Feature extraction Hyperparameter optimization == ReferencesEdit == == Further readingEdit == == External linksEdit == Feature Selection Package, Arizona State University (Matlab Code) NIPS challenge 2003 (see also NIPS) Naive Bayes implementation with feature selection in Visual Basic (includes executable and source code) Minimum-redundancy-maximum-relevance (mRMR) feature selection program FEAST (Open source Feature Selection algorithms in C and MATLAB)"
Feature Selection Toolbox,Feature Selection Toolbox,,,"Feature Selection Toolbox (FST) is software primarily for feature selection in the machine learning domain, written in C++, developed at the Institute of Information Theory and Automation (UTIA), of the Czech Academy of Sciences. == Version 1 == The first generation of Feature Selection Toolbox (FST1) was a Windows application with user interface allowing users to apply several sub-optimal, optimal and mixture-based feature selection methods on data stored in a trivial proprietary textual flat file format. == Version 3 == The third generation of Feature Selection Toolbox (FST3) was a library without user interface, written to be more efficient and versatile than the original FST1. FST3 supports several standard data mining tasks, more specifically, data preprocessing and classification, but its main focus is on feature selection. In feature selection context, it implements several common as well as less usual techniques, with particular emphasis put on threaded implementation of various sequential search methods (a form of hill-climbing). Implemented methods include individual feature ranking, floating search, oscillating search (suitable for very high-dimension problems) in randomized or deterministic form, optimal methods of branch and bound type, probabilistic class distance criteria, various classifier accuracy estimators, feature subset size optimization, feature selection with pre-specified feature weights, criteria ensembles, hybrid methods, detection of all equivalent solutions, or two-criterion optimization. FST3 is more narrowly specialized than popular software like the Waikato Environment for Knowledge Analysis Weka, RapidMiner or PRTools. By default, techniques implemented in the toolbox are predicated on the assumption that the data is available as a single flat file in a simple proprietary format or in Weka format ARFF, where each data point is described by a fixed number of numeric attributes. FST3 is provided without user interface, and is meant to be used by users familiar both with machine learning and C++ programming. The older FST1 software is more suitable for simple experimenting or educational purposes because it can be used with no need to code in C++. == History == In 1999, development of the first Feature Selection Toolbox version started at UTIA as part of a Ph.D. thesis. It was originally developed in Optima++ (later renamed Power++) RAD C++ environment. In 2002, the development of the first FST generation has been suspended, mainly due to end of Sybases support of the then used development environment. In 2002-2008, FST kernel was recoded and used for research experimentation within UTIA only. In 2009, 3rd FST kernel recoding from scratch begun. In 2010, FST3 was made publicly available in form of a C++ library without GUI. The accompanying webpage collects feature selection related links, references, documentation and the original FST1 available for download. In 2011, an update of FST3 to version 3.1 included new methods (particularly a novel dependency-aware feature ranking suitable for very-high-dimension recognition problems) and core code improvements. == See also == Feature selection Pattern recognition Machine learning Data mining OpenNN, Open neural networks library for predictive analytics Weka, comprehensive and popular Java open-source software from University of Waikato RapidMiner, formerly Yet Another Learning Environment (YALE) a commercial machine learning framework PRTools of the Delft University of Technology Infosel++ specialized in information theory based feature selection Tooldiag a C++ pattern recognition toolbox List of numerical analysis software == References == == External links == Official website"
Heaps' law,Heaps' law,,,"In linguistics, Heaps law (also called Herdans law) is an empirical law which describes the number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation). It can be formulated as V R ( n ) = K n ’â { V {R}(n)=Kn^{ }} where VR is the number of distinct words in an instance text of size n. K and ’â are free parameters determined empirically. With English text corpora, typically K is between 10 and 100, and ’â is between 0.4 and 0.6. The law is frequently attributed to Harold Stanley Heaps, but was originally discovered by Gustav Herdan (1960). Under mild assumptions, the Herdan-Heaps law is asymptotically equivalent to Zipfs law concerning the frequencies of individual words within a text. This is a consequence of the fact that the type-token relation (in general) of a homogenous text can be derived from the distribution of its types. Heaps law means that as more instance text is gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn. Heaps law also applies to situations in which the vocabulary is just some set of distinct types which are attributes of some collection of objects. For example, the objects could be people, and the types could be country of origin of the person. If persons are selected randomly (that is, we are not selecting based on country of origin), then Heaps law says we will quickly have representatives from most countries (in proportion to their population) but it will become increasingly difficult to cover the entire set of countries by continuing this method of sampling. == Notes == == References == Baeza-Yates, Ricardo; Navarro, Gonzalo, Block addressing indices for approximate text retrieval, Journal of the American Society for Information Science, 51 (1): 69-82, doi:10.1002/(sici)1097-4571(2000)51:1<69::aid-asi10>3.0.co;2-c . Egghe, L. (2007), Untangling Herdans law and Heaps law: Mathematical and informetric arguments, Journal of the American Society for Information Science and Technology, 58 (5): 702, doi:10.1002/asi.20524 . Heaps, Harold Stanley (1978), Information Retrieval: Computational and Theoretical Aspects, Academic Press . Heaps law is proposed in Section 7.5 (pp. 206-208). Herdan, Gustav (1960), Type-token mathematics, The Hague: Mouton . Kornai, Andras (1999), Zipfs law outside the middle range, in Rogers, James, Proceedings of the Sixth Meeting on Mathematics of Language, University of Central Florida, pp. 347-356 . MiliŠäŒÎka, JiÎÎé’‘ (2009), Type-token & Hapax-token Relation: A Combinatorial Model, Glottotheory. International Journal of Theoretical Linguistics, 1 (2): 99-110, doi:10.1515/glot-2009-0009 . van Leijenhorst, D. C; van der Weide, Th. P. (2005), A formal derivation of Heaps Law, Information Sciences, 170 (2-4): 263-272, doi:10.1016/j.ins.2004.03.006 . This article incorporates material from Heaps law on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License."
K-means clustering,K-means clustering,,,"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm. == Description == Given a set of observations (x1, x2, Š—ç’, xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (Š—ç’ n) sets S = {S1, S2, Š—ç’, Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find: a r g m i n S Š—ç’ ’” i = 1 k Š—ç’ ’” x Š—ç’ ’ S i Š—ç’ Î‚ x Š—ç’ ’« ’â i Š—ç’ Î‚ 2 = a r g m i n S Š—ç’ ’” i = 1 k | S i | Var Š—çÎ’ S i { }{ }} {i=1}^{k} { S {i}}| -{} {i}|^{2}={ }{ }} {i=1}^{k}|S {i}| S {i}} where ’â i is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster: Š—ç’ ’” Cluster C i Š—ç’ ’” Dimension d Š—ç’ ’” x , y Š—ç’ ’ C i ( x d Š—ç’ ’« y d ) 2 { {{}C {i}}, {{}d}, {x,y, ,C {i}}(x {d}-y {d})^{2}} Because the total variance is constant, this is also equivalent to maximizing the squared deviations between points in different clusters (between-cluster sum of squares, BCSS). == History == The term k-means was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1957. The standard algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation, though it wasnt published outside of Bell Labs until 1982. In 1965, E. W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy. == Algorithms == === Standard algorithm === The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the k-means algorithm; it is also referred to as Lloyds algorithm, particularly in the computer science community. Given an initial set of k means m1(1),Š—ç’,mk(1) (see below), the algorithm proceeds by alternating between two steps: Assignment step: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Since the sum of squares is the squared Euclidean distance, this is intuitively the nearest mean. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means). S i ( t ) = { x p : Š—ç’ Î‚ x p Š—ç’ ’« m i ( t ) Š—ç’ Î‚ 2 Š—ç’ Š—ç’ Î‚ x p Š—ç’ ’« m j ( t ) Š—ç’ Î‚ 2 Š—ç’ ’ j , 1 Š—ç’ j Š—ç’ k } , { S {i}^{(t)}={ x {p}:{ |}x {p}-m {i}^{(t)}{ |}^{2}x {p}-m {j}^{(t)}{ |}^{2} j,1 j k{ }},} where each x p { x {p}} is assigned to exactly one S ( t ) { S^{(t)}} , even if it could be assigned to two or more of them. Update step: Calculate the new means to be the centroids of the observations in the new clusters. m i ( t + 1 ) = 1 | S i ( t ) | Š—ç’ ’” x j Š—ç’ ’ S i ( t ) x j { m {i}^{(t+1)}={{|S {i}^{(t)}|}} {x {j} S {i}^{(t)}}x {j}} Since the arithmetic mean is a least-squares estimator, this also minimizes the within-cluster sum of squares (WCSS) objective. The algorithm has converged when the assignments no longer change. Since both steps optimize the WCSS objective, and there only exists a finite number of such partitionings, the algorithm must converge to a (local) optimum. There is no guarantee that the global optimum is found using this algorithm. The algorithm is often presented as assigning objects to the nearest cluster by distance. The standard algorithm aims at minimizing the WCSS objective, and thus assigns by least sum of squares, which is exactly equivalent to assigning by the smallest Euclidean distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures. ==== Initialization methods ==== Commonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the clusters randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas the approach by Bradley and Fayyad performs consistently in the best group and K-means++ performs generally well. Demonstration of the standard algorithm As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, k-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which k-means takes exponential time, that is 2’âÎ©(n), to converge. These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial. The assignment step is also referred to as expectation step, the update step as maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm. === Complexity === Regarding computational complexity, finding the optimal solution to the k-means clustering problem for observations in d dimensions is: NP-hard in general Euclidean space d even for 2 clusters NP-hard for a general number of clusters k even in the plane If k and d (the dimension) are fixed, the problem can be exactly solved in time O ( n d k + 1 ) { O(n^{dk+1})} , where n is the number of entities to be clustered Thus, a variety of heuristic algorithms such as Lloyds algorithm given above are generally used. The running time of Lloyds algorithm is naively O ( n k d i ) { O(nkdi)} , where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyds algorithm is therefore often considered to be of linear complexity in practice, although it is in the worst case superpolynomial. Following are some of the recent insights into this algorithm complexity behavior. In the worst-case, Lloyds algorithm needs i = 2 ’âÎ© ( n ) { i=2^{ ({})}} iterations, so that the worst-case complexity of Lloyds algorithm is superpolynomial. Lloyds k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in [ 0 , 1 ] d { [0,1]^{d}} , if each point is independently perturbed by a normal distribution with mean 0 and variance ’Œ’Ü 2 { ^{2}} , then the expected running time of k-means algorithm is bounded by O ( n 34 k 34 d 8 log 4 Š—çÎ’ ( n ) / ’Œ’Ü 6 ) { O(n^{34}k^{34}d^{8} ^{4}(n)/ ^{6})} , which is a polynomial in n, k, d and 1 / ’Œ’Ü { 1/ } . Better bounds are proven for simple cases. For example, showed that the running time of k-means algorithm is bounded by O ( d n 4 M 2 ) { O(dn^{4}M^{2})} for n points in an integer lattice { 1 , Š—ç’ , M } d { ^{d}} . Lloyds algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use the triangle inequality in order to create bounds and accelerate Lloyds algorithm. === Variations === Jenks natural breaks optimization: k-means applied to univariate data k-medians clustering uses the median in each dimension instead of the mean, and this way minimizes L 1 { L {1}} norm (Taxicab geometry). k-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions. Fuzzy C-Means Clustering is a soft version of K-means, where each data point has a fuzzy degree of belonging to each cluster. Gaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means. k-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective. The filtering algorithm uses kd-trees to speed up each k-means step. Some methods attempt to speed up each k-means step using the triangle inequality. Escape local optima by swapping points between clusters. The Spherical k-means clustering algorithm is suitable for textual data. Hierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset. Internal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters. Minkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters. Mini-batch K-means: K-means variation using mini batch samples for data sets that do not fit into memory. == Discussion == Three key features of k-means which make it efficient are often regarded as its biggest drawbacks: Euclidean distance is used as a metric and variance is used as a measure of cluster scatter. The number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set. Convergence to a local minimum may produce counterintuitive (wrong) results (see example in Fig.). A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of k = 3 { k=3} onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With k = 2 { k=2} , the two visible clusters (one containing two species) will be discovered, whereas with k = 3 { k=3} one of the two clusters will be split into two even parts. In fact, k = 2 { k=2} is more appropriate for this data set, despite the data set containing 3 classes. As with any other clustering algorithm, the k-means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works well on some data sets, while failing on others. The result of k-means can also be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the mouse example. The Gaussian models used by the Expectation-maximization algorithm (which can be seen as a generalization of k-means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). == Applications == k-means clustering, in particular when using heuristics such as Lloyds algorithm, is rather easy to implement and apply even on large data sets. As such, it has been successfully used in various topics, including market segmentation, computer vision, geostatistics, astronomy and agriculture. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration. === Vector quantization === k-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis. === Cluster analysis === In cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters). However, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case!). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation of the algorithm is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms have been developed since. === Feature learning === k-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, we have a choice of encoding functions, but we can use for example the thresholded matrix-product of the datum with the centroid locations, the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, by transforming the sample-cluster distance through a Gaussian RBF, one effectively obtains the hidden layer of a radial basis function network. This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data than the sophisticated methods, for equivalent performance, because each data point only contributes to one feature rather than multiple. == Relation to other statistical machine learning algorithms == === Gaussian Mixture Model === k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a k-means problem into a Gaussian mixture model. Another generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of codebook vectors. K-means corresponds to the special case of using a single codebook vector, with a weight of 1. === Principal component analysis (PCA) === It was proven that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA), and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data have 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the 1st PCA direction. Cutting the line at the center of mass separate the clusters (this is the continuous relaxation of the discreet cluster indicator). If the data have 3 clusters, the 2-dimensional plane spanned by 3 cluster centroids is the best 2-D projection. This plane is also the first 2 PCA dimensions. Well-separated clusters are effectively modeled by ball-shape clusters and thus discovered by K-means. Non-ball-shaped clusters are hard to separate when they are close-by. For example, two half-moon shaped clusters intertwined in space does not separate well when projected to PCA subspace. But neither is k-means supposed to do well on this data. However, that PCA is a useful relaxation of k-means clustering was not a new result, and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions. === Mean shift clustering === Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that there is no need to choose the number of clusters, because mean shift is likely to find only a few clusters if indeed only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants much as k-means does. === Independent component analysis (ICA) === It has been shown in that under sparsity assumptions and when input data is pre-processed with the whitening transformation k-means produces the solution to the linear Independent component analysis task. This aids in explaining the successful application of k-means to feature learning. === Bilateral filtering === k-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to K-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance. == Similar problems == The set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids. == Software implementations == Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25988 seconds. The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration. === Free Software/Open Source === the following implementations are available under Free/Open Source Software licenses, with publicly available source code. Accord.NET contains C# implementations for k-means, k-means++ and k-modes. CrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations. ELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms. Julia contains a k-means implementation in the JuliaStats Clustering package. Mahout contains a MapReduce based k-means. MLPACK contains a C++ implementation of k-means. Octave contains k-means. OpenCV contains a k-means implementation. PSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset. R contains three k-means variations. SciPy and scikit-learn contain multiple k-means implementations. Spark MLlib implements a distributed k-means algorithm. Torch contains an unsup package that provides k-means clustering. Weka contains k-means and x-means. === Proprietary === The following implementations are available under proprietary license terms, and may not have publicly available source code. Ayasdi MATLAB Mathematica RapidMiner SAP HANA SAS SPSS Stata == See also == K-means++ Centroidal Voronoi tessellation k q-flats Linde-Buzo-Gray algorithm Self-organizing map Head/tail Breaks == References =="
K-nearest neighbors algorithm,K-nearest neighbors algorithm,,,"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression: In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors. k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms. Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm is not to be confused with k-means, another popular machine learning technique. == Statistical setting == Suppose we have pairs ( X , Y ) , ( X 1 , Y 1 ) , Š—ç’ , ( X n , Y n ) { (X,Y),(X {1},Y {1}), ,(X {n},Y {n})} taking values in R d ’‘’• { 1 , 2 } { ^{d} } , where Y is the class label of X, so that X | Y = r Š—ç’ P r { X|Y=r P {r}} for r = 1 , 2 { r=1,2} (and probability distributions P r { P {r}} ). Given some norm Š—ç’ Î‚ Š—ç’“’ Š—ç’ Î‚ { | |} on R d { ^{d}} and a point x Š—ç’ ’ R d { x ^{d}} , let ( X ( 1 ) , Y ( 1 ) ) , Š—ç’ , ( X ( n ) , Y ( n ) ) { (X {(1)},Y {(1)}), ,(X {(n)},Y {(n)})} be a reordering of the training data such that Š—ç’ Î‚ X ( 1 ) Š—ç’ ’« x Š—ç’ Î‚ Š—ç’ Š—ç’“’Ù Š—ç’ Š—ç’ Î‚ X ( n ) Š—ç’ ’« x Š—ç’ Î‚ { |X {(1)}-x| |X {(n)}-x|} . == Algorithm == The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples. In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point. A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has also been employed with correlation coefficients such as Pearson and Spearman. Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis. A drawback of the basic majority voting classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number. One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM. == Parameter selection == The best choice of k depends upon the data; generally, larger values of k reduce the effect of noise on the classification, but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm. The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling. Another popular approach is to scale features by the mutual information of the training data with the training classes. In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method. == The 1-nearest neighbour classifier == The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is C n 1 n n ( x ) = Y ( 1 ) { C {n}^{1nn}(x)=Y {(1)}} . As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). == The weighted nearest neighbour classifier == The k-nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight 1 / k { 1/k} and all others 0 weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the ith nearest neighbour is assigned a weight w n i { w {ni}} , with Š—ç’ ’” i = 1 n w n i = 1 { {i=1}^{n}w {ni}=1} . An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds. Let C n w n n { C {n}^{wnn}} denote the weighted nearest classifier with weights { w n i } i = 1 n { } {i=1}^{n}} . Subject to regularity conditions on to class distributions the excess risk has the following asymptotic expansion R R ( C n w n n ) Š—ç’ ’« R R ( C B a y e s ) = ( B 1 s n 2 + B 2 t n 2 ) { 1 + o ( 1 ) } , {} {}(C {n}^{wnn})-{} {}(C^{Bayes})=(B {1}s {n}^{2}+B {2}t {n}^{2}),} for constants B 1 { B {1}} and B 2 { B {2}} where s n 2 = Š—ç’ ’” i = 1 n w n i 2 { s {n}^{2}= {i=1}^{n}w {ni}^{2}} and t n = n Š—ç’ ’« 2 / d Š—ç’ ’” i = 1 n w n i { i 1 + 2 / d Š—ç’ ’« ( i Š—ç’ ’« 1 ) 1 + 2 / d } { t {n}=n^{-2/d} {i=1}^{n}w {ni}-(i-1)^{1+2/d}}} . The optimal weighting scheme { w n i Š—ç’ ’• } i = 1 n { ^{*}} {i=1}^{n}} , that balances the two terms in the display above, is given as follows: set k Š—ç’ ’• = Š—ç’ B n 4 d + 4 Š—ç’ ’“ { k^{*}= Bn^{{d+4}} } , w n i Š—ç’ ’• = 1 k Š—ç’ ’• [ 1 + d 2 Š—ç’ ’« d 2 k Š—ç’ ’• 2 / d { i 1 + 2 / d Š—ç’ ’« ( i Š—ç’ ’« 1 ) 1 + 2 / d } ] { w {ni}^{*}={{k^{*}}}[1+{{2}}-{{2{k^{*}}^{2/d}}}-(i-1)^{1+2/d}}]} for i = 1 , 2 , Š—ç’ , k Š—ç’ ’• { i=1,2, ,k^{*}} and w n i Š—ç’ ’• = 0 { w {ni}^{*}=0} for i = k Š—ç’ ’• + 1 , Š—ç’ , n { i=k^{*}+1, ,n} . With optimal weights the dominant term in the asymptotic expansion of the excess risk is O ( n Š—ç’ ’« 4 d + 4 ) {}(n^{-{{d+4}}})} . Similar results are true when using a bagged nearest neighbour classifier. == Properties == k-NN is a special case of a variable-bandwidth, kernel density balloon estimator with a uniform kernel. The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed. k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). Various improvements to the k-NN speed are possible by using proximity graphs. For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of R Š—ç’ ’• Š—ç’ R k N N Š—ç’ R Š—ç’ ’• ( 2 Š—ç’ ’« M R Š—ç’ ’• M Š—ç’ ’« 1 ) { R^{*} R {k } R^{*}(2-{}{M-1}})} where R Š—ç’ ’• { R^{*}} is the Bayes error rate (which is the minimal error rate possible), R k N N { R {kNN}} is the k-NN error rate, and M is the number of classes in the problem. For M = 2 { M=2} and as the Bayesian error rate R Š—ç’ ’• { R^{*}} approaches zero, this limit reduces to not more than twice the Bayesian error rate. == Error rates == There are many results on the error rate of the k nearest neighbour classifiers. The k-nearest neighbour classifier is strongly (that is for any joint distribution on ( X , Y ) { (X,Y)} ) consistent provided k := k n { k:=k {n}} diverges and k n / n { k {n}/n} converges to zero as n Š—çÎ¾’« Š—ç’ { n } . Let C n k n n { C {n}^{knn}} denote the k nearest neighbour classifier based on a training set of size n. Under certain regularity conditions, the excess risk yields the following asymptotic expansion R R ( C n k n n ) Š—ç’ ’« R R ( C B a y e s ) = { B 1 1 k + B 2 ( k n ) 4 / d } { 1 + o ( 1 ) } , {} {}(C {n}^{knn})-{} {}(C^{Bayes})={{k}}+B {2}({{n}})^{4/d}},} for some constants B 1 { B {1}} and B 2 { B {2}} . The choice k Š—ç’ ’• = Š—ç’ B n 4 d + 4 Š—ç’ ’“ { k^{*}= Bn^{{d+4}} } offers a trade off between the two terms in the above display, for which the k Š—ç’ ’• { k^{*}} -nearest neighbour error converges to the Bayes error at the optimal (minimax) rate O ( n Š—ç’ ’« 4 d + 4 ) {}(n^{-{{d+4}}})} . == Metric learning == The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric. == Feature extraction == When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space. An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV): Haar face detection Mean-shift tracking analysis PCA or Fisher LDA projection into feature space, followed by k-NN classification == Dimension reduction == For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality. The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same). Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding. For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, random projections, sketches or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option. == Decision boundary == Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity. == Data reduction == Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the prototypes and can be found as follows: Select the class-outliers, that is, training data that are classified incorrectly by k-NN (for a given k) Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the absorbed points that can be correctly classified by k-NN using prototypes. The absorbed points can then be removed from the training set. === Selection of class-outliers === A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include: random error insufficient training examples of this class (an isolated example appears instead of a cluster) missing important features (the classes are separated in other dimensions which we do not know) too many training examples of other classes (unbalanced classes) that create a hostile background for the given small class Class outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes. === CNN for data reduction === Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification. It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set. Given a training set X, CNN works iteratively: Scan all elements of X, looking for an element x whose nearest prototype from U has a different label than x. Remove x from X and add it to U Repeat the scan until no more prototypes are added to U. Use U instead of X for classification. The examples that are not prototypes are called absorbed points. It is efficient to scan the training examples in order of decreasing border ratio. The border ratio of a training example x is defined as a(x) = ||x-y||/||x-y|| where ||x-y|| is the distance to the closest example y having a different color than x, and ||x-y|| is the distance from y to its closest example x with the same label as x. The border ratio is in the interval [0,1] because ||x-y||never exceeds ||x-y||. This ordering gives preference to the borders of the classes for inclusion in the set of prototypesU. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x . The border ratio a(x) = ||x-y|| / ||x-y||is the attribute of the initial point x. Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet. CNN model reduction for k-NN classifiers == k-NN regression == In k-NN regression, the k-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows: Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples. Order the labeled examples by increasing distance. Find a heuristically optimal number k of nearest neighbors, based on RMSE. This is done using cross validation. Calculate an inverse distance weighted average with the k-nearest multivariate neighbors. == k-NN outlier == The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier. Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis. == Validation of results == A confusion matrix or matching matrix is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied. == See also == Nearest centroid classifier Closest pair of points problem == References == == Further reading == When Is Nearest Neighbor Meaningful? Belur V. Dasarathy, ed. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. ISBN 0-8186-8930-7. Shakhnarovish, Darrell, and Indyk, eds. (2005). Nearest-Neighbor Methods in Learning and Vision. MIT Press. ISBN 0-262-19547-X. CS1 maint: Uses editors parameter (link) M’‘ kel’‘ H Pekkarinen A (2004-07-26). Estimation of forest stand volumes by Landsat TM imagery and stand-level field-inventory data. Forest Ecology and Management. 196 (2-3): 245-255. doi:10.1016/j.foreco.2004.02.049. Fast k nearest neighbor search using GPU. In Proceedings of the CVPR Workshop on Computer Vision on GPU, Anchorage, Alaska, USA, June 2008. V. Garcia and E. Debreuve and M. Barlaud. Scholarpedia article on k-NN google-all-pairs-similarity-search"
Nearest neighbor,Nearest neighbor,,,"Nearest neighbor may refer to: Nearest neighbor search in pattern recognition and in computational geometry Nearest-neighbor interpolation for interpolating data Nearest neighbor graph in geometry Nearest neighbor function in probability theory The k-nearest neighbor algorithm in machine learning, an application of generalized forms of nearest neighbor search and interpolation The nearest neighbour algorithm for approximately solving the travelling salesman problem The nearest neighbor method for determining the thermodynamics of nucleic acids The nearest neighbor method for calculating distances between clusters in hierarchical clustering. == See also == Moore neighborhood Von Neumann neighborhood"
Language model,Language model,,,"A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability P ( w 1 , Š—ç’ , w m ) { P(w {1}, ,w {m})} to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications, especially ones that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications. In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases recognize speech and wreck a nice beach are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model. Language models are used in information retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the documents language model P ( Q Š—ç’ Î£ M d ) { P(Q M {d})} . Commonly, the unigram language model is used for this purposeŠ—ç’ ’–otherwise known as the bag of words model. Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1. == Unigram models == A unigram model used in information retrieval can be treated as the combination of several one-state finite automata. It splits the probabilities of different terms in a context, e.g. from P ( t 1 t 2 t 3 ) = P ( t 1 ) P ( t 2 Š—ç’ Î£ t 1 ) P ( t 3 Š—ç’ Î£ t 1 t 2 ) { P(t {1}t {2}t {3})=P(t {1})P(t {2} t {1})P(t {3} t {1}t {2})} to P uni ( t 1 t 2 t 3 ) = P ( t 1 ) P ( t 2 ) P ( t 3 ) { P {}(t {1}t {2}t {3})=P(t {1})P(t {2})P(t {3})} . In this model, the probability of each word only depends on that words own probability in the document, so we only have one-state finite automata as units. The automaton itself has a probability distribution over the entire vocabulary of the model, summing to 1. The following is an illustration of a unigram model of a document. Š—ç’ ’” term in doc P ( term ) = 1 { {}P({})=1,} The probability generated for a specific query is calculated as P ( query ) = Š—ç’ Î term in query P ( term ) { P({})= {}P({})} For different documents, we can build their own unigram models, with different hitting probabilities of words in it. And we use probabilities from different documents to generate different hitting probabilities for a query. Then we can rank documents for a query according to the generating probabilities. Next is an example of two unigram models of two documents. In information retrieval contexts, unigram language models are often smoothed to avoid instances where P(term) = 0. A common approach is to generate a maximum-likelihood model for the entire collection and linearly interpolate the collection model with a maximum-likelihood model for each document to create a smoothed document model. == n-gram models == In an n-gram model, the probability P ( w 1 , Š—ç’ , w m ) { P(w {1}, ,w {m})} of observing the sentence w 1 , Š—ç’ , w m { w {1}, ,w {m}} is approximated as P ( w 1 , Š—ç’ , w m ) = Š—ç’ Î i = 1 m P ( w i Š—ç’ Î£ w 1 , Š—ç’ , w i Š—ç’ ’« 1 ) Š—ç’ ’ Š—ç’ Î i = 1 m P ( w i Š—ç’ Î£ w i Š—ç’ ’« ( n Š—ç’ ’« 1 ) , Š—ç’ , w i Š—ç’ ’« 1 ) { P(w {1}, ,w {m})= {i=1}^{m}P(w {i} w {1}, ,w {i-1}) {i=1}^{m}P(w {i} w {i-(n-1)}, ,w {i-1})} Here, it is assumed that the probability of observing the ith word wi in the context history of the preceding i Š—ç’ ’« 1 words can be approximated by the probability of observing it in the shortened context history of the preceding n Š—ç’ ’« 1 words (nth order Markov property). The conditional probability can be calculated from n-gram model frequency counts: P ( w i Š—ç’ Î£ w i Š—ç’ ’« ( n Š—ç’ ’« 1 ) , Š—ç’ , w i Š—ç’ ’« 1 ) = c o u n t ( w i Š—ç’ ’« ( n Š—ç’ ’« 1 ) , Š—ç’ , w i Š—ç’ ’« 1 , w i ) c o u n t ( w i Š—ç’ ’« ( n Š—ç’ ’« 1 ) , Š—ç’ , w i Š—ç’ ’« 1 ) { P(w {i} w {i-(n-1)}, ,w {i-1})={ (w {i-(n-1)}, ,w {i-1},w {i})}{ (w {i-(n-1)}, ,w {i-1})}}} The words bigram and trigram language model denote n-gram model language models with n = 2 and n = 3, respectively. Typically, however, the n-gram model probabilities are not derived directly from the frequency counts, because models derived this way have severe problems when confronted with any n-grams that have not explicitly been seen before. Instead, some form of smoothing is necessary, assigning some of the total probability mass to unseen words or n-grams. Various methods are used, from simple add-one smoothing (assign a count of 1 to unseen n-grams) to more sophisticated models, such as Good-Turing discounting or back-off models. === Example === In a bigram (n = 2) language model, the probability of the sentence I saw the red house is approximated as P ( I, saw, the, red, house ) Š—ç’ ’ P ( I Š—ç’ Î£ Š—ç’ÈÎŒ s Š—ç’ÈÎ© ) P ( saw Š—ç’ Î£ I ) P ( the Š—ç’ Î£ saw ) P ( red Š—ç’ Î£ the ) P ( house Š—ç’ Î£ red ) P ( Š—ç’ÈÎŒ / s Š—ç’ÈÎ© Š—ç’ Î£ house ) {)&P({} s )P({})P({})P({})P({})P( /s )}} whereas in a trigram (n = 3) language model, the approximation is P ( I, saw, the, red, house ) Š—ç’ ’ P ( I Š—ç’ Î£ Š—ç’ÈÎŒ s Š—ç’ÈÎ© , Š—ç’ÈÎŒ s Š—ç’ÈÎ© ) P ( saw Š—ç’ Î£ Š—ç’ÈÎŒ s Š—ç’ÈÎ© , I ) P ( the Š—ç’ Î£ I, saw ) P ( red Š—ç’ Î£ saw, the ) P ( house Š—ç’ Î£ the, red ) P ( Š—ç’ÈÎŒ / s Š—ç’ÈÎ© Š—ç’ Î£ red, house ) {)&P({} s , s )P({} s ,I)P({})P({})P({})P( /s )}} Note that the context of the first n - 1 n-grams is filled with start-of-sentence markers, typically denoted <s>. Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence *I saw the would always be higher than that of the longer sentence I saw the red house. == Exponential language models == Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is P ( w 1 , Š—ç’ , w m ) = 1 Z ( w 1 , Š—ç’ , w m Š—ç’ ’« 1 ) exp Š—çÎ’ ( a T f ( w 1 , Š—ç’ , w m ) ) { P(w {1}, ,w {m})={{Z(w {1}, ,w {m-1})}}(a^{T}f(w {1}, ,w {m}))} where Z ( w 1 , Š—ç’ , w m Š—ç’ ’« 1 ) { Z(w {1}, ,w {m-1})} is the partition function, a { a} is the parameter vector, and f ( w 1 , Š—ç’ , w m ) { f(w {1}, ,w {m})} is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain n-gram. It is helpful to use a prior on a { a} or some form of regularization. The log-bilinear model is another example of an exponential language model. == Neural language models == Neural language models (or Continuous space language models) use continuous representations or embeddings of words to make their predictions. These models make use of Neural networks. Continuous space embeddings help to alleviate the curse of dimensionality in language modeling: as language models are trained on larger and larger texts, the number of unique words (the vocabulary) increases and the number of possible sequences of words increases exponentially with the size of the vocabulary, causing a data sparsity problem because for each of the exponentially many sequences, statistics are needed to properly estimate probabilities. Neural networks avoid this problem by representing words in a distributed way, as non-linear combinations of weights in a neural net. The neural net architecture might be feed-forward or recurrent. Typically, neural net language models are constructed and trained as probabilistic classifiers that learn to predict a probability distribution P ( w t | c o n t e x t ) Š—ç’ ’ t Š—ç’ ’ V { P(w {t}| ), t V} . I.e., the network is trained to predict a probability distribution over the vocabulary, given some linguistic context. This is done using standard neural net training algorithms such as stochastic gradient descent with backpropagation. The context might be a fixed-size window of previous words, so that the network predicts P ( w t | w t Š—ç’ ’« k , Š—ç’ , w t Š—ç’ ’« 1 ) { P(w {t}|w {t-k}, ,w {t-1})} from a feature vector representing the previous k words. Another option is to use future words as well as past words as features, so that the estimated probability is P ( w t | w t Š—ç’ ’« k , Š—ç’ , w t Š—ç’ ’« 1 , w t + 1 , Š—ç’ , w t + k ) { P(w {t}|w {t-k}, ,w {t-1},w {t+1}, ,w {t+k})} . A third option, that allows faster training, is to invert the previous problem and make a neural network learn the context, given a word. One then maximizes the log-probability Š—ç’ ’” Š—ç’ ’« k Š—ç’ j Š—ç’ ’« 1 , j Š—ç’ k log Š—çÎ’ P ( w t + j | w t ) { {-k j-1,,j k} P(w {t+j}|w {t})} This is called a skip-gram language model, and is the basis of the popular word2vec program. Instead of using neural net language models to produce actual probabilities, it is common to instead use the distributed representation encoded in the networks hidden layers as representations of words; each word is then mapped onto an n-dimensional real vector called the word embedding, where n is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as linear combinations, capturing a form of compositionality. For example, in some such models, if v is the function that maps a word w to its n-d vector representation, then v ( k i n g ) Š—ç’ ’« v ( m a l e ) + v ( f e m a l e ) Š—ç’ ’ v ( q u e e n ) { v( )-v( )+v( ) v( )} where Š—ç’ ’ is made precise by stipulating that its right-hand side must be the nearest neighbor of the value of the left-hand side. == Other models == A positional language model is one that describes the probability of given words occurring close to one another in a text, not necessarily immediately adjacent. Similarly, bag-of-concepts models leverage on the semantics associated with multi-word expressions such as buy christmas present, even when they are used in information-rich sentences like today I bought a lot of very nice Christmas presents. == See also == Factored language model Cache language model Katzs back-off model == Notes == == References == === Further reading === == External links == Lecture notes on language models, parsing and machine translation with PCFG, CRF, MaxEnt, MEMM, EM, GLM, HMM by Michael Collins(Columbia University) CSLM - Free toolkit for feedforward neural language models DALM - Fast, Free software for language model queries IRSTLM - Free software for language modeling Kylm (Kyoto Language Modeling Toolkit) - Free language modeling toolkit in Java KenLM - Fast, Free software for language modeling LMSharp - Free language model toolkit for Kneser-Ney-smoothed n-gram models and recurrent neural network models MITLM - MIT Language Modeling toolkit. Free software NPLM - Free toolkit for feedforward neural language models OpenGrm NGram library - Free software for language modeling. Built on OpenFst. OxLM - Free toolkit for feedforward neural language models Positional Language Model RandLM - Free software for randomised language modeling RNNLM - Free recurrent neural network language model toolkit SRILM - Proprietary software for language modeling VariKN - Free software for creating, growing and pruning Kneser-Ney smoothed n-gram models. Language models trained on newswire data Web Page NGram Viewer"
Modeling language,Modeling language,,,"A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure. == Overview == A modeling language can be graphical or textual. Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions. An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. Not all modeling languages are executable, and for those that are, the use of them doesnt necessarily mean that programmers are no longer required. On the contrary, executable modeling languages are intended to amplify the productivity of skilled programmers, so that they can address more challenging problems, such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science, project management and systems engineering: Behavior Trees are a formal, graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN, and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems, a schematic representation of an algorithm or a stepwise process, and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages, which include IDEF0 for functional modeling, IDEF1X for information modeling, IDEF3 for business process modeling, IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java, C++, C#) programs and design patterns. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling, and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology, namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years, particularly in the areas of model checking, graphically oriented simulation, and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modelers perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0, the current version, supports thirteen different diagram techniques, and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture, virtualization, service-oriented architecture (SOA), cloud computing, and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. AADL (AADL) is a modeling language that supports early and repeated analyses of a systems architecture with respect to performance-critical properties through an exetendable notation, a tool framework, and precisely defined semantics. Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL), a language that aims to model ecological energetics & global economics. === Textual types === Information models can also be expressed in formalized natural languages, such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands), etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy, which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge, requirements and dictionaries, taxonomies and ontologies, but also information about individual things. All that information is expressed in one language and therefore it can all be integrated, independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions, that use natural language terms and formalized phrases. For example, a geographic information model might consist of a number of Gellish Formal English expressions, such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as city) and phrases that represent relation types (such as <is located in> and <is classified as a>) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models, because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements, queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS, AMPL, GAMS, LPL, MPL, OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization, which is supported by certain language elements like sets, indices, algebraic expressions, powerful sparse index and data handling variables, constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency, nondeterminism, synchronization, and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-Specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore, such language offers a distinct vocabulary, syntax, and notation for each stage, such as discovery, analysis, design, architecture, contraction, etc. For example, for the analysis phase of a project, the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase, however, logical design notation is used to depict relationship between software entities. In addition, the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems, most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages, so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words, concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Object-oriented ==== Object modeling language are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code, using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models, which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML), before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics, designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines, including computer science, information management, business process modeling, software engineering, and systems engineering. Modeling languages can be used to specify: system requirements, structures and behaviors. Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g., customers, operators, analysts, designers) can better understand the system being modeled. The more mature modeling languages are precise, consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements, structures and behaviors, but not much else. Executable modeling languages applied with proper tool support, however, are expected to automate system verification and validation, simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders, domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie, Sindre and Lindland (2003), since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague, but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict, but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this, the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholders knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. ==== Comprehensibility appropriateness ==== Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible, easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this, the goal should be as simple as possible and that each symbol in the language has a unique representation. ==== Tool appropriateness ==== To ensure that the domain actually modelled is usable for analyzing and further processing, the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. ==== Organizational appropriateness ==== The language used is appropriate for the organizational context, e.g. that the language is standardized within the organization, or that it is supported by tools that are chosen as standard in the organization. == See also == == References == == External links == Fundamental Modeling Concepts Software Modeling Languages Portal BIP -- Incremental Component-based Construction of Real-time Systems Gellish Formal English == Further reading == John Krogstie (2003) Evaluating UML using a generic quality framework . SINTEF Telecom andInformatics and IDI, NTNU, Norway Krogstie and S’‘’ lvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences. Anna Gunhild Nysetvold and John Krogstie (2005). Assessing business processing modeling languages using a generic quality framework. Institute of computer and information sciences."
Multiclass classification,Multiclass classification,,,"Not to be confused with multi-label classification. In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification). While some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies. Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance. == General strategies == The existing multi-class classification techniques can be categorized into (i) Transformation to binary (ii) Extension from binary and (iii) Hierarchical classification. === Transformation to binary === This section discusses strategies for reducing the problem of multiclass classification to multiple binary classification problems. It can be categorized into One vs Rest and One vs One. The techniques developed based on reducing the multi-class problem into multiple binary problems can also be called problem transformation techniques. ==== One-vs.-rest ==== The one-vs.-rest (or one-vs.-all, OvA or OvR, one-against-all, OAA) strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a class label; discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample. In pseudocode, the training algorithm for an OvA learner constructed from a binary classification learner L is as follows: Inputs: L, a learner (training algorithm for binary classifiers) samples X labels y where yi Š—ç’ ’ {1, Š—ç’ K} is the label for the sample Xi Output: a list of classifiers fk for k Š—ç’ ’ {1, Š—ç’, K} Procedure: For each k in {1, Š—ç’, K} Construct a new label vector z where zi = 1 if yi = k and zi = 0 otherwise Apply L to X, z to obtain fk Making decisions means applying all classifiers to an unseen sample x and predicting the label k for which the corresponding classifier reports the highest confidence score: y ^ = arg max k Š—ç’ ’ { 1 Š—ç’ K } f k ( x ) {}={{ ! }};f {k}(x)} Although this strategy is popular, it is a heuristic that suffers from several problems. Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives. ==== One-vs.-one ==== In the one-vs.-one (OvO) reduction, one trains K (K Š—ç’ ’« 1) / 2 binary classifiers for a K-way multiclass problem; each receives the samples of a pair of classes from the original training set, and must learn to distinguish these two classes. At prediction time, a voting scheme is applied: all K (K Š—ç’ ’« 1) / 2 classifiers are applied to an unseen sample and the class that got the highest number of +1 predictions gets predicted by the combined classifier. Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the same number of votes. === Extension from binary === This section discusses strategies of extending the existing binary classifiers to solve multi-class classification problems. Several algorithms have been developed based on neural networks, decision trees, k-nearest neighbors, naive Bayes, support vector machines and extreme learning machines to address multi-class classification problems. These types of techniques can also be called as algorithm adaptation techniques. ==== Neural networks ==== Multilayer perceptrons provide a natural extension to the multi-class problem. Instead of just having one neuron in the output layer, with binary output, one could have N binary neurons leading to multi-class classification. ===== Extreme learning machines ===== Extreme Learning Machines (ELM) is a special case of single hidden layer feed-forward neural networks (SLFNs) where in the input weights and the hidden node biases can be chosen at random. Many variants and developments are made to the ELM for multiclass classification. ==== k-nearest neighbours ==== k-nearest neighbors kNN is considered among the oldest non-parametric classification algorithms. To classify an unknown example, the distance from that example to every other training example is measured. The k smallest distances are identified, and the most represented class by these k nearest neighbours is considered the output class label. ==== Naive Bayes ==== Naive Bayes is a successful classifier based upon the principle of maximum a posteriori (MAP). This approach is naturally extensible to the case of having more than two classes, and was shown to perform well in spite of the underlying simplifying assumption of conditional independence. ==== Decision trees ==== Decision trees are a powerful classification technique. The tree tries to infer a split of the training data based on the values of the available features to produce a good generalization. The algorithm can naturally handle binary or multiclass classification problems. The leaf nodes can refer to either of the K classes concerned. ==== Support vector machines ==== Support vector machines are based upon the idea of maximizing the margin i.e. maximizing the minimum distance from the separating hyperplane to the nearest example. The basic SVM supports only binary classification, but extensions have been proposed to handle the multiclass classification case as well. In these extensions, additional parameters and constraints are added to the optimization problem to handle the separation of the different classes. === Hierarchical classification === Hierarchical classification tackles the multi-class classification problem by dividing the output space i.e. into a tree. Each parent node is divided into multiple child nodes and the process is continued until each child node represents only one class. Several methods have been proposed based on hierarchical classification. == Learning Paradigms == Based on learning paradigms, the existing multi-class classification techniques can be classified into batch learning and online learning. Batch learning algorithms require all the data samples to be available beforehand. It trains the model using the entire training data and then predicts the test sample using the found relationship. The online learning algorithms, on the other hand, incrementally build their models in sequential iterations. In iteration t, an online algorithm receives a sample, xt and predicts its label Î’t using the current model; the algorithm then receives yt, the true label of xt and updates its model based on the sample-label pair: (xt, yt). Recently, a new learning paradigm called progressive learning technique has been developed. The progressive learning technique is capable of not only learning from new samples but also capable of learning new classes of data and yet retain the knowledge learnt thus far. == See also == Binary classification One-class classification Multi-label classification Multiclass perceptron in Perceptron == Notes == == References =="
Statistical classification,Statistical classification,,,"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into spam or non-spam classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. A, B, AB or O, for blood type), ordinal (e.g. large, medium or small), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term classifier sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term classification normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article. == Relation to other problems == Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc. A common subclass of classification is probabilistic classification. Algorithms of this nature use statistical inference to find the best class for a given instance. Unlike other algorithms, which simply output a best class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes. The best class is normally then selected as the one with the highest probability. However, such an algorithm has numerous advantages over non-probabilistic classifiers: It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier). Correspondingly, it can abstain when its confidence of choosing any particular output is too low. Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. == Frequentist procedures == Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fishers linear discriminant function as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on slight different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation. == Bayesian procedures == Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the sub-populations associated with the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised. Some Bayesian procedures involve the calculation of group membership probabilities: these can be viewed as providing a more informative outcome of a data analysis than a simple attribution of a single group-label to each new observation. == Binary and multiclass classification == Classification can be thought of as two separate problems - binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers. == Feature vectors == Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent). Features may variously be binary (e.g. male or female); categorical (e.g. A, B, AB or O, for blood type); ordinal (e.g. large, medium or small); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure). If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words. Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10) == Linear classifiers == A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product. The predicted category is the one with the highest score. This type of score function is known as a linear predictor function and has the following general form: score Š—çÎ’ ( X i , k ) = ’â k Š—ç’“’ X i , { ( {i},k)={} {k} {i},} where Xi is the feature vector for instance i, ’â k is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k. In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k. Algorithms with this basic setup are known as linear classifiers. What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted. Examples of such algorithms are Logistic regression and Multinomial logistic regression Probit regression The perceptron algorithm Support vector machines Linear discriminant analysis. == Algorithms == Examples of classification algorithms include: Linear classifiers Fishers linear discriminant Logistic regression Naive Bayes classifier Perceptron Support vector machines Least squares support vector machines Quadratic classifiers Kernel estimation k-nearest neighbor Boosting (meta-algorithm) Decision trees Random forests Neural networks FMM Neural Networks Learning vector quantization == Evaluation == Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science. The measures precision and recall are popular metrics used to evaluate the quality of a classification system. More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms. As a performance metric, the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes. Further, it will not penalize an algorithm for simply rearranging the classes. == Application domains == Classification has many applications. In some of these it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken. Computer vision Medical imaging and medical image analysis Optical character recognition Video tracking Drug discovery and development Toxicogenomics Quantitative structure-activity relationship Geostatistics Speech recognition Handwriting recognition Biometric identification Biological classification Statistical natural language processing Document classification Internet search engines Credit scoring Pattern recognition Micro-array classification == See also == Class membership probabilities Classification rule Binary classification Compound term processing Data mining Fuzzy logic Data warehouse Information retrieval Artificial intelligence Machine learning Recommender system List of datasets for machine learning research == References == == External links == Classifier showdown A practical comparison of classification algorithms. Statistical Pattern Recognition Toolbox for Matlab. TOOLDIAG Pattern recognition toolbox. Statistical classification software based on adaptive kernel density estimation. PAL Classification Suite written in Java. kNN and Potential energy (Applet), University of Leicester scikit-learn a widely used package in python Weka A java based package with an extensive variety of algorithms."
Naive Bayes classifier,Naive Bayes classifier,,,"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s, and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics and computer science literature, Naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes theorem in the classifiers decision rule, but naive Bayes is not (necessarily) a Bayesian method. == Introduction == Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features. For some types of probability models, naive Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods. Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers. Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests. An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification. == Probabilistic model == Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector x = ( x 1 , Š—ç’ , x n ) { =(x {1}, ,x {n})} representing some n features (independent variables), it assigns to this instance probabilities p ( C k Š—ç’ Î£ x 1 , Š—ç’ , x n ) { p(C {k} x {1}, ,x {n}),} for each of k possible outcomes or classes C k { C {k}} . The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable. Using Bayes theorem, the conditional probability can be decomposed as p ( C k Š—ç’ Î£ x ) = p ( C k ) p ( x Š—ç’ Î£ C k ) p ( x ) { p(C {k} )={) p( C {k})}{p( )}},} In plain English, using Bayesian probability terminology, the above equation can be written as posterior = prior ’‘’• likelihood evidence {={}{}},} In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on C { C} and the values of the features F i { F {i}} are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model p ( C k , x 1 , Š—ç’ , x n ) { p(C {k},x {1}, ,x {n}),} which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability: p ( C k , x 1 , Š—ç’ , x n ) = p ( x 1 , Š—ç’ , x n , C k ) = p ( x 1 Š—ç’ Î£ x 2 , Š—ç’ , x n , C k ) p ( x 2 , Š—ç’ , x n , C k ) = p ( x 1 Š—ç’ Î£ x 2 , Š—ç’ , x n , C k ) p ( x 2 Š—ç’ Î£ x 3 , Š—ç’ , x n , C k ) p ( x 3 , Š—ç’ , x n , C k ) = Š—ç’ = p ( x 1 Š—ç’ Î£ x 2 , Š—ç’ , x n , C k ) p ( x 2 Š—ç’ Î£ x 3 , Š—ç’ , x n , C k ) Š—ç’ p ( x n Š—ç’ ’« 1 Š—ç’ Î£ x n , C k ) p ( x n Š—ç’ Î£ C k ) p ( C k ) {,x {1}, ,x {n})&=p(x {1}, ,x {n},C {k})&=p(x {1} x {2}, ,x {n},C {k})p(x {2}, ,x {n},C {k})&=p(x {1} x {2}, ,x {n},C {k})p(x {2} x {3}, ,x {n},C {k})p(x {3}, ,x {n},C {k})&= &=p(x {1} x {2}, ,x {n},C {k})p(x {2} x {3}, ,x {n},C {k}) p(x {n-1} x {n},C {k})p(x {n} C {k})p(C {k})}} Now the naive conditional independence assumptions come into play: assume that each feature F i { F {i}} is conditionally independent of every other feature F j { F {j}} for j Š—ç’ ’ i { j i} , given the category C { C} . This means that p ( x i Š—ç’ Î£ x i + 1 , Š—ç’ , x n , C k ) = p ( x i Š—ç’ Î£ C k ) { p(x {i} x {i+1}, ,x {n},C {k})=p(x {i} C {k}),} . Thus, the joint model can be expressed as p ( C k Š—ç’ Î£ x 1 , Š—ç’ , x n ) Š—ç’ Î p ( C k , x 1 , Š—ç’ , x n ) Š—ç’ Î p ( C k ) p ( x 1 Š—ç’ Î£ C k ) p ( x 2 Š—ç’ Î£ C k ) p ( x 3 Š—ç’ Î£ C k ) Š—ç’“’Ù Š—ç’ Î p ( C k ) Š—ç’ Î i = 1 n p ( x i Š—ç’ Î£ C k ) . { x {1}, ,x {n})& p(C {k},x {1}, ,x {n})& p(C {k}) p(x {1} C {k}) p(x {2} C {k}) p(x {3} C {k}) & p(C {k}) {i=1}^{n}p(x {i} C {k}),.}} This means that under the above independence assumptions, the conditional distribution over the class variable C { C} is: p ( C k Š—ç’ Î£ x 1 , Š—ç’ , x n ) = 1 Z p ( C k ) Š—ç’ Î i = 1 n p ( x i Š—ç’ Î£ C k ) { p(C {k} x {1}, ,x {n})={{Z}}p(C {k}) {i=1}^{n}p(x {i} C {k})} where the evidence Z = p ( x ) { Z=p( )} is a scaling factor dependent only on x 1 , Š—ç’ , x n { x {1}, ,x {n}} , that is, a constant if the values of the feature variables are known. === Constructing a classifier from the probability model === The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label y ^ = C k {}=C {k}} for some k as follows: y ^ = argmax k Š—ç’ ’ { 1 , Š—ç’ , K } p ( C k ) Š—ç’ Î i = 1 n p ( x i Š—ç’ Î£ C k ) . {}={{ }} p(C {k}) {i=1}^{n}p(x {i} C {k}).} == Parameter estimation and event models == A classs prior may be calculated by assuming equiprobable classes (i.e., priors = 1 / (number of classes)), or by calculating an estimate for the class probability from the training set (i.e., (prior for a given class) = (number of samples in the class) / (total number of samples)). To estimate the parameters for a features distribution, one must assume a distribution or generate nonparametric models for the features from the training set. The assumptions on distributions of features are called the event model of the Naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused. === Gaussian naive Bayes === When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contains a continuous attribute, x { x} . We first segment the data by the class, and then compute the mean and variance of x { x} in each class. Let ’â c { {c}} be the mean of the values in x { x} associated with class c, and let ’Œ’Ü c 2 { {c}^{2}} be the variance of the values in x { x} associated with class c. Suppose we have collected some observation value v { v} . Then, the probability distribution of v { v} given a class c { c} , p ( x = v Š—ç’ Î£ c ) { p(x=v c)} , can be computed by plugging v { v} into the equation for a Normal distribution parameterized by ’â c { {c}} and ’Œ’Ü c 2 { {c}^{2}} . That is, p ( x = v Š—ç’ Î£ c ) = 1 2 ’Œ’ ’Œ’Ü c 2 e Š—ç’ ’« ( v Š—ç’ ’« ’â c ) 2 2 ’Œ’Ü c 2 { p(x=v c)={{^{2}}}},e^{-{)^{2}}{2 {c}^{2}}}}} Another common technique for handling continuous values is to use binning to discretize the feature values, to obtain a new set of Bernoulli-distributed features; some literature in fact suggests that this is necessary to apply naive Bayes, but it is not, and the discretization may throw away discriminative information. === Multinomial naive Bayes === With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial ( p 1 , Š—ç’ , p n ) { (p {1}, ,p {n})} where p i { p {i}} is the probability that event i occurs (or K such multinomials in the multiclass case). A feature vector x = ( x 1 , Š—ç’ , x n ) { =(x {1}, ,x {n})} is then a histogram, with x i { x {i}} counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram x is given by p ( x Š—ç’ Î£ C k ) = ( Š—ç’ ’” i x i ) ! Š—ç’ Î i x i ! Š—ç’ Î i p k i x i { p( C {k})={x {i})!}{ {i}x {i}!}} {i}{p {ki}}^{x {i}}} The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space: log Š—çÎ’ p ( C k Š—ç’ Î£ x ) Š—ç’ Î log Š—çÎ’ ( p ( C k ) Š—ç’ Î i = 1 n p k i x i ) = log Š—çÎ’ p ( C k ) + Š—ç’ ’” i = 1 n x i Š—ç’“’ log Š—çÎ’ p k i = b + w k Š—ç x { )& (p(C {k}) {i=1}^{n}{p {ki}}^{x {i}})&= p(C {k})+ {i=1}^{n}x {i} p {ki}&=b+ {k}^{ } }} where b = log Š—çÎ’ p ( C k ) { b= p(C {k})} and w k i = log Š—çÎ’ p k i { w {ki}= p {ki}} . If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case. Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf-idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines. === Bernoulli naive Bayes === In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If x i { x {i}} is a boolean expressing the occurrence or absence of the ith term from the vocabulary, then the likelihood of a document given a class C k { C {k}} is given by p ( x Š—ç’ Î£ C k ) = Š—ç’ Î i = 1 n p k i x i ( 1 Š—ç’ ’« p k i ) ( 1 Š—ç’ ’« x i ) { p( C {k})= {i=1}^{n}p {ki}^{x {i}}(1-p {ki})^{(1-x {i})}} where p k i { p {ki}} is the probability of class C k { C {k}} generating the term w i { w {i}} . This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one. === Semi-supervised parameter estimation === Given a way to train a naive Bayes classifier from labeled data, its possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop: Given a collection D = L Š—ç U { D=L U} of labeled samples L and unlabeled samples U, start by training a naive Bayes classifier on L. Until convergence, do: Predict class probabilities P ( C Š—ç’ Î£ x ) { P(C x)} for all examples x in D { D} . Re-train the model based on the probabilities (not the labels) predicted in the previous step. Convergence is determined based on improvement to the model likelihood P ( D Š—ç’ Î£ ’â’ ) { P(D )} , where ’â’ { } denotes the parameters of the naive Bayes model. This training algorithm is an instance of the more general expectation-maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem. == Discussion == Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below. === Relation to logistic regression === In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with (multinomial) logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p ( C , x ) { p(C, )} , while logistic regression fits the same probability model to optimize the conditional p ( C Š—ç’ Î£ x ) { p(C )} . The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as predict class C 1 { C {1}} if the odds of p ( C 1 Š—ç’ Î£ x ) { p(C {1} )} exceed those of p ( C 2 Š—ç’ Î£ x ) { p(C {2} )} . Expressing this in log-space gives: log Š—çÎ’ p ( C 1 Š—ç’ Î£ x ) p ( C 2 Š—ç’ Î£ x ) = log Š—çÎ’ p ( C 1 Š—ç’ Î£ x ) Š—ç’ ’« log Š—çÎ’ p ( C 2 Š—ç’ Î£ x ) > 0 { )}{p(C {2} )}}= p(C {1} )- p(C {2} )>0} The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two discrete event models, it can be reparametrised as a linear function b + w Š—ç x > 0 { b+ ^{ }x>0} . Obtaining the probabilities is then a matter of applying the logistic function to b + w Š—ç x { b+ ^{ }x} , or in the multiclass case, the softmax function. Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster. == Examples == === Sex classification === Problem: classify whether a given person is a male or a female based on the measured features. The features include height, weight, and foot size. ==== Training ==== Example training set below. The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances): Lets say we have equiprobable classes so P(male)= P(female) = 0.5. This prior probability distribution might be based on our knowledge of frequencies in the larger population, or on frequency in the training set. ==== Testing ==== Below is a sample to be classified as male or female. We wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by posterior (male) = P ( male ) p ( height Š—ç’ Î£ male ) p ( weight Š—ç’ Î£ male ) p ( foot size Š—ç’ Î£ male ) e v i d e n c e {={),p({}),p({}),p({})}{evidence}}} For the classification as female the posterior is given by posterior (female) = P ( female ) p ( height Š—ç’ Î£ female ) p ( weight Š—ç’ Î£ female ) p ( foot size Š—ç’ Î£ female ) e v i d e n c e {={),p({}),p({}),p({})}{evidence}}} The evidence (also termed normalizing constant) may be calculated: evidence = P ( male ) p ( height Š—ç’ Î£ male ) p ( weight Š—ç’ Î£ male ) p ( foot size Š—ç’ Î£ male ) + P ( female ) p ( height Š—ç’ Î£ female ) p ( weight Š—ç’ Î£ female ) p ( foot size Š—ç’ Î£ female ) {=P({}),p({}),p({}),p({})+P({}),p({}),p({}),p({})}} However, given the sample, the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored. We now determine the probability distribution for the sex of the sample. P ( male ) = 0.5 { P({})=0.5} p ( height Š—ç’ Î£ male ) = 1 2 ’Œ’ ’Œ’Ü 2 exp Š—çÎ’ ( Š—ç’ ’« ( 6 Š—ç’ ’« ’â ) 2 2 ’Œ’Ü 2 ) Š—ç’ ’ 1.5789 { p({})={{}}} ({}{2 ^{2}}}) 1.5789} , where ’â = 5.855 { =5.855} and ’Œ’Ü 2 = 3.5033 Š—ç’“’ 10 Š—ç’ ’« 2 { ^{2}=3.5033 10^{-2}} are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here - it is a probability density rather than a probability, because height is a continuous variable. p ( weight Š—ç’ Î£ male ) = 5.9881 Š—ç’“’ 10 Š—ç’ ’« 6 { p({})=5.9881 10^{-6}} p ( foot size Š—ç’ Î£ male ) = 1.3112 Š—ç’“’ 10 Š—ç’ ’« 3 { p({})=1.3112 10^{-3}} posterior numerator (male) = their product = 6.1984 Š—ç’“’ 10 Š—ç’ ’« 9 {={}=6.1984 10^{-9}} P ( female ) = 0.5 { P({})=0.5} p ( height Š—ç’ Î£ female ) = 2.2346 Š—ç’“’ 10 Š—ç’ ’« 1 { p({})=2.2346 10^{-1}} p ( weight Š—ç’ Î£ female ) = 1.6789 Š—ç’“’ 10 Š—ç’ ’« 2 { p({})=1.6789 10^{-2}} p ( foot size Š—ç’ Î£ female ) = 2.8669 Š—ç’“’ 10 Š—ç’ ’« 1 { p({})=2.8669 10^{-1}} posterior numerator (female) = their product = 5.3778 Š—ç’“’ 10 Š—ç’ ’« 4 {={}=5.3778 10^{-4}} Since posterior numerator is greater in the female case, we predict the sample is female. === Document classification === Here is a worked example of naive Bayesian classification to the document classification problem. Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as p ( w i Š—ç’ Î£ C ) { p(w {i} C),} (For this treatment, we simplify things further by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.) Then the probability that a given document D contains all of the words w i { w {i}} , given a class C, is p ( D Š—ç’ Î£ C ) = Š—ç’ Î i p ( w i Š—ç’ Î£ C ) { p(D C)= {i}p(w {i} C),} The question that we desire to answer is: what is the probability that a given document D belongs to a given class C? In other words, what is p ( C Š—ç’ Î£ D ) { p(C D),} ? Now by definition p ( D Š—ç’ Î£ C ) = p ( D Š—ç’ Î© C ) p ( C ) { p(D C)={p(D C) p(C)}} and p ( C Š—ç’ Î£ D ) = p ( D Š—ç’ Î© C ) p ( D ) { p(C D)={p(D C) p(D)}} Bayes theorem manipulates these into a statement of probability in terms of likelihood. p ( C Š—ç’ Î£ D ) = p ( C ) p ( D Š—ç’ Î£ C ) p ( D ) { p(C D)={{p(D)}}} Assume for the moment that there are only two mutually exclusive classes, S and ’Ç’äS (e.g. spam and not spam), such that every element (email) is in either one or the other; p ( D Š—ç’ Î£ S ) = Š—ç’ Î i p ( w i Š—ç’ Î£ S ) { p(D S)= {i}p(w {i} S),} and p ( D Š—ç’ Î£ ’Ç’ä S ) = Š—ç’ Î i p ( w i Š—ç’ Î£ ’Ç’ä S ) { p(D S)= {i}p(w {i} S),} Using the Bayesian result above, we can write: p ( S Š—ç’ Î£ D ) = p ( S ) p ( D ) Š—ç’ Î i p ( w i Š—ç’ Î£ S ) { p(S D)={p(S) p(D)}, {i}p(w {i} S)} p ( ’Ç’ä S Š—ç’ Î£ D ) = p ( ’Ç’ä S ) p ( D ) Š—ç’ Î i p ( w i Š—ç’ Î£ ’Ç’ä S ) { p( S D)={p( S) p(D)}, {i}p(w {i} S)} Dividing one by the other gives: p ( S Š—ç’ Î£ D ) p ( ’Ç’ä S Š—ç’ Î£ D ) = p ( S ) Š—ç’ Î i p ( w i Š—ç’ Î£ S ) p ( ’Ç’ä S ) Š—ç’ Î i p ( w i Š—ç’ Î£ ’Ç’ä S ) {={p(S), {i}p(w {i} S) p( S), {i}p(w {i} S)}} Which can be re-factored as: p ( S Š—ç’ Î£ D ) p ( ’Ç’ä S Š—ç’ Î£ D ) = p ( S ) p ( ’Ç’ä S ) Š—ç’ Î i p ( w i Š—ç’ Î£ S ) p ( w i Š—ç’ Î£ ’Ç’ä S ) {={p(S) p( S)}, {i}{p(w {i} S) p(w {i} S)}} Thus, the probability ratio p(S | D) / p(’Ç’äS | D) can be expressed in terms of a series of likelihood ratios. The actual probability p(S | D) can be easily computed from log (p(S | D) / p(’Ç’äS | D)) based on the observation that p(S | D) + p(’Ç’äS | D) = 1. Taking the logarithm of all these ratios, we have: ln Š—çÎ’ p ( S Š—ç’ Î£ D ) p ( ’Ç’ä S Š—ç’ Î£ D ) = ln Š—çÎ’ p ( S ) p ( ’Ç’ä S ) + Š—ç’ ’” i ln Š—çÎ’ p ( w i Š—ç’ Î£ S ) p ( w i Š—ç’ Î£ ’Ç’ä S ) { =+ {i} S) p(w {i} S)}} (This technique of log-likelihood ratios is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.) Finally, the document can be classified as follows. It is spam if p ( S Š—ç’ Î£ D ) > p ( ’Ç’ä S Š—ç’ Î£ D ) { p(S D)>p( S D)} (i. e., ln Š—çÎ’ p ( S Š—ç’ Î£ D ) p ( ’Ç’ä S Š—ç’ Î£ D ) > 0 { >0} ), otherwise it is not spam. == See also == AODE Bayesian spam filtering Bayesian network Random naive Bayes Linear classifier Logistic regression Perceptron Take-the-best heuristic == References == === Further reading === Domingos, Pedro; Pazzani, Michael (1997). On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning. 29: 103-137. Webb, G. I.; Boughton, J.; Wang, Z. (2005). Not So Naive Bayes: Aggregating One-Dependence Estimators. Machine Learning. Springer. 58 (1): 5-24. doi:10.1007/s10994-005-4258-6. Mozina, M.; Demsar, J.; Kattan, M.; Zupan, B. (2004). Nomograms for Visualization of Naive Bayesian Classifier (PDF). Proc. PKDD-2004. pp. 337-348. Maron, M. E. (1961). Automatic Indexing: An Experimental Inquiry. JACM. 8 (3): 404-417. doi:10.1145/321075.321084. Minsky, M. (1961). Steps toward Artificial Intelligence. Proc. IRE. 49. pp. 8-30. == External links == Book Chapter: Naive Bayes text classification, Introduction to Information Retrieval Naive Bayes for Text Classification with Unbalanced Classes Benchmark results of Naive Bayes implementations Hierarchical Naive Bayes Classifiers for uncertain data (an extension of the Naive Bayes classifier). Software Naive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout, Mallet, NLTK, Orange, scikit-learn and Weka. IMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier. An interactive Microsoft Excel spreadsheet Naive Bayes implementation using VBA (requires enabled macros) with viewable source code. jBNC - Bayesian Network Classifier Toolbox Statistical Pattern Recognition Toolbox for Matlab. ifile - the first freely available (Naive) Bayesian mail/spam filter NClassifier - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J. Classifier4J - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier."
Nearest centroid classifier,Nearest centroid classifier,,,"In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation. When applied to text classification using tf*idf vectors to represent documents, the nearest centroid classifier is known as the Rocchio classifier because of its similarity to the Rocchio algorithm for relevance feedback. An extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of tumors. == Algorithm == Training procedure: given labeled training samples { ( x Š—çÎ¾’« 1 , y 1 ) , Š—ç’ , ( x Š—çÎ¾’« n , y n ) } { } {1},y {1}), ,({} {n},y {n})}} with class labels y i Š—ç’ ’ Y { y {i} } , compute the per-class centroids ’â l Š—çÎ¾’« = 1 | C l | Š—ç’ ’” i Š—ç’ ’ C l x Š—çÎ¾’« i { }}={{|C {l}|}}{}{ }}{} {i}} where C l { C {l}} is the set of indices of samples belonging to class l Š—ç’ ’ Y { l } . Prediction function: the class assigned to an observation x Š—çÎ¾’« {}} is y ^ = arg Š—çÎ’ min l Š—ç’ ’ Y Š—ç’ Î‚ ’â Š—çÎ¾’« l Š—ç’ ’« x Š—çÎ¾’« Š—ç’ Î‚ {}={ } {l }|{} {l}-{}|} . == See also == Cluster hypothesis k-means clustering k-nearest neighbor algorithm Linear discriminant analysis == References =="
Phrase search,Phrase search,,,"Phrase Search is a type of search that allows users to search for documents containing an exact sentence or phrase rather than comparing a set of keywords in random order. More recently, phrase search is one of the more important techniques associated with optimizing the textual content of web pages in such a way that it is likely to be found by someone searching for a certain string of text. Phrase Search is also a common term in search engine terminology. The term refers to a specific search syntax which involves using quotation marks () around a specific phrase to indicate that you want to search for instances of that search query. == References =="
Phrase,Phrase,,,"In everyday speech, a phrase may be any group of words, often carrying a special idiomatic meaning; in this sense it is roughly synonymous with expression. In linguistic analysis, a phrase is a group of words (or possibly a single word) that functions as a constituent in the syntax of a sentence, a single unit within a grammatical hierarchy. A phrase typically appears within a clause, but it is possible also for a phrase to be a clause or to contain a clause within it. == Common and technical use == There is a difference between the common use of the term phrase and its technical use in linguistics. In common usage, a phrase is usually a group of words with some special idiomatic meaning or other significance, such as all rights reserved, economical with the truth, kick the bucket, and the like. It may be a euphemism, a saying or proverb, a fixed expression, a figure of speech, etc. In grammatical analysis, particularly in theories of syntax, a phrase is any group of words, or sometimes a single word, which plays a particular role within the grammatical structure of a sentence. It does not have to have any special meaning or significance, or even exist anywhere outside of the sentence being analyzed, but it must function there as a complete grammatical unit. For example, in the sentence Yesterday I saw an orange bird with a white neck, the words an orange bird with a white neck form what is called a noun phrase, or a determiner phrase in some theories, which functions as the object of the sentence. Theorists of syntax differ in exactly what they regard as a phrase; however, it is usually required to be a constituent of a sentence, in that it must include all the dependents of the units that it contains. This means that some expressions that may be called phrases in everyday language are not phrases in the technical sense. For example, in the sentence I cant put up with Alex, the words put up with (meaning tolerate) may be referred to in common language as a phrase (English expressions like this are frequently called phrasal verbs) but technically they do not form a complete phrase, since they do not include Alex, which is the complement of the preposition with. == Heads and dependents == In grammatical analysis, most phrases contain a key word that identifies the type and linguistic features of the phrase; this is known as the head-word, or the head. The syntactic category of the head is used to name the category of the phrase; for example, a phrase whose head is a noun is called a noun phrase. The remaining words in a phrase are called the dependents of the head. In the following phrases the head-word, or head, is bolded: too slowly Š—ç’ ’– Adverb phrase (AdvP); the head is an adverb very happy Š—ç’ ’– Adjective phrase (AP); the head is an adjective the massive dinosaur Š—ç’ ’– Noun phrase (NP); the head is a noun (but see below for the determiner phrase analysis) at lunch Š—ç’ ’– Preposition phrase (PP); the head is a preposition watch TV Š—ç’ ’– Verb phrase (VP); the head is a verb The above five examples are the most common of phrase types; but, by the logic of heads and dependents, others can be routinely produced. For instance, the subordinator phrase: before that happened Š—ç’ ’– Subordinator phrase (SP); the head is a subordinating conjunctionŠ—ç’ ’–it subordinates the independent clause By linguistic analysis this is a group of words that qualifies as a phrase, and the head-word gives its syntactic name, subordinator, to the grammatical category of the entire phrase. But this phrase, before that happened, is more commonly classified in other grammars, including traditional English grammars, as a subordinate clause (or dependent clause); and it is then labelled not as a phrase, but as a clause. Most theories of syntax view most phrases as having a head, but some non-headed phrases are acknowledged. A phrase lacking a head is known as exocentric, and phrases with heads are endocentric. === Functional categories === Some modern theories of syntax introduce certain functional categories in which the head of a phrase is some functional word or item, which may even be covert, that is, it may be a theoretical construct that need not appear explicitly in the sentence. For example, in some theories, a phrase such as the man is taken to have the determiner the as its head, rather than the noun man - it is then classed as a determiner phrase (DP), rather than a noun phrase (NP). When a noun is used in a sentence without an explicit determiner, a null (covert) determiner may be posited. For full discussion, see Determiner phrase. Another type is the inflectional phrase, where (for example) a finite verb phrase is taken to be the complement of a functional, possibly covert head (denoted INFL) which is supposed to encode the requirements for the verb to inflect - for agreement with its subject (which is the specifier of INFL), for tense and aspect, etc. If these factors are treated separately, then more specific categories may be considered: tense phrase (TP), where the verb phrase is the complement of an abstract tense element; aspect phrase; agreement phrase and so on. Further examples of such proposed categories include topic phrase and focus phrase, which are assumed to be headed by elements that encode the need for a constituent of the sentence to be marked as the topic or as the focus. See the Generative approaches section of the latter article for details. == Phrase trees == Many theories of syntax and grammar illustrate sentence structure using phrase trees, which provide schematics of how the words in a sentence are grouped and relate to each other. Trees show the words, phrases, and, at times, clauses that make up sentences. Any word combination that corresponds to a complete subtree can be seen as a phrase. There are two established and competing principles for constructing trees; they produce constituency and dependency trees and both are illustrated here using an example sentence. The constituency-based tree is on the left and the dependency-based tree is on the right: The tree on the left is of the constituency-based, phrase structure grammar, and the tree on the right is of the dependency grammar. The node labels in the two trees mark the syntactic category of the different constituents, or word elements, of the sentence. In the constituency tree each phrase is marked by a phrasal node (NP, PP, VP); and there are eight phrases identified by phrase structure analysis in the example sentence. On the other hand, the dependency tree identifies a phrase by any node that exerts dependency upon, or dominates, another node. And, using dependency analysis, there are six phrases in the sentence. The trees and phrase-counts demonstrate that different theories of syntax differ in the word combinations they qualify as a phrase. Here the constituency tree identifies three phrases that the dependency tree does not, namely: house at the end of the street, end of the street, and the end. More analysis, including about the plausibilities of both grammars, can be made empirically by applying constituency tests. == Confusion: phrases in theories of syntax == The common use of the term phrase is different from that employed by some phrase structure theories of syntax. The everyday understanding of the phrase is that it consists of two or more words, whereas depending on the theory of syntax that one employs, individual words may or may not qualify as phrases. The trees in the previous section, for instance, do not view individual words as phrases. Theories of syntax that employ X-bar theory, in contrast, will acknowledge many individual words as phrases. This practice is because sentence structure is analysed in terms of a universal schema, the X-bar schema, which sees each head as projecting at least three levels of structure: a minimal level, an intermediate level, and a maximal level. Thus an individual noun (N), such as Susan in Susan laughed, will project up to an intermediate level (N) and a maximal level (NP, noun phrase), which means that Susan qualifies as a phrase. (The subject slot in the sentence is required to be filled by an NP, so regardless of whether the subject is a multi-word unit like the tall woman, or a single word performing the same function, like Susan, it is called an NP in these theories.) This concept of the phrase is a source of confusion for students of syntax. Many other theories of syntax do not employ the X-bar schema and are therefore less likely to encounter this confusion. For instance, dependency grammars do not acknowledge phrase structure in the manner associated with phrase structure grammars and therefore do not acknowledge individual words as phrases, a fact that is evident in the dependency grammar trees above and below. == The verb phrase (VP) as a source of controversy == Most if not all theories of syntax acknowledge verb phrases (VPs), but they can diverge greatly in the types of verb phrases that they posit. Phrase structure grammars acknowledge both finite verb phrases and non-finite verb phrases as constituents. Dependency grammars, in contrast, acknowledge just non-finite verb phrases as constituents. The distinction is illustrated with the following examples: The Republicans may nominate Newt. - Finite VP in bold The Republicans may nominate Newt. - Non-finite VP in bold The syntax trees of this sentence are next: The constituency tree on the left shows the finite verb string may nominate Newt as a phrase (= constituent); it corresponds to VP1. In contrast, this same string is not shown as a phrase in the dependency tree on the right. Observe that both trees, however, take the non-finite VP string nominate Newt to be a phrase, since in both trees nominate Newt corresponds to a complete subtree. Since there is disagreement concerning the status of finite VPs (whether they are constituents or not), empirical considerations are needed. Grammarians can (again) employ constituency tests to shed light on the controversy. Constituency tests are diagnostics for identifying the constituents of sentences and they are thus essential for identifying phrases. The results of most constituency tests do not support the existence of a finite VP constituent. == See also == == Notes == == References == == External links == The Phrase Finder - The meanings and origins of phrases, sayings, and idioms Phrases.net - A large collection of common phrases that can be heard and translated to several languages. Phras.in - An online tool that helps choosing the correct phrasing, based on web results frequency. phraseup* - A writing assistant that helps with completing sentences by finding the missing words we cant recall. Fraze.it - A search engine for sentences and phrases. Supports six languages, filtered by form, zone, context, etc."
Probabilistic relevance model,Probabilistic relevance model,,,"The probabilistic relevance model was devised by Robertson and Jones as a framework for probabilistic models to come. It is a formalism of information retrieval useful to derive ranking functions used by search engines and web search engines in order to rank matching documents according to their relevance to a given search query. It makes an estimation of the probability of finding if a document dj is relevant to a query q. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query q. Such an ideal answer set is called R and should maximize the overall probability of relevance to that user. The prediction is that documents in this set R are relevant to the query, while documents not present in the set are non-relevant. s i m ( d j , q ) = P ( R | d Š—çÎ¾’« j ) P ( R ’Ç’Ù | d Š—çÎ¾’« j ) { sim(d {j},q)={} {j})}{P({}|{} {j})}}} == Related models == There are some limitations to this framework that need to be addressed by further development: There is no accurate estimate for the first run probabilities Index terms are not weighted Terms are assumed mutually independent To address these and other concerns there are some developed models from the probabilistic relevance framework. The Binary Independence Model for one, as it is from the same author. The most known derivative of this framework is the Okapi(BM25) weighting scheme and its BM25F brother. == References =="
Okapi BM25,Okapi BM25,,,"In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Sp’‘ rck Jones, and others. The name of the actual ranking function is BM25. To set the right context, however, it usually referred to as Okapi BM25, since the Okapi information retrieval system, implemented at Londons City University in the 1980s and 1990s, was the first system to implement this function. BM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art TF-IDF-like retrieval functions used in document retrieval. == The ranking function == BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows. Given a query Q, containing keywords q 1 , . . . , q n { q {1},...,q {n}} , the BM25 score of a document D is: score ( D , Q ) = Š—ç’ ’” i = 1 n IDF ( q i ) Š—ç’“’ f ( q i , D ) Š—ç’“’ ( k 1 + 1 ) f ( q i , D ) + k 1 Š—ç’“’ ( 1 Š—ç’ ’« b + b Š—ç’“’ | D | avgdl ) , {(D,Q)= {i=1}^{n}{}(q {i}),D) (k {1}+1)}{f(q {i},D)+k {1} (1-b+b{}})}},} where f ( q i , D ) { f(q {i},D)} is q i { q {i}} s term frequency in the document D, | D | { |D|} is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. k 1 { k {1}} and b are free parameters, usually chosen, in absence of an advanced optimization, as k 1 Š—ç’ ’ [ 1.2 , 2.0 ] { k {1} [1.2,2.0]} and b = 0.75 { b=0.75} . IDF ( q i ) {(q {i})} is the IDF (inverse document frequency) weight of the query term q i { q {i}} . It is usually computed as: IDF ( q i ) = log Š—çÎ’ N Š—ç’ ’« n ( q i ) + 0.5 n ( q i ) + 0.5 , {(q {i})=)+0.5}{n(q {i})+0.5}},} where N is the total number of documents in the collection, and n ( q i ) { n(q {i})} is the number of documents containing q i { q {i}} . There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the Binary Independence Model. Please note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score. This means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way: Each summand can be given a floor of 0, to trim out common terms; The IDF function can be given a floor of a constant ’ŒÎµ { } , to avoid common terms being ignored at all; The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all. == IDF information theoretic interpretation == Here is an interpretation from information theory. Suppose a query term q { q} appears in n ( q ) { n(q)} documents. Then a randomly picked document D { D} will contain the term with probability n ( q ) N {{N}}} (where N { N} is again the cardinality of the set of documents in the collection). Therefore, the information content of the message D { D} contains q { q} is: Š—ç’ ’« log Š—çÎ’ n ( q ) N = log Š—çÎ’ N n ( q ) . { -{N}}={n(q)}}.} Now suppose we have two query terms q 1 { q {1}} and q 2 { q {2}} . If the two terms occur in documents entirely independently of each other, then the probability of seeing both q 1 { q {1}} and q 2 { q {2}} in a randomly picked document D { D} is: n ( q 1 ) N Š—ç’“’ n ( q 2 ) N , {)}{N}})}{N}},} and the information content of such an event is: Š—ç’ ’” i = 1 2 log Š—çÎ’ N n ( q i ) . { {i=1}^{2}{n(q {i})}}.} With a small variation, this is exactly what is expressed by the IDF component of BM25. == Modifications == At the extreme values of the coefficient b BM25 turns into ranking functions known as BM11 (for b = 1 { b=1} ) and BM15 (for b = 0 { b=0} ). BM25F is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance, term relevance saturation and length normalization. BM25+ is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter ’âÎâ { } (a default value is 1.0 in absence of a training data) as compared with BM25: score ( D , Q ) = Š—ç’ ’” i = 1 n IDF ( q i ) Š—ç’“’ [ f ( q i , D ) Š—ç’“’ ( k 1 + 1 ) f ( q i , D ) + k 1 Š—ç’“’ ( 1 Š—ç’ ’« b + b Š—ç’“’ | D | avgdl ) + ’âÎâ ] {(D,Q)= {i=1}^{n}{}(q {i}) [{,D) (k {1}+1)}{f(q {i},D)+k {1} (1-b+b{}})}}+ ]} == Footnotes == == References == TFxIDF Repository A definitive guide to the TFxIDF variants, including BMxx variants, and the evolution of BM25. Stephen E. Robertson; Steve Walker; Susan Jones; Micheline Hancock-Beaulieu & Mike Gatford (November 1994). Okapi at TREC-3. Proceedings of the Third Text REtrieval Conference (TREC 1994). Gaithersburg, USA. Stephen E. Robertson; Steve Walker & Micheline Hancock-Beaulieu (November 1998). Okapi at TREC-7 (PDF). Proceedings of the Seventh Text REtrieval Conference. Gaithersburg, USA. Sp’‘ rck Jones, K.; Walker, S.; Robertson, S. E. (2000). A probabilistic model of information retrieval: Development and comparative experiments: Part 1. Information Processing & Management. 36 (6): 779-808. doi:10.1016/S0306-4573(00)00015-7. Sp’‘ rck Jones, K.; Walker, S.; Robertson, S. E. (2000). A probabilistic model of information retrieval: Development and comparative experiments: Part 2. Information Processing & Management. 36 (6): 809-840. doi:10.1016/S0306-4573(00)00016-9. Stephen Robertson & Hugo Zaragoza (2009). The Probabilistic Relevance Framework: BM25 and Beyond. 3 (4). Found. Trends Inf. Retr.: 333-389. doi:10.1561/1500000019. == External links == Robertson, Stephen; Zaragoza, Hugo (2009). The Probabilistic Relevance Framework: BM25 and Beyond (PDF). NOW Publishers, Inc. ISBN 978-1-60198-308-4."
Query understanding,Query understanding,,,"Query understanding is the process of inferring the intent of a search engine user by extracting semantic meaning from the searcherŠ—ç’ Îés keywords. Query understanding methods generally take place before the search engine retrieves and ranks results. It is related to natural language processing but specifically focused on the understanding of search queries. Query understanding is at the heart of technologies like Amazon Alexa, Apples Siri. Google Assistant, IBMs Watson, and Microsofts Cortana. == Methods == === Tokenization === Tokenization is the process of breaking up a text string into words or other meaningful elements called tokens. Typically, tokenization occurs at the word level. However, it is sometimes difficult to define what is meant by a word. Often a tokenizer relies on simple heuristics, such as splitting the string on punctuation and whitespace characters. Tokenization is more challenging in languages without spaces between words, such as Chinese and Japanese. Tokenizing text in these languages requires the use of word segmentation algorithms. === Spelling correction === Spelling correction is the process of automatically detecting and correcting spelling errors in search queries. Most spelling correction algorithms are based on a language model, which determines the a priori probability of an intended query, and an error model (typically a noisy channel model), which determines the probability of a particular misspelling, given an intended query. === Stemming and lemmatization === Many, but not all, language inflect words to reflect their role in the utterance they appear in: a word such as *care* may appear as, besides the base form. as *cares*, *cared*, *caring*, and others. The variation between various forms of a word is likely to be of little importance for the relatively coarse-grained model of meaning involved in a retrieval system, and for this reason the task of conflating the various forms of a word is a potentially useful technique to increase recall of a retrieval system. The languages of the world vary in how much morphological variation they exhibit, and for some languages there are simple methods to reduce a word in query to its lemma or root form or its stem. For some other languages, this operation involves non-trivial string processing. A noun in English typically appears in four variants: *cat* *cats* *cats* *cats* or *child* *child’ÇÎâs* *children* *childrens*. Other languages have more variation. Finnish, e.g., potentially exhibits about 5000 forms for a noun, and for many languages the inflectional forms are not limited to affixes but change the core of the word itself. Stemming algorithms, also known as stemmers, typically use a collection of simple rules to remove suffixes intended to model the languageŠ—ç’ Îés inflection rules. More advanced methods, lemmatisation methods, group together the inflected forms of a word through more complex rule sets based on a wordŠ—ç’ Îés part of speech or its record in a lexical database, transforming an inflected word through lookup or a series of transformations to its lemma. For a long time, it was taken to be proven that morphological normalisation by and large did not help retrieval performance. Once the attention of the information retrieval field moved to languages other than English, it was found that for some languages there were obvious gains to be found. === Entity recognition === Entity recognition is the process of locating and classifying entities within a text string. Named-entity recognition specifically focuses on named entities, such as names of people, places, and organizations. In addition, entity recognition includes identifying concepts in queries that may be represented by multi-word phrases. Entity recognition systems typically use grammar-based linguistic techniques or statistical machine learning models. === Query rewriting === Query rewriting is the process of automatically reformulating a search query to more accurately capture its intent. Query expansion adds additional query terms, such as synonyms, in order to retrieve more documents and thereby increase recall. Query relaxation removes query terms to reduce the requirements for a document to match the query, thereby also increasing recall. Other forms of query rewriting, such as automatically converting consecutive query terms into phrases and restricting query terms to specific fields, aim to increase precision. Apache Lucene search engine uses query rewrite to transform complex queries to more primitive queries, such as expressions with wildcards (e.g. quer*) into a boolean query of the matching terms from the index (such as query OR queries). == See also == Daniel Tunkelangs blog on Query Understanding ACM SIGIR 2010 Workshop Report on Query Representation and Understanding ACM WSDM 2016 Workshop on Query Understanding for Search on All Devices == References =="
Web query classification,Web query classification,,,"A Web query topic classification/categorization is a problem in information science. The task is to assign a Web search query to one or more predefined categories, based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query Š—ç’ ’appleŠ—ç’ Î might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the document classification tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks. == KDDCUP 2005 == KDDCUP 2005 competition highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query Š—ç’ ’appleŠ—ç’ Î, it should be classified into ranked categories: Š—ç’ ’Computers Hardware; Living Food & CookingŠ—ç’ Î. == Difficulties == Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding: === How to derive an appropriate feature representation for Web queries? === Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, apple can mean a kind of fruit or a computer company. Java can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a vector space model for classification is not appropriate. Query-enrichment based methods start by enriching user queries to a collection of text documents through search engines. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as Naive Bayes (NB) and Support Vector Machines (SVMs). How about disadvantages and advantages?? give the answers: === How to adapt the changes of the queries and categories over time? === The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word Barcelona has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web. Intermediate taxonomy based method first builds a bridging classifier on an intermediate taxonomy, such as Open Directory Project (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries. === How to use the unlabeled query logs to help with query classification? === Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users knowledge about the World Wide Web. Query clustering method tries to associate related queries by clustering Š—ç’ ’session dataŠ—ç’ Î, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering. Selectional preference based method tries to exploit some association rules between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries. == Applications == Metasearch engines send a users query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users navigation. Vertical search, compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly. Online advertising aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs. All these services rely on the understanding Web users search intents through their Web queries. == See also == Document classification Web search query Information retrieval Query expansion Naive Bayes classifier Support vector machines Meta search Vertical search Online advertising == References == == Further reading == Shen. Learning-based Web Query Understanding. Phd Thesis, HKUST, June 2007."
Relevance feedback,Relevance feedback,,,"Relevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query. We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or pseudo feedback. == Explicit feedback == Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments. Users may indicate relevance explicitly using a binary or graded relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as not relevant, somewhat relevant, relevant, or very relevant). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance. An example of this would be the SearchWiki feature implemented by Google on their search website. The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known Rocchio Algorithm. A performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is NDCG. Other measures include precision at k and mean average precision. == Implicit feedback == Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [1]. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response The key differences of implicit relevance feedback from that of explicit include [2]: the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results. Another example of this is the Surf Canyon browser extension, which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result. == Blind feedback == Pseudo relevance feedback, also known as blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top k ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is: Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments). Select top 20-30 (indicative number) terms from these documents using for instance tf-idf weights. Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents. Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments. This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents. Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required. == Using relevance information == Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query. Relevance feedback is often implemented using the Rocchio Algorithm. == Further reading == Relevance feedback lecture notes - Jimmy Lins lecture notes, adapted from Doug Oards [3] - chapter from Modern Information Retrieval Stefan B’‘ ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010. == References =="
Search data structure,Search data structure,,,"In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database. The simplest, most general, and least efficient search structure is merely an unordered sequential list of all the items. Locating the desired item in such a list, by the linear search method, inevitably requires a number of operations proportional to the number n of items, in the worst case as well as in the average case. Useful search data structures allow faster retrieval; however, they are limited to queries of some specific kind. Moreover, since the cost of building such structures is at least proportional to n, they only pay off if several queries are to be performed on the same database (or on a database that changes little between queries). Static search structures are designed for answering many queries on a fixed database; dynamic structures also allow insertion, deletion, or modification of items between successive queries. In the dynamic case, one must also consider the cost of fixing the search structure to account for the changes in the database. == Classification == The simplest kind of query is to locate a record that has a specific field (the key) equal to a specified value v. Other common kinds of query are find the item with smallest (or largest) key value, find the item with largest key value not exceeding v, find all items with key values between specified bounds vmin and vmax. In certain databases the key values may be points in some multi-dimensional space. For example, the key may be a geographic position (latitude and longitude) on the Earth. In that case, common kinds of queries are find the record with a key closest to a given point v, or find all items whose key lies at a given distance from v, or find all items within a specified region R of the space. A common special case of the latter are simultaneous range queries on two or more simple keys, such as find all employee records with salary between 50,000 and 100,000 and hired between 1995 and 2007. === Single ordered keys === Array if the key values span a moderately compact interval. Priority-sorted list; see linear search Key-sorted array; see binary search Self-balancing binary search tree Hash table === Finding the smallest element === Heap ==== Asymptotic amortized worst-case analysis ==== In this table, the asymptotic notation O(f(n)) means not exceeding some fixed multiple of f(n) in the worst case. Note: Insert on an unsorted array is sometimes quoted as being O(n) due to the assumption that the element to be inserted must be inserted at one particular location of the array, which would require shifting all the subsequent elements by one position. However, in a classic array, the array is used to store arbitrary unsorted elements, and hence the exact position of any given element is of no consequence, and insert is carried out by increasing the array size by 1 and storing the element at the end of the array, which is a O(1) operation. Likewise, the deletion operation is sometimes quoted as being O(n) due to the assumption that subsequent elements must be shifted, but in a classic unsorted array the order is unimportant (though elements are implicitly ordered by insert-time), so deletion can be carried out by swapping the element to be deleted with the last element in the array and then decrementing the array size by 1, which is a O(1) operation. This table is only an approximate summary; for each data structure there are special situations and variants that may lead to different costs. Also two or more data structures can be combined to obtain lower costs. == Footnotes == == See also == List of data structures Skip list"
Persistent data structure,Persistent data structure,,,"In computing, a persistent data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. A data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called ephemeral. These types of data structures are particularly common in logical and functional programming, and in a purely functional program all data is immutable, so all data structures are automatically fully persistent. Persistent data structures can also be created using in-place updating of data and these may, in general, use less time or storage space than their purely functional counterparts. Purely functional data structures are persistent data structures that completely avoid the use of mutable state, but can often still achieve attractive amortized time complexity bounds. While persistence can be achieved by simple copying, this is inefficient in CPU and RAM usage, because most operations make only small changes to a data structure. A better method is to exploit the similarity between the new and old versions to share structure between them, such as using the same subtree in a number of tree structures. However, because it rapidly becomes infeasible to determine how many previous versions share which parts of the structure, and because it is often desirable to discard old versions, this necessitates an environment with garbage collection. However, it is not so infeasible that a sophisticated project, such as the ZFS copy-on-write file system, is unable to achieve this by tracking storage allocation directly. == Partially persistent == In the partial persistence model, we may query any previous version of the data structure, but we may only update the latest version. This implies a linear ordering among the versions. Three methods on balanced binary search tree: === Fat node === Fat node method is to record all changes made to node fields in the nodes themselves, without erasing old values of the fields. This requires that we allow nodes to become arbitrarily Š—ç’ ’fatŠ—ç’ Î. In other words, each fat node contains the same information and pointer fields as an ephemeral node, along with space for an arbitrary number of extra field values. Each extra field value has an associated field name and a version stamp which indicates the version in which the named field was changed to have the specified value. Besides, each fat node has its own version stamp, indicating the version in which the node was created. The only purpose of nodes having version stamps is to make sure that each node only contains one value per field name per version. In order to navigate through the structure, each original field value in a node has a version stamp of zero. ==== Complexity of fat node ==== With using fat node method, it requires O(1) space for every modification: just store the new data. Each modification takes O(1) additional time to store the modification at the end of the modification history. This is an amortized time bound, assuming we store the modification history in a growable array. For access time, we must find the right version at each node as we traverse the structure. If we made m modifications, then each access operation has O(log m) slowdown resulting from the cost of finding the nearest modification in the array. === Path copying === Path copy is to make a copy of all nodes on the path which contains the node we are about to insert or delete. Then we must cascade the change back through the data structure: all nodes that pointed to the old node must be modified to point to the new node instead. These modifications cause more cascading changes, and so on, until we reach to the root. We maintain an array of roots indexed by timestamp. The data structure pointed to by time tŠ—ç’ Îés root is exactly time tŠ—ç’ Îés date structure. ==== Complexity of path copying ==== With m modifications, this costs O(log m) additive lookup time. Modification time and space are bounded by the size of the structure, since a single modification may cause the entire structure to be copied. That is O(m) for one update, and thus O(n’Ç ) preprocessing time. === A combination === Sleator, Tarjan et al. came up with a way to combine the advantages of fat nodes and path copying, getting O(1) access slowdown and O(1) modification space and time. In each node, we store one modification box. This box can hold one modification to the nodeŠ—ç’ ’–either a modification to one of the pointers, or to the nodeŠ—ç’ Îés key, or to some other piece of node-specific dataŠ—ç’ ’–and a timestamp for when that modification was applied. Initially, every nodeŠ—ç’ Îés modification box is empty. Whenever we access a node, we check the modification box, and compare its timestamp against the access time. (The access time specifies the version of the data structure that we care about.) If the modification box is empty, or the access time is before the modification time, then we ignore the modification box and just deal with the normal part of the node. On the other hand, if the access time is after the modification time, then we use the value in the modification box, overriding that value in the node. (Say the modification box has a new left pointer. Then weŠ—ç’ Îéll use it instead of the normal left pointer, but weŠ—ç’ Îéll still use the normal right pointer.) Modifying a node works like this. (We assume that each modification touches one pointer or similar field.) If the nodeŠ—ç’ Îés modification box is empty, then we fill it with the modification. Otherwise, the modification box is full. We make a copy of the node, but using only the latest values.(That is, we overwrite one of the nodeŠ—ç’ Îés fields with the value that was stored in the modification box.) Then we perform the modification directly on the new node, without using the modification box. (We overwrite one of the new nodeŠ—ç’ Îés fields, and its modification box stays empty.) Finally, we cascade this change to the nodeŠ—ç’ Îés parent, just like path copying. (This may involve filling the parentŠ—ç’ Îés modification box, or making a copy of the parent recursively. If the node has no parentŠ—ç’ ’–itŠ—ç’ Îés the rootŠ—ç’ ’–we add the new root to a sorted array of roots.) With this algorithm, given any time t, at most one modification box exists in the data structure with time t. Thus, a modification at time t splits the tree into three parts: one part contains the data from before time t, one part contains the data from after time t, and one part was unaffected by the modification. ==== Complexity of the combination ==== Time and space for modifications require amortized analysis. A modification takes O(1) amortized space, and O(1) amortized time. To see why, use a potential function ’ŒÎÇ, where ’ŒÎÇ(T) is the number of full live nodes in T . The live nodes of T are just the nodes that are reachable from the current root at the current time (that is, after the last modification). The full live nodes are the live nodes whose modification boxes are full. Each modification involves some number of copies, say k, followed by 1 change to a modification box. (Well, not quiteŠ—ç’ ’–you could add a new rootŠ—ç’ ’–but that doesnŠ—ç’ Îét change the argument.) Consider each of the k copies. Each costs O(1) space and time, but decreases the potential function by one. (First, the node we copy must be full and live, so it contributes to the potential function. The potential function will only drop, however, if the old node isnŠ—ç’ Îét reachable in the new tree. But we know it isnŠ—ç’ Îét reachable in the new treeŠ—ç’ ’–the next step in the algorithm will be to modify the nodeŠ—ç’ Îés parent to point at the copy. Finally, we know the copyŠ—ç’ Îés modification box is empty. Thus, weŠ—ç’ Îéve replaced a full live node with an empty live node, and ’ŒÎÇ goes down by one.) The final step fills a modification box, which costs O(1) time and increases ’ŒÎÇ by one. Putting it all together, the change in ’ŒÎÇ is ’â’–’ŒÎÇ =1Š—ç’ ’« k.Thus, weŠ—ç’ Îéve paid O(k +’â’–’ŒÎÇ)= O(1) space and O(k +’â’–’ŒÎÇ +1) = O(1) time == Fully persistent == In fully persistent model, both updates and queries are allowed on any version of the data structure. == Confluently persistent == In confluently persistent model, we use combinators to combine input of more than one previous version to output a new single version. Rather than a branching tree, combinations of versions induce a DAG (directed acyclic graph) structure on the version graph. == Examples of persistent data structures == Perhaps the simplest persistent data structure is the singly linked list or cons-based list, a simple list of objects formed by each carrying a reference to the next in the list. This is persistent because we can take a tail of the list, meaning the last k items for some k, and add new nodes on to the front of it. The tail will not be duplicated, instead becoming shared between both the old list and the new list. So long as the contents of the tail are immutable, this sharing will be invisible to the program. Many common reference-based data structures, such as red-black trees, stacks, and treaps, can easily be adapted to create a persistent version. Some others need slightly more effort, for example: queues, deques, and extensions including min-deques (which have an additional O(1) operation min returning the minimal element) and random access deques (which have an additional operation of random access with sub-linear, most often logarithmic, complexity). There also exist persistent data structures which use destructive operations, making them impossible to implement efficiently in purely functional languages (like Haskell outside specialized monads like state or IO), but possible in languages like C or Java. These types of data structures can often be avoided with a different design. One primary advantage to using purely persistent data structures is that they often behave better in multi-threaded environments. === Linked lists === This example is taken from Okasaki. See the bibliography. Singly linked lists are the bread-and-butter data structure in functional languages. In ML-derived languages and Haskell, they are purely functional because once a node in the list has been allocated, it cannot be modified, only copied or destroyed. Note that ML itself is not purely functional. Consider the two lists: xs = [0, 1, 2] ys = [3, 4, 5] These would be represented in memory by: where a circle indicates a node in the list (the arrow out representing the second element of the node which is a pointer to another node). Now concatenating the two lists: zs = xs ++ ys results in the following memory structure: Notice that the nodes in list xs have been copied, but the nodes in ys are shared. As a result, the original lists (xs and ys) persist and have not been modified. The reason for the copy is that the last node in xs (the node containing the original value 2) cannot be modified to point to the start of ys, because that would change the value of xs. === Trees === This example is taken from Okasaki. See the bibliography. Consider a binary tree used for fast searching, where every node has the recursive invariant that subnodes on the left are less than the node, and subnodes on the right are greater than the node. For instance, the set of data xs = [a, b, c, d, f, g, h] might be represented by the following binary search tree: A function which inserts data into the binary tree and maintains the invariant is: After executing ys = insert (e, xs) we end up with the following: Notice two points: Firstly the original tree (xs) persists. Secondly many common nodes are shared between the old tree and the new tree. Such persistence and sharing is difficult to manage without some form of garbage collection (GC) to automatically free up nodes which have no live references, and this is why GC is a feature commonly found in functional programming languages. == Reference cycles == Since every value in a purely functional computation is built up out of existing values, it would seem that it is impossible to create a cycle of references. In that case, the reference graph (the graph of the references from object to object) could only be a directed acyclic graph. However, in most functional languages, functions can be defined recursively; this capability allows recursive structures using functional suspensions. In lazy languages, such as Haskell, all data structures are represented as implicitly suspended thunks; in these languages any data structure can be recursive because a value can be defined in terms of itself. Some other languages, such as OCaml, allow the explicit definition of recursive values. == See also == Persistent data Navigational database Copy-on-write Retroactive data structures == References == == Further reading == == External links == Lightweight Java implementation of Persistent Red-Black Trees Efficient persistent structures in C#"
Skip list,Skip list,,,"In computer science, a skip list is a data structure that allows fast search within an ordered sequence of elements. Fast search is made possible by maintaining a linked hierarchy of subsequences, with each successive subsequence skipping over fewer elements than the previous one. Searching starts in the sparsest subsequence until two consecutive elements have been found, one smaller and one larger than or equal to the element searched for. Via the linked hierarchy, these two elements link to elements of the next sparsest subsequence, where searching is continued until finally we are searching in the full sequence. The elements that are skipped over may be chosen probabilistically or deterministically, with the former being more common. == Description == A skip list is built in layers. The bottom layer is an ordinary ordered linked list. Each higher layer acts as an express lane for the lists below, where an element in layer i appears in layer i+1 with some fixed probability p (two commonly used values for p are 1/2 or 1/4). On average, each element appears in 1/(1-p) lists, and the tallest element (usually a special head element at the front of the skip list) in all the lists. The skip list contains log 1 / p Š—çÎ’ n { {1/p}n,} lists. A search for a target element begins at the head element in the top list, and proceeds horizontally until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most 1/p, which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is ( log 1 / p Š—çÎ’ n ) / p , { ( {1/p}n)/p,,} which is O ( log Š—çÎ’ n ) {}( n),} when p is a constant. By choosing different values of p, it is possible to trade search costs against storage costs. === Implementation details === The elements used for a skip list can contain more than one pointer since they can participate in more than one list. Insertions and deletions are implemented much like the corresponding linked-list operations, except that tall elements must be inserted into or deleted from more than one linked list. O ( n ) {}(n)} operations, which force us to visit every node in ascending order (such as printing the entire list), provide the opportunity to perform a behind-the-scenes derandomization of the level structure of the skip-list in an optimal way, bringing the skip list to O ( log Š—çÎ’ n ) {}( n)} search time. (Choose the level of the ith finite node to be 1 plus the number of times we can repeatedly divide i by 2 before it becomes odd. Also, i=0 for the negative infinity header as we have the usual special case of choosing the highest possible level for negative and/or positive infinite nodes.) However this also allows someone to know where all of the higher-than-level 1 nodes are and delete them. Alternatively, we could make the level structure quasi-random in the following way: make all nodes level 1 j Š—çÎ¾Î 1 while the number of nodes at level j > 1 do for each ith node at level j do if i is odd if i is not the last node at level j randomly choose whether to promote it to level j+1 else do not promote end if else if i is even and node i-1 was not promoted promote it to level j+1 end if repeat j Š—çÎ¾Î j + 1 repeat Like the derandomized version, quasi-randomization is only done when there is some other reason to be running an O ( n ) {}(n)} operation (which visits every node). The advantage of this quasi-randomness is that it doesnt give away nearly as much level-structure related information to an adversarial user as the de-randomized one. This is desirable because an adversarial user who is able to tell which nodes are not at the lowest level can pessimize performance by simply deleting higher-level nodes. (Bethea and Reiter however argue that nonetheless an adversary can use probabilistic and timing methods to force performance degradation.) The search performance is still guaranteed to be logarithmic. It would be tempting to make the following optimization: In the part which says Next, for each ith..., forget about doing a coin-flip for each even-odd pair. Just flip a coin once to decide whether to promote only the even ones or only the odd ones. Instead of O ( n log Š—çÎ’ n ) {}(n n)} coin flips, there would only be O ( log Š—çÎ’ n ) {}( n)} of them. Unfortunately, this gives the adversarial user a 50/50 chance of being correct upon guessing that all of the even numbered nodes (among the ones at level 1 or higher) are higher than level one. This is despite the property that he has a very low probability of guessing that a particular node is at level N for some integer N. A skip list does not provide the same absolute worst-case performance guarantees as more traditional balanced tree data structures, because it is always possible (though with very low probability) that the coin-flips used to build the skip list will produce a badly balanced structure. However, they work well in practice, and the randomized balancing scheme has been argued to be easier to implement than the deterministic balancing schemes used in balanced binary search trees. Skip lists are also useful in parallel computing, where insertions can be done in different parts of the skip list in parallel without any global rebalancing of the data structure. Such parallelism can be especially advantageous for resource discovery in an ad-hoc wireless network because a randomized skip list can be made robust to the loss of any single node. === Indexable skiplist === As described above, a skiplist is capable of fast O ( log Š—çÎ’ n ) {}( n)} insertion and removal of values from a sorted sequence, but it has only slow O ( n ) {}(n)} lookups of values at a given position in the sequence (i.e. return the 500th value); however, with a minor modification the speed of random access indexed lookups can be improved to O ( log Š—çÎ’ n ) {}( n)} . For every link, also store the width of the link. The width is defined as the number of bottom layer links being traversed by each of the higher layer express lane links. For example, here are the widths of the links in the example at the top of the page: 1 10 o---> o---------------------------------------------------------> o Top level 1 3 2 5 o---> o---------------> o---------> o---------------------------> o Level 3 1 2 1 2 3 2 o---> o---------> o---> o---------> o---------------> o---------> o Level 2 1 1 1 1 1 1 1 1 1 1 1 o---> o---> o---> o---> o---> o---> o---> o---> o---> o---> o---> o Bottom level Head 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th NIL Node Node Node Node Node Node Node Node Node Node Notice that the width of a higher level link is the sum of the component links below it (i.e. the width 10 link spans the links of widths 3, 2 and 5 immediately below it). Consequently, the sum of all widths is the same on every level (10 + 1 = 1 + 3 + 2 + 5 = 1 + 2 + 1 + 2 + 5). To index the skip list and find the ith value, traverse the skip list while counting down the widths of each traversed link. Descend a level whenever the upcoming width would be too large. For example, to find the node in the fifth position (Node 5), traverse a link of width 1 at the top level. Now four more steps are needed but the next width on this level is ten which is too large, so drop one level. Traverse one link of width 3. Since another step of width 2 would be too far, drop down to the bottom level. Now traverse the final link of width 1 to reach the target running total of 5 (1+3+1). function lookupByPositionIndex(i) node Š—çÎ¾Î head i Š—çÎ¾Î i + 1 # dont count the head as a step for level from top to bottom do while i Š—ç’ Î‚ node.width[level] do # if next step is not too far i Š—çÎ¾Î i - node.width[level] # subtract the current width node Š—çÎ¾Î node.next[level] # traverse forward at the current level repeat repeat return node.value end function This method of implementing indexing is detailed in Section 3.4 Linear List Operations in A skip list cookbook by William Pugh. == History == Skip lists were first described in 1989 by William Pugh. To quote the author: Skip lists are a probabilistic data structure that seem likely to supplant balanced trees as the implementation method of choice for many applications. Skip list algorithms have the same asymptotic expected time bounds as balanced trees and are simpler, faster and use less space. == Usages == List of applications and frameworks that use skip lists: MemSQL uses skip lists as its prime indexing structure for its database technology. Cyrus IMAP server offers a skiplist backend DB implementation (source file) Lucene uses skip lists to search delta-encoded posting lists in logarithmic time. QMap (up to Qt 4) template class of Qt that provides a dictionary. Redis, an ANSI-C open-source persistent key/value store for Posix systems, uses skip lists in its implementation of ordered sets. nessDB, a very fast key-value embedded Database Storage Engine (Using log-structured-merge (LSM) trees), uses skip lists for its memtable. skipdb is an open-source database format using ordered key/value pairs. ConcurrentSkipListSet and ConcurrentSkipListMap in the Java 1.6 API. Speed Tables are a fast key-value datastore for Tcl that use skiplists for indexes and lockless shared memory. leveldb, a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values Con Kolivas MuQSS Scheduler for the Linux kernel uses skip lists SkiMap uses skip lists as base data structure to build a more complex 3D Sparse Grid for Robot Mapping systems . Skip lists are used for efficient statistical computations of running medians (also known as moving medians). Skip lists are also used in distributed applications (where the nodes represent physical computers, and pointers represent network connections) and for implementing highly scalable concurrent priority queues with less lock contention, or even without locking, as well as lockless concurrent dictionaries. There are also several US patents for using skip lists to implement (lockless) priority queues and concurrent dictionaries. == See also == Bloom filter Skip graph == References == == External links == Skip list entry in the Dictionary of Algorithms and Data Structures Skip Lists: A Linked List with Self-Balancing BST-Like Properties on MSDN in C# 2.0 Why Skip Lists? Skip Lists lecture (MIT OpenCourseWare: Introduction to Algorithms) Open Data Structures - Chapter 4 - Skiplists Skip trees, an alternative data structure to skip lists in a concurrent approach Skip tree graphs, a distributed version of skip trees More on skip tree graphs, a distributed version of skip trees === Demo applets === Skip List Applet by Kubo Kovac Thomas Wengers demo applet on skiplists === Implementations === Algorithm::SkipList, implementation in Perl on CPAN Raymond Hettingers implementation in Python ConcurrentSkipListSet documentation for Java 6 (and sourcecode)"
List of Skip Beat! chapters,List of Skip Beat! chapters,,,"The chapters of the ongoing Japanese shÎÎjo manga series Skip Beat! are written and illustrated by Yoshiki Nakamura. It is the story of KyÎÎko Mogami, a 16-year-old girl who discovers her childhood friend, ShÎÎ Fuwa, who is an aspiring pop idol as well as the boy she loves, only keeps her around to act as a maid and earn money. Furious and heartbroken, she vows to get revenge by beating him in show business. In Japan, the manga was first published in Hakusenshas shÎÎjo manga anthology Hana to Yume in February 2002, while in the United States, it began publishing under Viz Medias Shojo Beat label in 2006. As of March 20, 2017, 40 volumes and one fanbook have been released in Japan, and as of September 6, 2016, 37 volumes have been released in the United States. Viz Media began re-issuing the Skip Beat! manga series in a 3-in-1 VIZBIG EDITION in March 2012, and as of January 3, 2017, 12 volumes have been released. == Volume list == == Chapters not yet published in tankoubon == Extra: Skip Beat! Special Side Story 243. Undead Monster 244. Undead Monster 245. Survivor Combat 246. Survivor Combat 247. Survivor Combat 248. Flying Shock 249. Flying Shock == References =="
Speech disorder,Speech disorder,,,"Speech disorders or speech impediments are a type of communication disorder where normal speech is disrupted. This can mean stuttering, lisps, etc. Someone who is unable to speak due to a speech disorder is considered mute. == Classification == Classifying speech into normal and disordered is more problematic than it first seems. By a strict classification, only 5% to 10% of the population has a completely normal manner of speaking (with respect to all parameters) and healthy voice; all others suffer from one disorder or another. There are three different levels of classification when determining the magnitude and type of a speech disorders and the proper treatment or therapy: Sounds the patient can produce Phonemic - can be produced easily; used meaningfully and constructively Phonetic - produced only upon request; not used consistently, meaningfully, or constructively; not used in connected speech Stimulate sounds Easily stimulated Stimulate after demonstration and probing (i.e. with a tongue depressor) Cannot produce the sound Cannot be produced voluntarily No production ever observed === Types of disorder === Apraxia of speech may result from stroke or progressive illness, and involves inconsistent production of speech sounds and rearranging of sounds in a word (potato may become topato and next totapo). Production of words becomes more difficult with effort, but common phrases may sometimes be spoken spontaneously without effort. Cluttering, a speech and fluency disorder characterized primarily by a rapid rate of speech, which makes speech difficult to understand. Developmental verbal dyspraxia also known as childhood apraxia of speech. Dysarthria is a weakness or paralysis of speech muscles caused by damage to the nerves or brain. Dysarthria is often caused by strokes, Parkinsons disease, ALS, head or neck injuries, surgical accident, or cerebral palsy. Dysprosody is the rarest neurological speech disorder. It is characterized by alterations in intensity, in the timing of utterance segments, and in rhythm, cadence, and intonation of words. The changes to the duration, the fundamental frequency, and the intensity of tonic and atonic syllables of the sentences spoken, deprive an individuals particular speech of its characteristics. The cause of dysprosody is usually associated with neurological pathologies such as brain vascular accidents, cranioencephalic traumatisms, and brain tumors. Muteness is complete inability to speak. Speech sound disorders involve difficulty in producing specific speech sounds (most often certain consonants, such as /s/ or /r/), and are subdivided into articulation disorders (also called phonetic disorders) and phonemic disorders. Articulation disorders are characterized by difficulty learning to produce sounds physically. Phonemic disorders are characterized by difficulty in learning the sound distinctions of a language, so that one sound may be used in place of many. However, it is not uncommon for a single person to have a mixed speech sound disorder with both phonemic and phonetic components. Stuttering affects approximately 1% of the adult population. Voice disorders are impairments, often physical, that involve the function of the larynx or vocal resonance. == Causes == In most cases the cause is unknown. However, there are various known causes of speech impediments, such as hearing loss, neurological disorders, brain injury, intellectual disability, drug abuse, physical impairments such as cleft lip and palate, and vocal abuse or misuse. == Treatment == Many of these types of disorders can be treated by speech therapy, but others require medical attention by a doctor in phoniatrics. Other treatments include correction of organic conditions and psychotherapy. In the United States, school-age children with a speech disorder are often placed in special education programs. Children who struggle to learn to talk often experience persistent communication difficulties in addition to academic struggles. More than 700,000 of the students served in the public schoolsŠ—ç’ Îé special education programs in the 2000-2001 school year were categorized as having a speech or language impediment. This estimate does not include children who have speech and language impairments secondary to other conditions such as deafness. Many school districts provide the students with speech therapy during school hours, although extended day and summer services may be appropriate under certain circumstances. Patients will be treated in teams, depending on the type of disorder they have. A team can include SLPs, specialists, family doctors, teachers,and family members. == Social effects == Suffering from a speech disorder can have negative social effects, especially among young children. Those with a speech disorder can be targets of bullying because of their disorder. The bullying can result in decreased self-esteem. == Language disorders == Language disorders are usually considered distinct from speech disorders, even though they are often used synonymously. Speech disorders refer to problems in producing the sounds of speech or with the quality of voice, where language disorders are usually an impairment of either understanding words or being able to use words and do not have to do with speech production. == See also == == References == == External links == Communication AND Speech Disorders Online voice and speech disorder community (VoiceMatters.net) Speech and Language Disorders Lisp Issues - A look at solutions for lispers Australian Speak Easy Association (Support Group For People Who Stutter)"
Communication disorder,Communication disorder,,,"A communication disorder is any disorder that affects an individuals ability to comprehend, detect, or apply language and speech to engage in discourse effectively with others. The delays and disorders can range from simple sound substitution to the inability to understand or use ones native language. == General definition == Disorders and tendencies included and excluded under the category of communication disorders may vary by source. For example, the definitions offered by the American Speech-Language-Hearing Association differ from that of the Diagnostic Statistical Manual 4th edition (DSM-IV). Gleanson (2001) defines a communication disorder as a speech and language disorder which refers to problems in communication and in related areas such as oral motor function. The delays and disorders can range from simple sound substitution to the inability to understand or use their native language. In general, communications disorders commonly refer to problems in speech (comprehension and/or expression) that significantly interfere with an individualŠ—ç’ Îés achievement and/or quality of life. Knowing the operational definition of the agency performing an assessment or giving a diagnosis may help. Persons who speak more than one language or are considered to have an accent in their location of residence do not have speech disorders if they are speaking in a manner consistent with their home environment or a blending of their home and foreign environment. == DSM-IV-TR == According to the DSM-IV-TR, communication disorders are usually first diagnosed in childhood or adolescence though they are not limited as childhood disorders and may persist into adulthood. They may also occur with other disorders. Diagnosis involves testing and evaluation during which it is determined if the scores/performance are substantially below developmental expectations and if they significantly interfere with academic achievement, social interactions and daily living. This assessment may also determine if the characteristic is deviant or delayed. Therefore, it may be possible for an individual to have communication challenges but not meet the criteria of being substantially below criteria of the DSM IV-TR. It should also be noted that the DSM diagnoses do not comprise a complete list of all communication disorders, for example, auditory processing disorder is not classified under the DSM or ICD-10. The following diagnoses are included in the communication disorders: Expressive language disorder - Characterized by difficulty expressing oneself beyond simple sentences and a limited vocabulary. An individual understands language better than their ability to use it; they may have a lot to say but have difficulties organizing and retrieving the words to get an idea across beyond what is expected for their developmental stage. Mixed receptive-expressive language disorder - problems comprehending the commands of others. Stuttering - a speech disorder characterized by a break in fluency, where sounds, syllables or words may be repeated or prolonged. Phonological disorder - a speech sound disorder characterized by problems in making patterns of sound errors, i.e. dat for that. Communication disorder NOS (not otherwise specified) - the DSM-IV diagnosis in which disorders that do not meet the specific criteria for the disorder listed above may be classified. == Changes in DSM-5 == The DSM-5 diagnoses for communication disorders completely rework the ones stated above. The diagnoses are made more general in order to capture the various aspects of communications disorders in a way that emphasizes their childhood onset and differentiate these communications disorders from those associated with other disorders (i.e. autism spectrum disorders) Language disorder - The important characteristics of language disorder are difficulties in learning and using language, which is caused by problems with vocabulary, with grammar, and with putting sentences together in a proper manner. Problems can both be receptive (understanding language) and expressive (producing language). Speech sound disorder - previously called phonological disorder, for those with problems with pronunciation and articulation of their native language. Childhood-Onset Fluency Disorder (Stuttering) - standard fluency and rhythm of speech is interrupted, often causing the repetition of whole words and syllables. May also include the prolongation of words and syllables; pauses within a word; and/or the avoidance of pronouncing difficult words and replacing them with easier words that the individual is better able to pronounce. This disorder causes many communication problems for the individual and may interfere with social communication and performance in work and/or school settings where communication is essential. Social (pragmatic) communication disorder - this diagnosis described difficulties in the social uses of verbal and nonverbal communication in naturalistic contexts, which affects the development of social relationships and discourse comprehension. The difference between this diagnosis and autism spectrum disorder is that in the latter there is also a restricted or repetitive pattern of behavior. Unspecified communication disorder - for those who have symptoms of a communication disorder but who do not meet all criteria, and whose symptoms cause distress or impairment. == Examples == Examples of disorders that may include or create challenges in language and communication and/or may co-occur with the above disorders: autism spectrum disorder - autistic disorder (also called classic autism), pervasive developmental disorder, and Asperger syndrome - developmental disorders that affect the brains normal development of social and communication skills. expressive language disorder - affects speaking and understanding where there is no delay in non-verbal intelligence. mixed receptive-expressive language disorder - affects speaking, understanding, reading and writing where there is no delay in non-verbal intelligence. specific language impairment - a language disorder that delays the mastery of language skills in children who have no hearing loss or other developmental delays. SLI is also called developmental language disorder, language delay, or developmental dysphasia. === Sensory impairments === Blindness - A link between communication skills and visual impairment with children who are blind is currently being investigated. Deafness/frequent ear infections - Trouble with hearing during language acquisition may lead to spoken language problems. Children who suffer from frequent ear infections may temporarily develop problems pronouncing words correctly. It should also be noted that some of the above communication disorders can occur with people who use sign language. The inability to hear is not in itself a communication disorder. === Aphasia === Aphasia is loss of the ability to produce or comprehend language. There are acute aphasias which result from stroke or brain injury, and primary progressive aphasias caused by progressive illnesses such as dementia. Acute aphasias Expressive aphasia also known as Brocas aphasia, expressive aphasia is a non-fluent aphasia that is characterized by damage to the frontal lobe region of the brain. A person with expressive aphasia usually speaks in short sentences that make sense but take great effort to produce. Also, a person with expressive aphasia understands another persons speech but has trouble responding quickly. Receptive aphasia also known as Wernickes aphasia, receptive aphasia is a fluent aphasia that is categorized by damage to the temporal lobe region of the brain. A person with receptive aphasia usually speaks in long sentences that have no meaning or content. People with this type of aphasia often have trouble understanding others speech and generally do not realize that they are not making any sense. Conduction aphasia Anomic aphasia Global aphasia Primary progressive aphasias Progressive nonfluent aphasia Semantic dementia Logopenic progressive aphasia === Learning disability === Dyscalculia - a defect of the systems used in communicating numbers Dyslexia - a defect of the systems used in reading Dysgraphia - a defect in the systems used in writing === Speech disorders === cluttering - a syndrome characterized by a speech delivery rate which is either abnormally fast, irregular, or both. dysarthria - a condition that occurs when problems with the muscles that helps a person to talk make it difficult to pronounce words. esophageal voice - involves the patient injecting or swallowing air into the esophagus. Usually learnt and used by patients who cannot use their larynges to speak. Once the patient has forced the air into their esophagus, the air vibrates a muscle and creates esophageal voice. Esophageal voice tends to be difficult to learn and patients are often only able to talk in short phrases with a quiet voice. lisp - a speech impediment that is also known as sigmatism. speech sound disorder - Speech-sound disorders (SSD) involve impairments in speech-sound production and range from mild articulation issues involving a limited number of speech sounds to more severe phonologic disorders involving multiple errors in speech-sound production and reduced intelligibility. stuttering - a speech disorder in which sounds, syllables, or words are repeated or last longer than normal. These problems cause a break in the flow of speech (called disfluency). == See also == == References == == Further reading == Cherney LR, Gardner P, Logemann JA, et al. (2010). The role of speech-language pathology and audiology in the optimal management of the service member returning from Iraq or Afghanistan with a blast-related head injury: position of the Communication Sciences and Disorders Clinical Trials Research Group. J Head Trauma Rehabil. 25 (3): 219-24. PMID 20473095. doi:10.1097/HTR.0b013e3181dc82c1. Wong PC, Perrachione TK, Gunasekera G, Chandrasekaran B; Perrachione; Gunasekera; Chandrasekaran (August 2009). Communication disorders in speakers of tone languages: etiological bases and clinical considerations. Semin Speech Lang. 30 (3): 162-73. PMC 2805066 . PMID 19711234. doi:10.1055/s-0029-1225953. CS1 maint: Multiple names: authors list (link) == External links == Communication Disorders Aphasia - National Institute on Deafness and Other Communication Disorders (NIDCD) Dysgraphia - National Institute on Deafness and Other Communication Disorders Voice and Speech Disorder Online Community (VoiceMatters.net) List of communication disorder related links Child Language Disorders Talking Point Check the progress of your childs language development"
Spell checker,Spell checker,,,"In computing, a spell checker (or spell check) is an application program that flags words in a document that may not be spelled correctly. Spell checkers may be stand-alone, capable of operating on a block of text, or as part of a larger application, such as a word processor, email client, electronic dictionary, or search engine. == Design == A basic spell checker carries out the following processes: It scans the text and extracts the words contained in it. It then compares each word with a known list of correctly spelled words (i.e. a dictionary). This might contain just a list of words, or it might also contain additional information, such as hyphenation points or lexical and grammatical attributes. An additional step is a language-dependent algorithm for handling morphology. Even for a lightly inflected language like English, the spell-checker will need to consider different forms of the same word, such as plurals, verbal forms, contractions, and possessives. For many other languages, such as those featuring agglutination and more complex declension and conjugation, this part of the process is more complicated. It is unclear whether morphological analysisŠ—ç’ ’–allowing for many different forms of a word depending on its grammatical roleŠ—ç’ ’–provides a significant benefit for English, though its benefits for highly synthetic languages such as German, Hungarian or Turkish are clear. As an adjunct to these components, the programs user interface will allow users to approve or reject replacements and modify the programs operation. An alternative type of spell checker uses solely statistical information, such as n-grams, to recognize errors instead of correctly-spelled words. This approach usually requires a lot of effort to obtain sufficient statistical information. Key advantages include needing less runtime storage and the ability to correct errors in words that are not included in a dictionary. In some cases spell checkers use a fixed list of misspellings and suggestions for those misspellings; this less flexible approach is often used in paper-based correction methods, such as the see also entries of encyclopedias. Clustering algorithms have also been used for spell checking combined with phonetic information. == History == Research extends back to 1957, including spelling checkers for bitmap images of cursive writing and special applications to find records in databases in spite of incorrect entries. In 1961, Les Earnest, who headed the research on this budding technology, saw it necessary to include the first spell checker that accessed a list of 10,000 acceptable words. Ralph Gorin, a graduate student under Earnest at the time, created the first true spelling checker program written as an applications program (rather than research) for general English text: Spell for the DEC PDP-10 at Stanford Universitys Artificial Intelligence Laboratory, in February 1971. Gorin wrote SPELL in assembly language, for faster action; he made the first spelling corrector by searching the word list for plausible correct spellings that differ by a single letter or adjacent letter transpositions and presenting them to the user. Gorin made SPELL publicly accessible, as was done with most SAIL (Stanford Artificial Intelligence Laboratory) programs, and it soon spread around the world via the new ARPAnet, about ten years before personal computers came into general use. Spell, its algorithms and data structures inspired the Unix ispell program. The first spell checkers were widely available on mainframe computers in the late 1970s. A group of six linguists from Georgetown University developed the first spell-check system for the IBM corporation. The first spell checkers for personal computers appeared for CP/M and TRS-80 computers in 1980, followed by packages for the IBM PC after it was introduced in 1981. Developers such as Maria Mariani, Random House, Soft-Art, Microlytics, Proximity, Circle Noetics, and Reference Software rushed OEM packages or end-user products into the rapidly expanding software market, primarily for the PC but also for Apple Macintosh, VAX, and Unix. On the PCs, these spell checkers were standalone programs, many of which could be run in TSR mode from within word-processing packages on PCs with sufficient memory. However, the market for standalone packages was short-lived, as by the mid-1980s developers of popular word-processing packages like WordStar and WordPerfect had incorporated spell checkers in their packages, mostly licensed from the above companies, who quickly expanded support from just English to European and eventually even Asian languages. However, this required increasing sophistication in the morphology routines of the software, particularly with regard to heavily-agglutinative languages like Hungarian and Finnish. Although the size of the word-processing market in a country like Iceland might not have justified the investment of implementing a spell checker, companies like WordPerfect nonetheless strove to localize their software for as many national markets as possible as part of their global marketing strategy. Firefox 2.0, a web browser, has spell check support for user-written content, such as when editing Wikitext, writing on many webmail sites, blogs, and social networking websites. The web browsers Google Chrome, Konqueror, and Opera, the email client Kmail and the instant messaging client Pidgin also offer spell checking support, transparently using GNU Aspell as their engine. Mac OS X now has spell check systemwide, extending the service to virtually all bundled and third party applications. == Functionality == The first spell checkers were verifiers instead of correctors. They offered no suggestions for incorrectly spelled words. This was helpful for typos but it was not so helpful for logical or phonetic errors. The challenge the developers faced was the difficulty in offering useful suggestions for misspelled words. This requires reducing words to a skeletal form and applying pattern-matching algorithms. It might seem logical that where spell-checking dictionaries are concerned, the bigger, the better, so that correct words are not marked as incorrect. In practice, however, an optimal size for English appears to be around 90,000 entries. If there are more than this, incorrectly spelled words may be skipped because they are mistaken for others. For example, a linguist might determine on the basis of corpus linguistics that the word baht is more frequently a misspelling of bath or bat than a reference to the Thai currency. Hence, it would typically be more useful if a few people who write about Thai currency were slightly inconvenienced than if the spelling errors of the many more people who discuss baths were overlooked. The first MS-DOS spell checkers were mostly used in proofing mode from within word processing packages. After preparing a document, a user scanned the text looking for misspellings. Later, however, batch processing was offered in such packages as Oracles short-lived CoAuthor and allowed a user to view the results after a document was processed and correct only the words that were known to be wrong. When memory and processing power became abundant, spell checking was performed in the background in an interactive way, such as has been the case with the Sector Software produced Spellbound program released in 1987 and Microsoft Word since Word 95. In recent years, spell checkers have become increasingly sophisticated; some are now capable of recognizing simple grammatical errors. However, even at their best, they rarely catch all the errors in a text (such as homophone errors) and will flag neologisms and foreign words as misspellings. Nonetheless, spell checkers can be considered as a type of foreign language writing aid that non-native language learners can rely on to detect and correct their misspellings in the target language. == Spell-checking non-English languages == English is unusual in that most words used in formal writing have a single spelling that can be found in a typical dictionary, with the exception of some jargon and modified words. In many languages, words are often concatenated into new combinations of words. In German, compound nouns are frequently coined from other existing nouns. Some scripts do not clearly separate one word from another, requiring word-splitting algorithms. Each of these presents unique challenges to non-English language spell checkers. == Context-sensitive spell checkers == Recently, research has focused on developing algorithms that are capable of recognizing a misspelled word, even if the word itself is in the vocabulary, based on the context of the surrounding words. Not only does this allow words such as those in the poem above to be caught, but it mitigates the detrimental effect of enlarging dictionaries, allowing more words to be recognized. For example, baht in the same paragraph as Thai or Thailand would not be recognized as a misspelling of bath. The most common example of errors caught by such a system are homophone errors, such as the bold words in the following sentence: Their coming too sea if its reel. The most successful algorithm to date is Andrew Golding and Dan Roths Winnow-based spelling correction algorithm, published in 1999, which is able to recognize about 96% of context-sensitive spelling errors, in addition to ordinary non-word spelling errors. A context-sensitive spell checker appears in Microsoft Office 2007, Google Wave, and in Ghotit Dyslexia Software context spell checker tuned for people with dyslexia. == Criticism == Some critics of technology and computers have attempted to link spell checkers to a trend of skill losses in writing, reading, and speaking. They claim that computers have made people lazy, often not proofreading written work except for a simple pass through a spell checker. Supporters claim that these changes may actually be beneficial to society, by making writing and learning new languages more accessible to ordinary people. They claim that the skills lost by the use of automated spell checkers are replaced by better skills, such as faster and more efficient research skills. Other supporters of technology point out that these skills are not being lost by people who use written language frequently in their work, such as authors and journalists. An example of the folly of relying completely on spell checkers is shown in the Spell-checker Poem above. It was originally composed by Dr Jerrold H. Zar in 1991, assisted by Mark Eckman, with an original length of 225 words, and containing 123 incorrectly used words. According to most unsophisticated spell checkers, the poem is valid, but most people can tell at a simple glance that most words are used incorrectly. As a result, spell checkers are sometimes derided as spilling chuckers or similar, slightly misspelled names. Not all critics are opponents of technological progress, however. An article based on research by Galletta et al. reports that higher verbal skills are needed for highest performance when using a spell checker. The theory suggested that only writers with higher verbal skills could recognize and ignore false positives or incorrect suggestions. However, it was found that those with the higher skills lost their unaided performance advantage in multiple categories of errors, performing as poorly as the low verbals with the spell-checkers turned on. The conclusion points to some evidence of a loss of skill. == See also == Approximate string matching Cupertino effect Grammar checker Record linkage problem Spelling suggestion Words (Unix) == References == == External links == List of spell checkers on dmoz.org Norvig.com, How to Write a Spelling Corrector, by Peter Norvig BBK.ac.uk, Spellchecking by computer, by Roger Mitton CBSNews.com, Spell-Check Crutch Curtails Correctness, by Lloyd de Vries NIU.edu, Candidate for a Pullet Surprise - Complete corrected poem Microsoft Word Spelling and Grammar Check Demonstration"
Grammar checker,Grammar checker,,,"A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness. Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text. The implementation of a grammar checker makes use of natural language processing == History == The earliest grammar checkers were programs that checked for punctuation and style inconsistencies, rather than a complete range of possible grammatical errors. The first system was called Writers Workbench, and was a set of writing tools included with Unix systems as far back as the 1970s. The whole Writers Workbench package included several separate tools to check for various writing problems. The diction tool checked for wordy, trite, clich’‘Î©d or misused phrases in a text. The tool would output a list of questionable phrases, and provide suggestions for improving the writing. The style tool analyzed the writing style of a given text. It performed a number of readability tests on the text and output the results, and gave some statistical information about the sentences of the text. Aspen Software of Albuquerque, NM released the earliest version of a diction and style checker for personal computers, Grammatik, in 1981. Grammatik was first available for a Radio Shack - TRS-80, and soon had versions for CP/M and the IBM PC. Reference Software of San Francisco, CA, acquired Grammatik in 1985. Development of Grammatik continued, and it became an actual grammar checker that could detect writing errors beyond simple style checking. Other early diction and style checking programs included Punctuation & Style, Correct Grammar, and RightWriter. While all the earliest programs started out as simple diction and style checkers, all eventually added various levels of language processing, and developed some level of true grammar checking capability. Until 1992, grammar checkers were sold as add-on programs. There were a large number of different word processing programs available at that time, with WordPerfect and Microsoft Word the top two in market share. In 1992, Microsoft decided to add grammar checking as a feature of Word, and licensed CorrecText, a grammar checker from Houghton Mifflin that had not yet been marketed as a standalone product. WordPerfect answered Microsofts move by acquiring Reference Software, and the direct descendant of Grammatik is still included with WordPerfect. There are free and open-source grammar checking software like LanguageTool, which can be used from Wikimedia Labs onto Wikipedia articles. == Technical issues == The earliest writing style programs checked for wordy, trite, clich’‘Î©d, or misused phrases in a text. This process was based on simple pattern matching. The heart of the program was a list of many hundreds or thousands of phrases that are considered poor writing by many experts. The list of questionable phrases included alternative wording for each phrase. The checking program would simply break text into sentences, check for any matches in the phrase dictionary, flag suspect phrases and show an alternative. These programs could also perform some mechanical checks. For example, they would typically flag doubled words, doubled punctuation, some capitalization errors, and other simple mechanical mistakes. True grammar checking is more complex. While a computer programming language has a very specific syntax and grammar, this is not so for natural languages. One can write a somewhat complete formal grammar for a natural language, but there are usually so many exceptions in real usage that a formal grammar is of minimal help in writing a grammar checker. One of the most important parts of a natural language grammar checker is a dictionary of all the words in the language, along with the part of speech of each word. The fact that natural words can take many different parts of speech greatly increases the complexity of any grammar checker. A grammar checker will find each sentence in a text, look up each word in the dictionary, and then attempt to parse the sentence into a form that matches a grammar. Using various rules, the program can then detect various errors, such as agreement in tense, number, word order, and so on. It is also possible to detect some stylistic problems with the text. For example, some popular style guides such as The Elements of Style deprecate excessive use of the passive voice. Grammar checkers may attempt to identify passive sentences and suggest an active-voice alternative. The software elements required for grammar checking are closely related to some of the development issues that need to be addressed for voice recognition software. In voice recognition, parsing can be used to help predict which word is most likely intended, based on part of speech and position in the sentence. In grammar checking, the parsing is used to detect words that fail to follow accepted grammar usage. Recently, research has focused on developing algorithms which can recognize grammar errors based on the context of the surrounding words. Context-based grammar checkers appear in Microsoft Office 2010, Microsoft Office 2007, Google Wave, Ghotit Dyslexia Software, Grammarly, SpellCheckPlus.com, Nounplus.net, GrammarCheck.net, PaperRater, Ginger Software, VirtualWritingTutor.com, and WhiteSmoke. == Criticism == Grammar checkers are considered as a type of foreign language writing aid which non-native speakers can use to proofread their writings as such programs endeavor to identify syntactical errors. However, as with other computerized writing aids such as spell checkers, popular grammar checkers are often criticized when they fail to spot errors and incorrectly flag correct text as erroneous. The linguist Geoffrey K. Pullum has argued that they are generally so inaccurate as to do more harm than good: for the most part, accepting the advice of a computer grammar checker on your prose will make it much worse, sometimes hilariously incoherent. == See also == Spell checker Link grammar == References =="
Standard Boolean model,Standard Boolean model,,,"The Boolean model of information retrieval (BIR) is a classical information retrieval (IR) model and, at the same time, the first and most adopted one. It is used by many IR systems to this day. == Definitions == The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the users query are conceived as sets of terms. Retrieval is based on whether or not the documents contain the query terms. Given a finite set T = {t1, t2, ..., tj, ..., tm} of elements called index terms (e.g. words or expressions - which may be stemmed - describing or characterising documents such as keywords given for a journal article), a finite set D = {D1, ..., Di, ..., Dn}, where Di is an element of the powerset of T of elements called documents. Given a Boolean expression - in a normal form - Q called a query as follows: Q = (Wi OR Wk OR ...) AND ... AND (Wj OR Ws OR ...), with Wi=ti, Wk=tk, Wj=tj, Ws=ts, or Wi=NON ti, Wk=NON tk, Wj=NON tj, Ws=NON ts where ti means that the term ti is present in document Di, whereas NON ti means that it is not. Equivalently, Q can be given in a disjunctive normal form, too. An operation called retrieval, consisting of two steps, is defined as follows: 1. The sets Sj of documents are obtained that contain or not term tj (depending on whether Wj=tj or Wj=NON tj) : Sj = {Di|Wj element of Di} 2. Those documents are retrieved in response to Q which are the result of the corresponding sets operations, i.e. the answer to Q is as follows: UNION ( INTERSECTION Sj) == Example == Let the set of original (real) documents be, for example O = {O1, O2, O3} where O1 = Bayes Principle: The principle that, in estimating a parameter, one should initially assume that each possible value has equal probability (a uniform prior distribution). O2 = Bayesian Decision Theory: A mathematical theory of decision-making which presumes utility and probability functions, and according to which the act to be chosen is the Bayes act, i.e. the one with highest subjective expected utility. If one had unlimited time and calculating power with which to make every decision, this procedure would be the best way to make any decision. O3 = Bayesian Epistemology: A philosophical theory which holds that the epistemic status of a proposition (i.e. how well proven or well established it is) is best measured by a probability and that the proper way to revise this probability is given by Bayesian conditionalisation or similar procedures. A Bayesian epistemologist would use probability to define, and explore the relationship between, concepts such as epistemic status, support or explanatory power. Let the set T of terms be: T = {t1 = Bayes Principle, t2 = probability, t3 = decision-making, t4 = Bayesian Epistemology} Then, the set D of documents is as follows: D = {D1, D2, D3} where D1 = {Bayes Principle, probability} D2 = {probability, decision-making} D3 = {probability, Bayesian Epistemology} Let the query Q be: Q = probability AND decision-making 1. Firstly, the following sets S1 and S2 of documents Di are obtained (retrieved): S1 = {D1, D2, D3} S2 = {D2} 2. Finally, the following documents Di are retrieved in response to Q: {D1, D2, D3} INTERSECTION {D2} = {D2} This means that the original document O2 (corresponding to D2) is the answer to Q. Obviously, if there is more than one document with the same representation, every such document is retrieved. Such documents are, in the BIR, indistinguishable (or, in other words, equivalent). == Advantages == Clean formalism Easy to implement Intuitive concept == Disadvantages == Exact matching may retrieve too few or too many documents Hard to translate a query into a Boolean expression All terms are equally weighted More like data retrieval than information retrieval == Data structures and algorithms == From a pure formal mathematical point of view, the BIR is straightforward. From a practical point of view, however, several further problems should be solved that relate to algorithms and data structures, such as, for example, the choice of terms (manual or automatic selection or both), stemming, hash tables, inverted file structure, and so on. === Hash Sets === Another possibility is to use hash sets. Each document is represented by a hash table which contains every single term of that document. Since Hash-table size increases and decreases in real time with the addition and removal of terms, each document will occupy much less space in memory. However, it will have a slowdown in performance because the operations are more complex than with bit vectors. On the worst-case performance can degrade from O(n) to O(n2). On the average case, the performance slowdown will not be that much worse than bit vectors and the space usage is much more efficient. == References == Lashkari, A.H.; Mahdavi, F.; Ghomi, V. (2009), A Boolean Model in Information Retrieval for Search Engines, doi:10.1109/ICIME.2009.101"
Stop words,Stop words,,,"In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though stop words usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. Any group of words can be chosen as the stop words for a given purpose. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as The Who, The The, or Take That. Other search engines remove some of the most common wordsŠ—ç’ ’–including lexical words, such as wantŠ—ç’ ’–from a query in order to improve performance. Hans Peter Luhn, one of the pioneers in information retrieval, is credited with coining the phrase and using the concept. The phrase stop word, which is not in Luhns 1959 presentation, and the associated terms stop list and stoplist appear in the literature shortly afterwards. A predecessor concept was used in creating some concordances. For example, the first Hebrew concordance, MeŠ—ç’ Îéir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words. == See also == == References == == External links == List of English Stop Words (PHP array, CSV) Full-Text Stopwords in MySQL English Stop Words (CSV) Hindi Stop Words German Stop Words,German Stop Words and phrases, another list of German stop words Polish Stop Words Collection of stop words in 29 languages [1] A Detailed Explanation of Stop Words by Kavita Ganesan"
Text segmentation,Text segmentation,,,"Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages. Compare speech segmentation, the process of dividing speech into linguistically meaningful portions. == Segmentation problems == === Word segmentation === Word segmentation is the problem of dividing a string of written language into its component words. In English and many other languages using some form of the Latin alphabet, the space is a good approximation of a word divider (word delimiter). (Some examples where the space character alone may not be sufficient include contractions like wont for will not.) However the equivalent to this character is not found in all written scripts, and without it word segmentation is a difficult problem. Languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited. In some writing systems however, such as the Geez script used for Amharic and Tigrinya among other languages, words are explicitly delimited (at least historically) with a non-whitespace character. The Unicode Consortium has published a Standard Annex on Text Segmentation, exploring the issues of segmentation in multiscript texts. Word splitting is the process of parsing concatenated text (i.e. text that contains no spaces or other word separators) to infer where word breaks exist. Word splitting may also refer to the process of hyphenation. === Intent segmentation === Intent segmentation is the problem of dividing written words into keyphrases (2 or more group of words). In English and all other languages the core intent or desire is identified and become the corner-stone of the keyphrase Intent segmentation. Core product/service, idea, action & or thought anchor the keyphrase. [All things are made of atoms]. [Little particles that move] [around in perpetual motion], [attraction each other] [when they are a little distance apart], [but repelling] [upon being squeezed] [into one another]. === Sentence segmentation === Sentence segmentation is the problem of dividing a string of written language into its component sentences. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation. However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence. For example Mr. is not its own sentence in Mr. Smith went to the shops in Jones Street. When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries. As with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries. === Topic segmentation === Topic analysis consists of two main tasks: topic identiŠ—¢’äÎcation and text segmentation. While the first is a simple classification of a specific text, the latter case implies that a document may contain multiple topics, and the task of computerized text segmentation may be to discover these topics automatically and segment the text accordingly. The topic boundaries may be apparent from section titles and paragraphs. In other cases, one needs to use techniques similar to those used in document classification. Segmenting the text into topics or discourse turns might be useful in some natural processing tasks: it can improve information retrieval or speech recognition significantly (by indexing/recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result). It is also needed in topic detection and tracking systems and text summarizing problems. Many different approaches have been tried: e.g. HMM, lexical chains, passage similarity using word co-occurrence, clustering, topic modeling, etc. It is quite an ambiguous task - people evaluating the text segmentation systems often differ in topic boundaries. Hence, text segment evaluation is also a challenging problem. === Other segmentation problems === Processes may be required to segment text into segments besides mentioned, including morphemes (a task usually called morphological analysis) or paragraphs. == Automatic segmentation approaches == Automatic segmentation is the problem in natural language processing of implementing a computer process to segment text. When punctuation and similar clues are not consistently available, the segmentation task often requires fairly non-trivial techniques, such as statistical decision-making, large dictionaries, as well as consideration of syntactic and semantic constraints. Effective natural language processing systems and text segmentation tools usually operate on text in specific domains and sources. As an example, processing text used in medical records is a very different problem than processing news articles or real estate advertisements. The process of developing text segmentation tools starts with collecting a large corpus of text in an application domain. There are two general approaches: Manual analysis of text and writing custom software Annotate the sample corpus with boundary information and use machine learning Some text segmentation systems take advantage of any markup like HTML and know document formats like PDF to provide additional evidence for sentence and paragraph boundaries. == See also == Hyphenation Natural language processing Speech segmentation Lexical analysis Word count Line breaking == References == == External links == Word Segment An open source software tool for word segmentation in Chinese. Word Split An open source software tool designed to split conjoined words into human-readable text. Stanford Segmenter An open source software tool for word segmentation in Chinese or morpheme segmentation in Arabic. KyTea An open source software tool for word segmentation in Japanese and Chinese. Chinese Notes A Chinese-English dictionary that also does word segmentation. Zhihuita Segmentor A high precision and high performance Chinese segmentation freeware. Python wordsegment module An open source Python module for English word segmentation."
Wildcard character,Wildcard character,,,"In software, a wildcard character is a single character, such as an asterisk (*), used to represent a number of characters or an empty string. It is often used in file searches so the full name need not be typed. == Telecommunication == In telecommunications, a wildcard is a character that may be substituted for any of a defined subset of all possible characters. In high-frequency (HF) radio automatic link establishment, the wildcard character ? may be substituted for any one of the 36 upper-case alphanumeric characters. Whether the wildcard character represents a single character or a string of characters must be specified. == Computing == In computer (software) technology, a wildcard is a symbol used to replace or represent one or more characters. === File and directory patterns === When specifying file names (or paths) in CP/M, DOS, Microsoft Windows, and Unix-like operating systems, the asterisk character (*, also called star) matches zero or more characters. For example, doc* matches doc and document but not dodo. In Unix-like and DOS operating systems, the question mark ? matches exactly one character. In DOS, if the question mark is placed at the end of the word, it will also match missing (zero) trailing characters; for example, the pattern 123? will match 123 and 1234, but not 12345. In Unix shells and Windows PowerShell, ranges of characters enclosed in square brackets ([ and ]) match a single character within the set; for example, [A-Za-z] matches any single uppercase or lowercase letter. In Unix shells, a leading exclamation mark ! negates the set and matches only a character not within the list. In shells that interpret ! as a history substitution, a leading caret ^ can be used instead. The operation of matching of wildcard patterns to multiple file or path names is referred to as globbing. === Databases === In SQL, wildcard characters can be used in LIKE expressions; the percent sign % matches zero or more characters, and underscore a single character. Transact-SQL also supports square brackets ([ and ]) to list sets and ranges of characters to match, a leading caret ^ negates the set and matches only a character not within the list. In Microsoft Access, the asterisk sign * matches zero or more characters, the question mark ? matches a single character, the number sign # matches a single digit (0-9), and square brackets can be used for sets or ranges of characters to match. === Regular expressions === In regular expressions, the period (., also called dot) is the wildcard pattern which matches a single character. Combined with the asterisk operator .* it will match any number of characters. In this case, the asterisk is also known as the Kleene star. == See also == glob (programming) Kleene star Pattern matching Query by Example Wildcard DNS record wildmat == References == This article incorporates public domain material from the General Services Administration document Federal Standard 1037C (in support of MIL-STD-188). == External links == How to Use Wildcards"
Text normalization,Text normalization,,,"Text normalization is the process of transforming text into a single canonical form that it might not have had before. Normalizing text before storing or processing it allows for separation of concerns, since input is guaranteed to be consistent before operations are performed on it. Text normalization requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose normalization procedure. == Applications == Text normalization is frequently used when converting text to speech. Numbers, dates, acronyms, and abbreviations are non-standard words that need to be pronounced differently depending on context. For example: $200 would be pronounced as two hundred dollars in English, but as lua selau tŠäŒÎlŠäŒÎ in Samoan. vi could be pronounced as vie, vee, or the sixth depending on the surrounding words. Text can also be normalized for storing and searching in a database. For instance, if a search for resume is to match the word r’‘Î©sum’‘Î©, then the text would be normalized by removing diacritical marks; and if john is to match John, the text would be converted to a single case. To prepare text for searching, it might also be stemmed (e.g. converting flew and flying both into fly), canonicalized (e.g. consistently using American or British English spelling), or have stop words removed. == Techniques == For simple, context-independent normalization, such as removing non-alphanumeric characters or diacritical marks, regular expressions would suffice. For example, the sed script sed -e s/+/ /g inputfile would normalize runs of whitespace characters into a single space. More complex normalization requires correspondingly complicated algorithms, including domain knowledge of the language and vocabulary being normalized. Among other approaches, text normalization has been modeled as a problem of tokenizing and tagging streams of text and as a special case of machine translation. == References == == See also == Canonicalization Unicode equivalence"
Zipf's law,Zipf's law,,,"Zipfs law () is an empirical law formulated using mathematical statistics that refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. The law is named after the American linguist George Kingsley Zipf (1902-1950), who popularized it and sought to explain it (Zipf 1935, 1949), though he did not claim to have originated it. The French stenographer Jean-Baptiste Estoup (1868-1950) appears to have noticed the regularity before Zipf. It was also noted in 1913 by German physicist Felix Auerbach (1856-1933). == Motivation == Zipfs law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. For example, in the Brown Corpus of American English text, the word the is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipfs Law, the second-place word of accounts for slightly over 3.5% of words (36,411 occurrences), followed by and (28,852). Only 135 vocabulary items are needed to account for half the Brown Corpus. The same relationship occurs in many other rankings unrelated to language, such as the population ranks of cities in various countries, corporation sizes, income rankings, ranks of number of people watching the same TV channel, and so on. The appearance of the distribution in rankings of cities by population was first noticed by Felix Auerbach in 1913. Empirically, a data set can be tested to see whether Zipfs law applies by checking the goodness of fit of an empirical distribution to the hypothesized power law distribution with a Kolmogorov-Smirnov test, and then comparing the (log) likelihood ratio of the power law distribution to alternative distributions like an exponential distribution or lognormal distribution. When Zipfs law is checked for cities, a better fit has been found with exponent s = 1.07; i.e. the n t h { n^{th}} largest settlement is 1 n 1.07 {{n^{1.07}}}} the size of the largest settlement. While Zipfs law holds for the upper tail of the distribution, the entire distribution of cities is log-normal and follows Gibrats law. Both laws are consistent because a log-normal tail can typically not be distinguished from a Pareto (Zipf) tail. == Theoretical review == Zipfs law is most easily observed by plotting the data on a log-log graph, with the axes being log (rank order) and log (frequency). For example, the word the (as described above) would appear at x = log(1), y = log(69971). It is also possible to plot reciprocal rank against frequency or reciprocal frequency or interword interval against rank. The data conform to Zipfs law to the extent that the plot is linear. Formally, let: N be the number of elements; k be their rank; s be the value of the exponent characterizing the distribution. Zipfs law then predicts that out of a population of N elements, the frequency of elements of rank k, f(k;s,N), is: f ( k ; s , N ) = 1 / k s Š—ç’ ’” n = 1 N ( 1 / n s ) { f(k;s,N)={}{ {n=1}^{N}(1/n^{s})}}} Zipfs law holds if the number of elements with a given frequency is a random variable with power law distribution p ( f ) = ’âÎ± f Š—ç’ ’« 1 Š—ç’ ’« 1 / s . { p(f)= f^{-1-1/s}.} It has been claimed that this representation of Zipfs law is more suitable for statistical testing, and in this way it has been analyzed in more than 30,000 English texts. The goodness-of-fit tests yield that only about 15% of the texts are statistically compatible with this form of Zipfs law. Slight variations in the definition of Zipfs law can increase this percentage up to close to 50%. In the example of the frequency of words in the English language, N is the number of words in the English language and, if we use the classic version of Zipfs law, the exponent s is 1. f(k; s,N) will then be the fraction of the time the kth most common word occurs. The law may also be written: f ( k ; s , N ) = 1 k s H N , s { f(k;s,N)={{k^{s}H {N,s}}}} where HN,s is the Nth generalized harmonic number. The simplest case of Zipfs law is a 1Š—çÎ’£f function. Given a set of Zipfian distributed frequencies, sorted from most common to least common, the second most common frequency will occur ’Ç as often as the first. The third most common frequency will occur - as often as the first. The fourth most common frequency will occur ’Ç as often as the first. The nth most common frequency will occur 1Š—çÎ’£n as often as the first. However, this cannot hold exactly, because items must occur an integer number of times; there cannot be 2.5 occurrences of a word. Nevertheless, over fairly wide ranges, and to a fairly good approximation, many natural phenomena obey Zipfs law. Mathematically, the sum of all relative frequencies in a Zipf distribution is equal to the harmonic series, and Š—ç’ ’” n = 1 Š—ç’ 1 n = Š—ç’ . { {n=1}^{ }{{n}}= .!} In human languages, word frequencies have a very heavy-tailed distribution, and can therefore be modeled reasonably well by a Zipf distribution with an s close to 1. As long as the exponent s exceeds 1, it is possible for such a law to hold with infinitely many words, since if s > 1 then ’âÎ ( s ) = Š—ç’ ’” n = 1 Š—ç’ 1 n s < Š—ç’ . { (s)= {n=1}^{ }{{n^{s}}}< .!} where ’âÎ is Riemanns zeta function. In a more basic explanation, the most searched word/object will be 1/1 because it is the most searched, the second be 1/2 because it is half as searched then the first one, and third 1/3 for the third searched object. Every graph (That depicts most searched of anything) will follow a similar pattern. It can be represented by 1/n. The variable n will represent what place it has been searched from the second most searched object to the 3 millionth. N: Stands for number == Statistical explanation == Although ZipfŠ—ç’ Îés Law holds for most languages, even for non-natural languages like Esperanto, the reason is still not well understood. However, it may be partially explained by the statistical analysis of randomly generated texts. Wentian Li has shown that in a document in which each character has been chosen randomly from a uniform distribution of all letters (plus a space character), the words follow the general trend of Zipfs law (appearing approximately linear on log-log plot). Vitold Belevitch in a paper, On the Statistical Laws of Linguistic Distribution offered a mathematical derivation. He took a large class of well-behaved statistical distributions (not only the normal distribution) and expressed them in terms of rank. He then expanded each expression into a Taylor series. In every case Belevitch obtained the remarkable result that a first-order truncation of the series resulted in Zipfs law. Further, a second-order truncation of the Taylor series resulted in Mandelbrots law. The principle of least effort is another possible explanation: Zipf himself proposed that neither speakers nor hearers using a given language want to work any harder than necessary to reach understanding, and the process that results in approximately equal distribution of effort leads to the observed Zipf distribution. Similarly, preferential attachment (intuitively, the rich get richer or success breeds success) that results in the Yule-Simon distribution has been shown to fit word frequency versus rank in language and population versus city rank better than Zipfs law. It was originally derived to explain population versus rank in species by Yule, and applied to cities by Simon. == Related laws == Zipfs law in fact refers more generally to frequency distributions of rank data, in which the relative frequency of the nth-ranked item is given by the Zeta distribution, 1/(ns’âÎ (s)), where the parameter s > 1 indexes the members of this family of probability distributions. Indeed, Zipfs law is sometimes synonymous with zeta distribution, since probability distributions are sometimes called laws. This distribution is sometimes called the Zipfian distribution. A generalization of Zipfs law is the Zipf-Mandelbrot law, proposed by Beno’‘Îåt Mandelbrot, whose frequencies are: f ( k ; N , q , s ) = [ constant ] ( k + q ) s . { f(k;N,q,s)={]}{(k+q)^{s}}}.,} The constant is the reciprocal of the Hurwitz zeta function evaluated at s. In practice, as easily observable in distribution plots for large corpora, the observed distribution can better be modelled as a sum of separate distributions for different subsets or subtypes of words that follow different parameterizations of the Zipf-Mandelbrot distribution, in particular the closed class of functional words exhibit s lower than 1, while open-ended vocabulary growth with document size and corpus size require s greater than 1 for convergence of the Generalized Harmonic Series. Zipfian distributions can be obtained from Pareto distributions by an exchange of variables. The Zipf distribution is sometimes called the discrete Pareto distribution because it is analogous to the continuous Pareto distribution in the same way that the discrete uniform distribution is analogous to the continuous uniform distribution. The tail frequencies of the Yule-Simon distribution are approximately f ( k ; ’ŒÎ ) Š—ç’ ’ [ constant ] k ’ŒÎ + 1 { f(k; )]}{k^{ +1}}}} for any choice of ’ŒÎ > 0. In the parabolic fractal distribution, the logarithm of the frequency is a quadratic polynomial of the logarithm of the rank. This can markedly improve the fit over a simple power-law relationship. Like fractal dimension, it is possible to calculate Zipf dimension, which is a useful parameter in the analysis of texts. It has been argued that Benfords law is a special bounded case of Zipfs law, with the connection between these two laws being explained by their both originating from scale invariant functional relations from statistical physics and critical phenomena. The ratios of probabilities in Benfords law are not constant. The leading digits of data satisfying Zipfs law with s = 1 satisfy Benfords law. == Applications == In information theory, a symbol (event, signal) of probability p { p} contains lg Š—çÎ’ ( 1 / p ) { (1/p)} bits of information. Hence, Zipf law for natural numbers: Pr ( x ) Š—ç’ ’ 1 / x {}(x) 1/x} is equivalent with number x { x} containing lg Š—çÎ’ ( x ) { (x)} bits of information. To add information from a symbol of probability p { p} into information already stored in a natural number x { x} , we should go to x Š—ç’ { x} such that lg Š—çÎ’ ( x Š—ç’ ) Š—ç’ ’ lg Š—çÎ’ ( x ) + lg Š—çÎ’ ( 1 / p ) { (x) (x)+(1/p)} , or equivalently x Š—ç’ Š—ç’ ’ x / p { x x/p} . For instance, in standard binary system we would have x Š—ç’ = 2 x + s { x=2x+s} , what is optimal for Pr ( s = 0 ) = Pr ( s = 1 ) = 1 / 2 {}(s=0)={}(s=1)=1/2} probability distribution. Using x Š—ç’ Š—ç’ ’ x / p { x x/p} rule for a general probability distribution is the base of Asymmetric Numeral Systems family of entropy coding methods used in data compression, which state distribution is also governed by Zipf law. == See also == == References == == Further reading == Primary: George K. Zipf (1949) Human Behavior and the Principle of Least Effort. Addison-Wesley. George K. Zipf (1935) The Psychobiology of Language. Houghton-Mifflin. (see citations at http://citeseer.ist.psu.edu/context/64879/0 ) Secondary: Alexander Gelbukh and Grigori Sidorov (2001) Zipf and Heaps LawsŠ—ç’ Îé Coefficients Depend on Language. Proc. CICLing-2001, Conference on Intelligent Text Processing and Computational Linguistics, February 18-24, 2001, Mexico City. Lecture Notes in Computer Science N 2004, ISSN 0302-9743, ISBN 3-540-41687-0, Springer-Verlag: 332-335. Dami’‘’n H. Zanette (2006) Zipfs law and the creation of musical context, Musicae Scientiae 10: 3-18. Frans J. Van Droogenbroeck (2016) Handling the Zipf distribution in computerized authorship attribution Kali R. (2003) The city as a giant component: a random graph approach to Zipfs law, Applied Economics Letters 10: 717-720(4) Gabaix, Xavier (August 1999). Zipfs Law for Cities: An Explanation (PDF). Quarterly Journal of Economics. 114 (3): 739-67. ISSN 0033-5533. doi:10.1162/003355399556133. Axtell, Robert L; Zipf distribution of US firm sizes, Science, 293, 5536, 1818, 2001, American Association for the Advancement of Science Ramu Chenna, Toby Gibson; Evaluation of the Suitability of a Zipfian Gap Model for Pairwise Sequence Alignment, International Conference on Bioinformatics Computational Biology: 2011. Shyklo A. (2017); Simple Explanation of Zipfs Mystery via New Rank-Share Distribution, Derived from Combinatorics of the Ranking Process, Available at SSRN: https://ssrn.com/abstract=2918642. == External links == Strogatz, Steven (2009-05-29). Guest Column: Math and the City. The New York Times. Retrieved 2009-05-29. Š—ç’ ’–An article on Zipfs law applied to city populations Seeing Around Corners (Artificial societies turn up Zipfs law) PlanetMath article on Zipfs law Distributions de type fractal parabolique dans la Nature (French, with English summary) An analysis of income distribution Zipf List of French words Zipf list for English, French, Spanish, Italian, Swedish, Icelandic, Latin, Portuguese and Finnish from Gutenberg Project and online calculator to rank words in texts Citations and the Zipf-Mandelbrots law Zipfs Law examples and modelling (1985) Complex systems: Unzipping Zipfs law (2011) BenfordŠ—ç’ Îés law, ZipfŠ—ç’ Îés law, and the Pareto distribution by Terence Tao."
Tf-idf,Tf-idf,,,"In information retrieval, tf-idf, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes. For instance, 83% of text-based recommender systems in the domain of digital libraries use tf-idf. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a documents relevance given a user query. tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification. One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model. == Motivations == === Term frequency === Suppose we have a set of English text documents and wish to determine which document is most relevant to the query the brown cow. A simple way to start out is by eliminating documents that do not contain all three words the, brown, and cow, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, in the case where the length of documents vary greatly, adjustments are often made (see definition below). The first form of term weighting is due to Hans Peter Luhn (1957) and is based on the Luhn Assumption: The weight of a term that occurs in a document is simply proportional to the term frequency. === Inverse document frequency === Because the term the is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word the more frequently, without giving enough weight to the more meaningful terms brown and cow. The term the is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words brown and cow. Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. Karen Sp’‘ rck Jones (1972) conceived a statistical interpretation of term specificity called Inverse Document Frequency (IDF), which became a cornerstone of term weighting: The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. == Definition == tf-idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. === Term frequency === In the case of the term frequency tf(t,d), the simplest choice is to use the raw count of a term in a document, i.e. the number of times that term t occurs in document d. If we denote the raw count by ft,d, then the simplest tf scheme is tf(t,d) = ft,d. Other possibilities include Boolean frequencies: tf(t,d) = 1 if t occurs in d and 0 otherwise; term frequency adjusted for document length : ft,d / (number of words in d) logarithmically scaled frequency: tf(t,d) = 1 + log ft,d, or zero if ft,d is zero; augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the raw frequency of the most occurring term in the document: t f ( t , d ) = 0.5 + 0.5 Š—ç’“’ f t , d max { f t Š—ç’ , d : t Š—ç’ Š—ç’ ’ d } { (t,d)=0.5+0.5}{:t d}}}} === Inverse document frequency === The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. i d f ( t , D ) = log Š—çÎ’ N | { d Š—ç’ ’ D : t Š—ç’ ’ d } | { (t,D)={||}}} with N { N} : total number of documents in the corpus N = | D | { N={|D|}} | { d Š—ç’ ’ D : t Š—ç’ ’ d } | { ||} : number of documents where the term t { t} appears (i.e., t f ( t , d ) Š—ç’ ’ 0 { (t,d) 0} ). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to 1 + | { d Š—ç’ ’ D : t Š—ç’ ’ d } | { 1+||} . === Term frequency-Inverse document frequency === Then tf-idf is calculated as t f i d f ( t , d , D ) = t f ( t , d ) Š—ç’“’ i d f ( t , D ) { (t,d,D)= (t,d) (t,D)} A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idfs log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0. == Justification of idf == Idf was introduced, as term specificity, by Karen Sp’‘ rck Jones in a 1972 paper. Although it has worked well as a heuristic, its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find information theoretic justifications for it. Sp’‘ rck Joness own explanation did not propose much theory, aside from a connection to Zipfs law. Attempts have been made to put idf on a probabilistic footing, by estimating the probability that a given document d contains a term t as the relative document frequency, P ( t | d ) = | { d Š—ç’ ’ D : t Š—ç’ ’ d } | N , { P(t|d)={{N}},} so that we can define idf as i d f = Š—ç’ ’« log Š—çÎ’ P ( t | d ) = log Š—çÎ’ 1 P ( t | d ) = log Š—çÎ’ N | { d Š—ç’ ’ D : t Š—ç’ ’ d } | { &=- P(t|d)&={P(t|d)}}&={||}}}} Namely, the inverse document frequency is the logarithm of inverse relative document frequency. This probabilistic interpretation in turn takes the same form as that of self-information. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate event spaces for the required probability distributions: not only documents need to be taken into account, but also queries and terms. == Example of tf-idf == Suppose that we have term count tables of a corpus consisting of only two documents, as listed on the right. The calculation of tf-idf for the term this is performed as follows: In its raw frequency form, tf is just the frequency of the this for each document. In each document, the word this appears once; but as the document 2 has more words, its relative frequency is smaller. t f ( Š—ç’ t h i s Š—ç’ , d 1 ) = 1 5 = 0.2 { ({},d {1})={{5}}=0.2} t f ( Š—ç’ t h i s Š—ç’ , d 2 ) = 1 7 Š—ç’ ’ 0.14 { ({},d {2})={{7}} 0.14} An idf is constant per corpus, and accounts for the ratio of documents that include the word this. In this case, we have a corpus of two documents and all of them include the word this. i d f ( Š—ç’ t h i s Š—ç’ , D ) = log Š—çÎ’ ( 2 2 ) = 0 { ({},D)= ({{2}})=0} So tf-idf is zero for the word this, which implies that the word is not very informative as it appears in all documents. t f i d f ( Š—ç’ t h i s Š—ç’ , d 1 ) = 0.2 ’‘’• 0 = 0 { ({},d {1})=0.2 0=0} t f i d f ( Š—ç’ t h i s Š—ç’ , d 2 ) = 0.14 ’‘’• 0 = 0 { ({},d {2})=0.14 0=0} A slightly more interesting example arises from the word example, which occurs three times but only in the second document: t f ( Š—ç’ e x a m p l e Š—ç’ , d 1 ) = 0 5 = 0 { ({},d {1})={{5}}=0} t f ( Š—ç’ e x a m p l e Š—ç’ , d 2 ) = 3 7 Š—ç’ ’ 0.429 { ({},d {2})={{7}} 0.429} i d f ( Š—ç’ e x a m p l e Š—ç’ , D ) = log Š—çÎ’ ( 2 1 ) = 0.301 { ({},D)= ({{1}})=0.301} Finally, t f i d f ( Š—ç’ e x a m p l e Š—ç’ , d 1 ) = t f ( Š—ç’ e x a m p l e Š—ç’ , d 1 ) ’‘’• i d f ( Š—ç’ e x a m p l e Š—ç’ , D ) = 0 ’‘’• 0.301 = 0 { ({},d {1})= ({},d {1}) ({},D)=0 0.301=0} t f i d f ( Š—ç’ e x a m p l e Š—ç’ , d 2 ) = t f ( Š—ç’ e x a m p l e Š—ç’ , d 2 ) ’‘’• i d f ( Š—ç’ e x a m p l e Š—ç’ , D ) = 0.429 ’‘’• 0.301 Š—ç’ ’ 0.13 { ({},d {2})= ({},d {2}) ({},D)=0.429 0.301 0.13} (using the base 10 logarithm). == tf-idf Beyond Terms == The idea behind TF-IDF has also been applied to entities other than terms. In 1998, the concept of IDF was applied to citations. The authors argued that if a very uncommon citation is shared by two documents, this should be weighted more highly than a citation made by a large number of documents. In addition, tf-idf was applied to visual words with the purpose of conducting object matching in videos, and entire sentences. However, not in all cases did the concept of TF-IDF proved to be more effective than a plain TF scheme (without IDF). When TF-IDF was applied to citations, researchers could find no improvement over a simple citation-count weight that had no IDF component. == tf-idf Derivates == There are a number of term-weighting schemes that derived from TF-IDF. One of them is TF-PDF (Term Frequency * Proportional Document Frequency). TF-PDF was introduced in 2001 in the context of identifying emerging topics in the media. The PDF component measures the difference of how often a term occurs in different domains. Another derivate is TF-IDuF. In TF-IDuF, IDF is not calculated based on the document corpus that is to be searched or recommended. Instead, IDF is calculated based on users personal document collections. The authors report that TF-IDuF was equally effective as tf-idf but could also be applied in situations when e.g. a user modeling system has no access to a global document corpus. == See also == == References == Salton, G; McGill, M. J. (1986). Introduction to modern information retrieval. McGraw-Hill. ISBN 978-0-07-054484-0. Salton, G.; Fox, E. A.; Wu, H. (1983). Extended Boolean information retrieval. Communications of the ACM. 26 (11): 1022-1036. doi:10.1145/182.358466. Salton, G.; Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. Information Processing & Management. 24 (5): 513-523. doi:10.1016/0306-4573(88)90021-0. Wu, H. C.; Luk, R. W. P.; Wong, K. F.; Kwok, K. L. (2008). Interpreting TF-IDF term weights as making relevance decisions. ACM Transactions on Information Systems. 26 (3): 1. doi:10.1145/1361684.1361686. == External links and suggested reading == TFxIDF Repository: A definitive guide to the variants and their evolution. Gensim is a Python library for vector space modeling and includes tf-idf weighting. Robust Hyperlinking: An application of tf-idf for stable document addressability. A demo of using tf-idf with PHP and Euclidean distance for Classification Anatomy of a search engine tf-idf and related definitions as used in Lucene TfidfTransformer in scikit-learn Text to Matrix Generator (TMG) MATLAB toolbox that can be used for various tasks in text mining (TM) specifically i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tf-idf. Pyevolve: A tutorial series explaining the tf-idf calculation. TF/IDF with Google n-Grams and POS Tags"